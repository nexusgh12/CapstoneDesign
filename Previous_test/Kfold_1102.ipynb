{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kfold_1102.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMmZrpD5OVTr"
      },
      "source": [
        "#import\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBVrh_83Oe8p"
      },
      "source": [
        "#import Pytorch Lib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data_utils\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsYTVpSOOhqv"
      },
      "source": [
        "#matplotlib setting\n",
        "%matplotlib inline\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
        "plt.rcParams['axes.grid'] = True "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQ_6fIzoOmzq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "acd972e1-5886-42b4-e5e8-8c4dabb33afc"
      },
      "source": [
        "pcr_df = pd.read_csv('./농도테스트(201007).csv')\n",
        "display(pcr_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x1_10^6</th>\n",
              "      <th>x1_10^6.1</th>\n",
              "      <th>x1_10^6.2</th>\n",
              "      <th>x1_10^6.3</th>\n",
              "      <th>x1_10^6.4</th>\n",
              "      <th>x1_10^6.5</th>\n",
              "      <th>x2_10^6</th>\n",
              "      <th>x2_10^6.1</th>\n",
              "      <th>x2_10^6.2</th>\n",
              "      <th>x2_10^6.3</th>\n",
              "      <th>x2_10^6.4</th>\n",
              "      <th>x3_10^6</th>\n",
              "      <th>x3_10^6.1</th>\n",
              "      <th>x3_10^6.2</th>\n",
              "      <th>x3_10^6.3</th>\n",
              "      <th>x3_10^6.4</th>\n",
              "      <th>x4_10^6</th>\n",
              "      <th>x4_10^6.1</th>\n",
              "      <th>x4_10^6.2</th>\n",
              "      <th>x4_10^6.3</th>\n",
              "      <th>X1_10^5</th>\n",
              "      <th>X1_10^5.1</th>\n",
              "      <th>X1_10^5.2</th>\n",
              "      <th>X1_10^5.3</th>\n",
              "      <th>X1_10^5.4</th>\n",
              "      <th>X1_10^5.5</th>\n",
              "      <th>x2_10^5</th>\n",
              "      <th>x2_10^5.1</th>\n",
              "      <th>x2_10^5.2</th>\n",
              "      <th>x2_10^5.3</th>\n",
              "      <th>x2_10^5.4</th>\n",
              "      <th>x2_10^5.5</th>\n",
              "      <th>x3_10^5</th>\n",
              "      <th>x3_10^5.1</th>\n",
              "      <th>x3_10^5.2</th>\n",
              "      <th>x3_10^5.3</th>\n",
              "      <th>x3_10^5.4</th>\n",
              "      <th>x3_10^5.5</th>\n",
              "      <th>x4_10^5</th>\n",
              "      <th>x4_10^5.1</th>\n",
              "      <th>...</th>\n",
              "      <th>x2_10^3.6</th>\n",
              "      <th>x2_10^3.7</th>\n",
              "      <th>x2_10^3.8</th>\n",
              "      <th>x2_10^3.9</th>\n",
              "      <th>x2_10^3.10</th>\n",
              "      <th>x2_10^3.11</th>\n",
              "      <th>x2_10^3.12</th>\n",
              "      <th>x2_10^3.13</th>\n",
              "      <th>x3_10^3</th>\n",
              "      <th>x3_10^3.1</th>\n",
              "      <th>x3_10^3.2</th>\n",
              "      <th>x3_10^3.3</th>\n",
              "      <th>x3_10^3.4</th>\n",
              "      <th>x3_10^3.5</th>\n",
              "      <th>x3_10^3.6</th>\n",
              "      <th>x3_10^3.7</th>\n",
              "      <th>x3_10^3.8</th>\n",
              "      <th>x3_10^3.9</th>\n",
              "      <th>x3_10^3.10</th>\n",
              "      <th>x3_10^3.11</th>\n",
              "      <th>x3_10^3.12</th>\n",
              "      <th>x3_10^3.13</th>\n",
              "      <th>x4_10^3</th>\n",
              "      <th>x4_10^3.1</th>\n",
              "      <th>x4_10^3.2</th>\n",
              "      <th>x4_10^3.3</th>\n",
              "      <th>x4_10^3.4</th>\n",
              "      <th>x4_10^3.5</th>\n",
              "      <th>x4_10^3.6</th>\n",
              "      <th>x4_10^3.7</th>\n",
              "      <th>x4_10^3.8</th>\n",
              "      <th>x4_10^3.9</th>\n",
              "      <th>x4_10^3.10</th>\n",
              "      <th>x4_10^3.11</th>\n",
              "      <th>x4_10^3.12</th>\n",
              "      <th>x4_10^3.13</th>\n",
              "      <th>X1_Neg</th>\n",
              "      <th>X2_Neg</th>\n",
              "      <th>X3_Neg</th>\n",
              "      <th>X4_Neg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2169</td>\n",
              "      <td>1424</td>\n",
              "      <td>1461</td>\n",
              "      <td>1489</td>\n",
              "      <td>1360</td>\n",
              "      <td>2204</td>\n",
              "      <td>1469</td>\n",
              "      <td>1483</td>\n",
              "      <td>1464</td>\n",
              "      <td>2181</td>\n",
              "      <td>2295</td>\n",
              "      <td>1257</td>\n",
              "      <td>1323</td>\n",
              "      <td>1169</td>\n",
              "      <td>2075</td>\n",
              "      <td>1966</td>\n",
              "      <td>1483</td>\n",
              "      <td>2149</td>\n",
              "      <td>2445</td>\n",
              "      <td>2295</td>\n",
              "      <td>2091</td>\n",
              "      <td>2105</td>\n",
              "      <td>1953</td>\n",
              "      <td>1811</td>\n",
              "      <td>2048</td>\n",
              "      <td>1996</td>\n",
              "      <td>2217</td>\n",
              "      <td>2073</td>\n",
              "      <td>2091</td>\n",
              "      <td>2534</td>\n",
              "      <td>2247</td>\n",
              "      <td>2313</td>\n",
              "      <td>1828</td>\n",
              "      <td>1811</td>\n",
              "      <td>1725</td>\n",
              "      <td>1905</td>\n",
              "      <td>1805</td>\n",
              "      <td>1561</td>\n",
              "      <td>2206</td>\n",
              "      <td>2249</td>\n",
              "      <td>...</td>\n",
              "      <td>2691</td>\n",
              "      <td>2340</td>\n",
              "      <td>2543</td>\n",
              "      <td>2489</td>\n",
              "      <td>2442</td>\n",
              "      <td>2232.0</td>\n",
              "      <td>2653</td>\n",
              "      <td>2212</td>\n",
              "      <td>1890</td>\n",
              "      <td>1964</td>\n",
              "      <td>2066</td>\n",
              "      <td>1775</td>\n",
              "      <td>1922</td>\n",
              "      <td>1836</td>\n",
              "      <td>1669</td>\n",
              "      <td>1957</td>\n",
              "      <td>2068</td>\n",
              "      <td>2108</td>\n",
              "      <td>2055</td>\n",
              "      <td>1849</td>\n",
              "      <td>2349</td>\n",
              "      <td>1921</td>\n",
              "      <td>2132</td>\n",
              "      <td>2109</td>\n",
              "      <td>2450</td>\n",
              "      <td>2306</td>\n",
              "      <td>2382</td>\n",
              "      <td>2348</td>\n",
              "      <td>2451</td>\n",
              "      <td>2294</td>\n",
              "      <td>2761</td>\n",
              "      <td>2368</td>\n",
              "      <td>2754</td>\n",
              "      <td>2598</td>\n",
              "      <td>2913</td>\n",
              "      <td>2494</td>\n",
              "      <td>2020</td>\n",
              "      <td>2380</td>\n",
              "      <td>1948</td>\n",
              "      <td>2377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2164</td>\n",
              "      <td>1416</td>\n",
              "      <td>1454</td>\n",
              "      <td>1480</td>\n",
              "      <td>1348</td>\n",
              "      <td>2187</td>\n",
              "      <td>1451</td>\n",
              "      <td>1475</td>\n",
              "      <td>1464</td>\n",
              "      <td>2177</td>\n",
              "      <td>2274</td>\n",
              "      <td>1240</td>\n",
              "      <td>1330</td>\n",
              "      <td>1160</td>\n",
              "      <td>2054</td>\n",
              "      <td>1984</td>\n",
              "      <td>1465</td>\n",
              "      <td>2136</td>\n",
              "      <td>2423</td>\n",
              "      <td>2259</td>\n",
              "      <td>2077</td>\n",
              "      <td>2094</td>\n",
              "      <td>1939</td>\n",
              "      <td>1810</td>\n",
              "      <td>2039</td>\n",
              "      <td>1981</td>\n",
              "      <td>2242</td>\n",
              "      <td>2053</td>\n",
              "      <td>2065</td>\n",
              "      <td>2514</td>\n",
              "      <td>2248</td>\n",
              "      <td>2275</td>\n",
              "      <td>1828</td>\n",
              "      <td>1804</td>\n",
              "      <td>1726</td>\n",
              "      <td>1911</td>\n",
              "      <td>1800</td>\n",
              "      <td>1553</td>\n",
              "      <td>2181</td>\n",
              "      <td>2225</td>\n",
              "      <td>...</td>\n",
              "      <td>2551</td>\n",
              "      <td>2319</td>\n",
              "      <td>2497</td>\n",
              "      <td>2457</td>\n",
              "      <td>2412</td>\n",
              "      <td>2217.0</td>\n",
              "      <td>2609</td>\n",
              "      <td>2196</td>\n",
              "      <td>1907</td>\n",
              "      <td>1941</td>\n",
              "      <td>2055</td>\n",
              "      <td>1783</td>\n",
              "      <td>1902</td>\n",
              "      <td>1825</td>\n",
              "      <td>1651</td>\n",
              "      <td>1927</td>\n",
              "      <td>2060</td>\n",
              "      <td>2092</td>\n",
              "      <td>2040</td>\n",
              "      <td>1839</td>\n",
              "      <td>2343</td>\n",
              "      <td>1897</td>\n",
              "      <td>2117</td>\n",
              "      <td>2097</td>\n",
              "      <td>2441</td>\n",
              "      <td>2289</td>\n",
              "      <td>2357</td>\n",
              "      <td>2347</td>\n",
              "      <td>2418</td>\n",
              "      <td>2288</td>\n",
              "      <td>2740</td>\n",
              "      <td>2352</td>\n",
              "      <td>2750</td>\n",
              "      <td>2567</td>\n",
              "      <td>2851</td>\n",
              "      <td>2484</td>\n",
              "      <td>2003</td>\n",
              "      <td>2360</td>\n",
              "      <td>1913</td>\n",
              "      <td>2341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2167</td>\n",
              "      <td>1408</td>\n",
              "      <td>1446</td>\n",
              "      <td>1478</td>\n",
              "      <td>1344</td>\n",
              "      <td>2179</td>\n",
              "      <td>1447</td>\n",
              "      <td>1478</td>\n",
              "      <td>1460</td>\n",
              "      <td>2170</td>\n",
              "      <td>2265</td>\n",
              "      <td>1239</td>\n",
              "      <td>1324</td>\n",
              "      <td>1142</td>\n",
              "      <td>2029</td>\n",
              "      <td>1960</td>\n",
              "      <td>1460</td>\n",
              "      <td>2140</td>\n",
              "      <td>2419</td>\n",
              "      <td>2270</td>\n",
              "      <td>2068</td>\n",
              "      <td>2085</td>\n",
              "      <td>1916</td>\n",
              "      <td>1806</td>\n",
              "      <td>2022</td>\n",
              "      <td>1982</td>\n",
              "      <td>2238</td>\n",
              "      <td>2040</td>\n",
              "      <td>2051</td>\n",
              "      <td>2478</td>\n",
              "      <td>2251</td>\n",
              "      <td>2263</td>\n",
              "      <td>1825</td>\n",
              "      <td>1787</td>\n",
              "      <td>1716</td>\n",
              "      <td>1912</td>\n",
              "      <td>1767</td>\n",
              "      <td>1545</td>\n",
              "      <td>2175</td>\n",
              "      <td>2232</td>\n",
              "      <td>...</td>\n",
              "      <td>2505</td>\n",
              "      <td>2305</td>\n",
              "      <td>2484</td>\n",
              "      <td>2437</td>\n",
              "      <td>2390</td>\n",
              "      <td>2223.0</td>\n",
              "      <td>2610</td>\n",
              "      <td>2190</td>\n",
              "      <td>1904</td>\n",
              "      <td>1914</td>\n",
              "      <td>2051</td>\n",
              "      <td>1785</td>\n",
              "      <td>1902</td>\n",
              "      <td>1820</td>\n",
              "      <td>1753</td>\n",
              "      <td>1911</td>\n",
              "      <td>2050</td>\n",
              "      <td>2090</td>\n",
              "      <td>2021</td>\n",
              "      <td>1833</td>\n",
              "      <td>2346</td>\n",
              "      <td>1897</td>\n",
              "      <td>2122</td>\n",
              "      <td>2089</td>\n",
              "      <td>2426</td>\n",
              "      <td>2276</td>\n",
              "      <td>2342</td>\n",
              "      <td>2355</td>\n",
              "      <td>2418</td>\n",
              "      <td>2273</td>\n",
              "      <td>2732</td>\n",
              "      <td>2327</td>\n",
              "      <td>2743</td>\n",
              "      <td>2550</td>\n",
              "      <td>2825</td>\n",
              "      <td>2477</td>\n",
              "      <td>1989</td>\n",
              "      <td>2321</td>\n",
              "      <td>1917</td>\n",
              "      <td>2321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2172</td>\n",
              "      <td>1409</td>\n",
              "      <td>1436</td>\n",
              "      <td>1473</td>\n",
              "      <td>1335</td>\n",
              "      <td>2175</td>\n",
              "      <td>1450</td>\n",
              "      <td>1475</td>\n",
              "      <td>1459</td>\n",
              "      <td>2175</td>\n",
              "      <td>2252</td>\n",
              "      <td>1237</td>\n",
              "      <td>1320</td>\n",
              "      <td>1142</td>\n",
              "      <td>2022</td>\n",
              "      <td>1964</td>\n",
              "      <td>1450</td>\n",
              "      <td>2140</td>\n",
              "      <td>2414</td>\n",
              "      <td>2255</td>\n",
              "      <td>2056</td>\n",
              "      <td>2065</td>\n",
              "      <td>1908</td>\n",
              "      <td>1797</td>\n",
              "      <td>2022</td>\n",
              "      <td>1978</td>\n",
              "      <td>2229</td>\n",
              "      <td>2023</td>\n",
              "      <td>2041</td>\n",
              "      <td>2488</td>\n",
              "      <td>2244</td>\n",
              "      <td>2250</td>\n",
              "      <td>1817</td>\n",
              "      <td>1770</td>\n",
              "      <td>1695</td>\n",
              "      <td>1905</td>\n",
              "      <td>1753</td>\n",
              "      <td>1535</td>\n",
              "      <td>2173</td>\n",
              "      <td>2227</td>\n",
              "      <td>...</td>\n",
              "      <td>2468</td>\n",
              "      <td>2294</td>\n",
              "      <td>2474</td>\n",
              "      <td>2425</td>\n",
              "      <td>2362</td>\n",
              "      <td>2201.0</td>\n",
              "      <td>2595</td>\n",
              "      <td>2173</td>\n",
              "      <td>1903</td>\n",
              "      <td>1914</td>\n",
              "      <td>2060</td>\n",
              "      <td>1794</td>\n",
              "      <td>1883</td>\n",
              "      <td>1817</td>\n",
              "      <td>1723</td>\n",
              "      <td>1902</td>\n",
              "      <td>2047</td>\n",
              "      <td>2079</td>\n",
              "      <td>2014</td>\n",
              "      <td>1821</td>\n",
              "      <td>2358</td>\n",
              "      <td>1887</td>\n",
              "      <td>2115</td>\n",
              "      <td>2073</td>\n",
              "      <td>2416</td>\n",
              "      <td>2270</td>\n",
              "      <td>2339</td>\n",
              "      <td>2358</td>\n",
              "      <td>2419</td>\n",
              "      <td>2265</td>\n",
              "      <td>2722</td>\n",
              "      <td>2315</td>\n",
              "      <td>2728</td>\n",
              "      <td>2540</td>\n",
              "      <td>2798</td>\n",
              "      <td>2471</td>\n",
              "      <td>1997</td>\n",
              "      <td>2306</td>\n",
              "      <td>1903</td>\n",
              "      <td>2307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2174</td>\n",
              "      <td>1406</td>\n",
              "      <td>1428</td>\n",
              "      <td>1472</td>\n",
              "      <td>1332</td>\n",
              "      <td>2170</td>\n",
              "      <td>1448</td>\n",
              "      <td>1474</td>\n",
              "      <td>1457</td>\n",
              "      <td>2164</td>\n",
              "      <td>2264</td>\n",
              "      <td>1234</td>\n",
              "      <td>1316</td>\n",
              "      <td>1143</td>\n",
              "      <td>2040</td>\n",
              "      <td>1950</td>\n",
              "      <td>1445</td>\n",
              "      <td>2138</td>\n",
              "      <td>2399</td>\n",
              "      <td>2252</td>\n",
              "      <td>2053</td>\n",
              "      <td>2056</td>\n",
              "      <td>1895</td>\n",
              "      <td>1798</td>\n",
              "      <td>2015</td>\n",
              "      <td>1975</td>\n",
              "      <td>2224</td>\n",
              "      <td>2010</td>\n",
              "      <td>2040</td>\n",
              "      <td>2463</td>\n",
              "      <td>2239</td>\n",
              "      <td>2241</td>\n",
              "      <td>1802</td>\n",
              "      <td>1760</td>\n",
              "      <td>1679</td>\n",
              "      <td>1881</td>\n",
              "      <td>1786</td>\n",
              "      <td>1537</td>\n",
              "      <td>2169</td>\n",
              "      <td>2224</td>\n",
              "      <td>...</td>\n",
              "      <td>2421</td>\n",
              "      <td>2287</td>\n",
              "      <td>2477</td>\n",
              "      <td>2404</td>\n",
              "      <td>2316</td>\n",
              "      <td>2218.0</td>\n",
              "      <td>2565</td>\n",
              "      <td>2159</td>\n",
              "      <td>1884</td>\n",
              "      <td>1938</td>\n",
              "      <td>2025</td>\n",
              "      <td>1794</td>\n",
              "      <td>1865</td>\n",
              "      <td>1813</td>\n",
              "      <td>1717</td>\n",
              "      <td>1904</td>\n",
              "      <td>2042</td>\n",
              "      <td>2075</td>\n",
              "      <td>1992</td>\n",
              "      <td>1811</td>\n",
              "      <td>2370</td>\n",
              "      <td>1876</td>\n",
              "      <td>2100</td>\n",
              "      <td>2077</td>\n",
              "      <td>2413</td>\n",
              "      <td>2251</td>\n",
              "      <td>2319</td>\n",
              "      <td>2375</td>\n",
              "      <td>2411</td>\n",
              "      <td>2261</td>\n",
              "      <td>2716</td>\n",
              "      <td>2313</td>\n",
              "      <td>2712</td>\n",
              "      <td>2539</td>\n",
              "      <td>2776</td>\n",
              "      <td>2445</td>\n",
              "      <td>1993</td>\n",
              "      <td>2300</td>\n",
              "      <td>1896</td>\n",
              "      <td>2296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2173</td>\n",
              "      <td>1402</td>\n",
              "      <td>1422</td>\n",
              "      <td>1469</td>\n",
              "      <td>1327</td>\n",
              "      <td>2166</td>\n",
              "      <td>1437</td>\n",
              "      <td>1476</td>\n",
              "      <td>1453</td>\n",
              "      <td>2164</td>\n",
              "      <td>2260</td>\n",
              "      <td>1227</td>\n",
              "      <td>1308</td>\n",
              "      <td>1121</td>\n",
              "      <td>2038</td>\n",
              "      <td>1953</td>\n",
              "      <td>1447</td>\n",
              "      <td>2129</td>\n",
              "      <td>2388</td>\n",
              "      <td>2259</td>\n",
              "      <td>2037</td>\n",
              "      <td>2051</td>\n",
              "      <td>1883</td>\n",
              "      <td>1795</td>\n",
              "      <td>2013</td>\n",
              "      <td>1967</td>\n",
              "      <td>2216</td>\n",
              "      <td>2009</td>\n",
              "      <td>2035</td>\n",
              "      <td>2452</td>\n",
              "      <td>2243</td>\n",
              "      <td>2237</td>\n",
              "      <td>1805</td>\n",
              "      <td>1779</td>\n",
              "      <td>1679</td>\n",
              "      <td>1892</td>\n",
              "      <td>1780</td>\n",
              "      <td>1526</td>\n",
              "      <td>2161</td>\n",
              "      <td>2208</td>\n",
              "      <td>...</td>\n",
              "      <td>2399</td>\n",
              "      <td>2274</td>\n",
              "      <td>2450</td>\n",
              "      <td>2393</td>\n",
              "      <td>2293</td>\n",
              "      <td>2230.0</td>\n",
              "      <td>2553</td>\n",
              "      <td>2160</td>\n",
              "      <td>1889</td>\n",
              "      <td>1925</td>\n",
              "      <td>2027</td>\n",
              "      <td>1791</td>\n",
              "      <td>1860</td>\n",
              "      <td>1802</td>\n",
              "      <td>1722</td>\n",
              "      <td>1910</td>\n",
              "      <td>2034</td>\n",
              "      <td>2060</td>\n",
              "      <td>1981</td>\n",
              "      <td>1810</td>\n",
              "      <td>2367</td>\n",
              "      <td>1872</td>\n",
              "      <td>2094</td>\n",
              "      <td>2072</td>\n",
              "      <td>2416</td>\n",
              "      <td>2258</td>\n",
              "      <td>2316</td>\n",
              "      <td>2356</td>\n",
              "      <td>2402</td>\n",
              "      <td>2246</td>\n",
              "      <td>2701</td>\n",
              "      <td>2299</td>\n",
              "      <td>2691</td>\n",
              "      <td>2532</td>\n",
              "      <td>2756</td>\n",
              "      <td>2440</td>\n",
              "      <td>1995</td>\n",
              "      <td>2301</td>\n",
              "      <td>1886</td>\n",
              "      <td>2287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2177</td>\n",
              "      <td>1397</td>\n",
              "      <td>1419</td>\n",
              "      <td>1465</td>\n",
              "      <td>1322</td>\n",
              "      <td>2159</td>\n",
              "      <td>1431</td>\n",
              "      <td>1470</td>\n",
              "      <td>1457</td>\n",
              "      <td>2158</td>\n",
              "      <td>2255</td>\n",
              "      <td>1239</td>\n",
              "      <td>1302</td>\n",
              "      <td>1121</td>\n",
              "      <td>2036</td>\n",
              "      <td>1938</td>\n",
              "      <td>1440</td>\n",
              "      <td>2125</td>\n",
              "      <td>2390</td>\n",
              "      <td>2259</td>\n",
              "      <td>2039</td>\n",
              "      <td>2058</td>\n",
              "      <td>1881</td>\n",
              "      <td>1792</td>\n",
              "      <td>2007</td>\n",
              "      <td>1962</td>\n",
              "      <td>2217</td>\n",
              "      <td>2002</td>\n",
              "      <td>2025</td>\n",
              "      <td>2447</td>\n",
              "      <td>2237</td>\n",
              "      <td>2226</td>\n",
              "      <td>1790</td>\n",
              "      <td>1766</td>\n",
              "      <td>1683</td>\n",
              "      <td>1880</td>\n",
              "      <td>1769</td>\n",
              "      <td>1532</td>\n",
              "      <td>2153</td>\n",
              "      <td>2213</td>\n",
              "      <td>...</td>\n",
              "      <td>2384</td>\n",
              "      <td>2278</td>\n",
              "      <td>2459</td>\n",
              "      <td>2389</td>\n",
              "      <td>2278</td>\n",
              "      <td>2237.0</td>\n",
              "      <td>2547</td>\n",
              "      <td>2159</td>\n",
              "      <td>1897</td>\n",
              "      <td>1928</td>\n",
              "      <td>2030</td>\n",
              "      <td>1791</td>\n",
              "      <td>1866</td>\n",
              "      <td>1796</td>\n",
              "      <td>1712</td>\n",
              "      <td>1918</td>\n",
              "      <td>2024</td>\n",
              "      <td>2061</td>\n",
              "      <td>1981</td>\n",
              "      <td>1802</td>\n",
              "      <td>2358</td>\n",
              "      <td>1863</td>\n",
              "      <td>2099</td>\n",
              "      <td>2065</td>\n",
              "      <td>2420</td>\n",
              "      <td>2260</td>\n",
              "      <td>2318</td>\n",
              "      <td>2370</td>\n",
              "      <td>2394</td>\n",
              "      <td>2249</td>\n",
              "      <td>2694</td>\n",
              "      <td>2285</td>\n",
              "      <td>2701</td>\n",
              "      <td>2523</td>\n",
              "      <td>2739</td>\n",
              "      <td>2434</td>\n",
              "      <td>1989</td>\n",
              "      <td>2300</td>\n",
              "      <td>1880</td>\n",
              "      <td>2285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2176</td>\n",
              "      <td>1392</td>\n",
              "      <td>1418</td>\n",
              "      <td>1460</td>\n",
              "      <td>1323</td>\n",
              "      <td>2161</td>\n",
              "      <td>1432</td>\n",
              "      <td>1473</td>\n",
              "      <td>1458</td>\n",
              "      <td>2155</td>\n",
              "      <td>2249</td>\n",
              "      <td>1215</td>\n",
              "      <td>1284</td>\n",
              "      <td>1139</td>\n",
              "      <td>2028</td>\n",
              "      <td>1943</td>\n",
              "      <td>1432</td>\n",
              "      <td>2120</td>\n",
              "      <td>2387</td>\n",
              "      <td>2253</td>\n",
              "      <td>2033</td>\n",
              "      <td>2066</td>\n",
              "      <td>1876</td>\n",
              "      <td>1787</td>\n",
              "      <td>2010</td>\n",
              "      <td>1957</td>\n",
              "      <td>2226</td>\n",
              "      <td>1998</td>\n",
              "      <td>2026</td>\n",
              "      <td>2454</td>\n",
              "      <td>2245</td>\n",
              "      <td>2214</td>\n",
              "      <td>1801</td>\n",
              "      <td>1779</td>\n",
              "      <td>1688</td>\n",
              "      <td>1892</td>\n",
              "      <td>1750</td>\n",
              "      <td>1532</td>\n",
              "      <td>2143</td>\n",
              "      <td>2213</td>\n",
              "      <td>...</td>\n",
              "      <td>2357</td>\n",
              "      <td>2273</td>\n",
              "      <td>2458</td>\n",
              "      <td>2364</td>\n",
              "      <td>2275</td>\n",
              "      <td>2230.0</td>\n",
              "      <td>2544</td>\n",
              "      <td>2162</td>\n",
              "      <td>1884</td>\n",
              "      <td>1925</td>\n",
              "      <td>2034</td>\n",
              "      <td>1781</td>\n",
              "      <td>1859</td>\n",
              "      <td>1785</td>\n",
              "      <td>1713</td>\n",
              "      <td>1925</td>\n",
              "      <td>2010</td>\n",
              "      <td>2051</td>\n",
              "      <td>1965</td>\n",
              "      <td>1803</td>\n",
              "      <td>2363</td>\n",
              "      <td>1856</td>\n",
              "      <td>2092</td>\n",
              "      <td>2069</td>\n",
              "      <td>2415</td>\n",
              "      <td>2263</td>\n",
              "      <td>2320</td>\n",
              "      <td>2377</td>\n",
              "      <td>2391</td>\n",
              "      <td>2245</td>\n",
              "      <td>2692</td>\n",
              "      <td>2285</td>\n",
              "      <td>2710</td>\n",
              "      <td>2515</td>\n",
              "      <td>2727</td>\n",
              "      <td>2430</td>\n",
              "      <td>1994</td>\n",
              "      <td>2308</td>\n",
              "      <td>1874</td>\n",
              "      <td>2295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2181</td>\n",
              "      <td>1391</td>\n",
              "      <td>1416</td>\n",
              "      <td>1457</td>\n",
              "      <td>1320</td>\n",
              "      <td>2157</td>\n",
              "      <td>1424</td>\n",
              "      <td>1472</td>\n",
              "      <td>1457</td>\n",
              "      <td>2164</td>\n",
              "      <td>2253</td>\n",
              "      <td>1208</td>\n",
              "      <td>1293</td>\n",
              "      <td>1136</td>\n",
              "      <td>2004</td>\n",
              "      <td>1928</td>\n",
              "      <td>1430</td>\n",
              "      <td>2123</td>\n",
              "      <td>2383</td>\n",
              "      <td>2257</td>\n",
              "      <td>2028</td>\n",
              "      <td>2062</td>\n",
              "      <td>1872</td>\n",
              "      <td>1782</td>\n",
              "      <td>2012</td>\n",
              "      <td>1954</td>\n",
              "      <td>2214</td>\n",
              "      <td>1992</td>\n",
              "      <td>2018</td>\n",
              "      <td>2455</td>\n",
              "      <td>2238</td>\n",
              "      <td>2217</td>\n",
              "      <td>1815</td>\n",
              "      <td>1773</td>\n",
              "      <td>1676</td>\n",
              "      <td>1890</td>\n",
              "      <td>1767</td>\n",
              "      <td>1525</td>\n",
              "      <td>2148</td>\n",
              "      <td>2197</td>\n",
              "      <td>...</td>\n",
              "      <td>2350</td>\n",
              "      <td>2274</td>\n",
              "      <td>2451</td>\n",
              "      <td>2354</td>\n",
              "      <td>2267</td>\n",
              "      <td>2226.0</td>\n",
              "      <td>2540</td>\n",
              "      <td>2166</td>\n",
              "      <td>1894</td>\n",
              "      <td>1909</td>\n",
              "      <td>2023</td>\n",
              "      <td>1787</td>\n",
              "      <td>1858</td>\n",
              "      <td>1786</td>\n",
              "      <td>1711</td>\n",
              "      <td>1914</td>\n",
              "      <td>2021</td>\n",
              "      <td>2043</td>\n",
              "      <td>1955</td>\n",
              "      <td>1803</td>\n",
              "      <td>2356</td>\n",
              "      <td>1854</td>\n",
              "      <td>2089</td>\n",
              "      <td>2067</td>\n",
              "      <td>2420</td>\n",
              "      <td>2251</td>\n",
              "      <td>2311</td>\n",
              "      <td>2395</td>\n",
              "      <td>2406</td>\n",
              "      <td>2246</td>\n",
              "      <td>2691</td>\n",
              "      <td>2276</td>\n",
              "      <td>2727</td>\n",
              "      <td>2514</td>\n",
              "      <td>2722</td>\n",
              "      <td>2417</td>\n",
              "      <td>1993</td>\n",
              "      <td>2318</td>\n",
              "      <td>1879</td>\n",
              "      <td>2304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2188</td>\n",
              "      <td>1388</td>\n",
              "      <td>1412</td>\n",
              "      <td>1459</td>\n",
              "      <td>1317</td>\n",
              "      <td>2152</td>\n",
              "      <td>1427</td>\n",
              "      <td>1470</td>\n",
              "      <td>1458</td>\n",
              "      <td>2168</td>\n",
              "      <td>2251</td>\n",
              "      <td>1206</td>\n",
              "      <td>1289</td>\n",
              "      <td>1151</td>\n",
              "      <td>2025</td>\n",
              "      <td>1933</td>\n",
              "      <td>1427</td>\n",
              "      <td>2124</td>\n",
              "      <td>2374</td>\n",
              "      <td>2251</td>\n",
              "      <td>2025</td>\n",
              "      <td>2061</td>\n",
              "      <td>1871</td>\n",
              "      <td>1780</td>\n",
              "      <td>2012</td>\n",
              "      <td>1952</td>\n",
              "      <td>2215</td>\n",
              "      <td>1996</td>\n",
              "      <td>2017</td>\n",
              "      <td>2457</td>\n",
              "      <td>2238</td>\n",
              "      <td>2219</td>\n",
              "      <td>1785</td>\n",
              "      <td>1774</td>\n",
              "      <td>1686</td>\n",
              "      <td>1895</td>\n",
              "      <td>1760</td>\n",
              "      <td>1530</td>\n",
              "      <td>2144</td>\n",
              "      <td>2183</td>\n",
              "      <td>...</td>\n",
              "      <td>2329</td>\n",
              "      <td>2276</td>\n",
              "      <td>2442</td>\n",
              "      <td>2344</td>\n",
              "      <td>2263</td>\n",
              "      <td>2250.0</td>\n",
              "      <td>2539</td>\n",
              "      <td>2162</td>\n",
              "      <td>1892</td>\n",
              "      <td>1919</td>\n",
              "      <td>2010</td>\n",
              "      <td>1786</td>\n",
              "      <td>1856</td>\n",
              "      <td>1785</td>\n",
              "      <td>1715</td>\n",
              "      <td>1919</td>\n",
              "      <td>2020</td>\n",
              "      <td>2040</td>\n",
              "      <td>1948</td>\n",
              "      <td>1801</td>\n",
              "      <td>2361</td>\n",
              "      <td>1860</td>\n",
              "      <td>2090</td>\n",
              "      <td>2052</td>\n",
              "      <td>2426</td>\n",
              "      <td>2256</td>\n",
              "      <td>2316</td>\n",
              "      <td>2413</td>\n",
              "      <td>2405</td>\n",
              "      <td>2246</td>\n",
              "      <td>2687</td>\n",
              "      <td>2277</td>\n",
              "      <td>2730</td>\n",
              "      <td>2512</td>\n",
              "      <td>2720</td>\n",
              "      <td>2418</td>\n",
              "      <td>1996</td>\n",
              "      <td>2330</td>\n",
              "      <td>1876</td>\n",
              "      <td>2309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2187</td>\n",
              "      <td>1388</td>\n",
              "      <td>1412</td>\n",
              "      <td>1458</td>\n",
              "      <td>1316</td>\n",
              "      <td>2149</td>\n",
              "      <td>1431</td>\n",
              "      <td>1472</td>\n",
              "      <td>1457</td>\n",
              "      <td>2171</td>\n",
              "      <td>2262</td>\n",
              "      <td>1220</td>\n",
              "      <td>1299</td>\n",
              "      <td>1148</td>\n",
              "      <td>2020</td>\n",
              "      <td>1945</td>\n",
              "      <td>1429</td>\n",
              "      <td>2116</td>\n",
              "      <td>2378</td>\n",
              "      <td>2265</td>\n",
              "      <td>2023</td>\n",
              "      <td>2054</td>\n",
              "      <td>1870</td>\n",
              "      <td>1775</td>\n",
              "      <td>2011</td>\n",
              "      <td>1950</td>\n",
              "      <td>2219</td>\n",
              "      <td>1991</td>\n",
              "      <td>2020</td>\n",
              "      <td>2451</td>\n",
              "      <td>2232</td>\n",
              "      <td>2220</td>\n",
              "      <td>1812</td>\n",
              "      <td>1768</td>\n",
              "      <td>1671</td>\n",
              "      <td>1886</td>\n",
              "      <td>1759</td>\n",
              "      <td>1524</td>\n",
              "      <td>2153</td>\n",
              "      <td>2202</td>\n",
              "      <td>...</td>\n",
              "      <td>2330</td>\n",
              "      <td>2276</td>\n",
              "      <td>2445</td>\n",
              "      <td>2337</td>\n",
              "      <td>2256</td>\n",
              "      <td>2259.0</td>\n",
              "      <td>2541</td>\n",
              "      <td>2177</td>\n",
              "      <td>1907</td>\n",
              "      <td>1914</td>\n",
              "      <td>2017</td>\n",
              "      <td>1788</td>\n",
              "      <td>1863</td>\n",
              "      <td>1773</td>\n",
              "      <td>1715</td>\n",
              "      <td>1922</td>\n",
              "      <td>2017</td>\n",
              "      <td>2028</td>\n",
              "      <td>1942</td>\n",
              "      <td>1796</td>\n",
              "      <td>2368</td>\n",
              "      <td>1854</td>\n",
              "      <td>2091</td>\n",
              "      <td>2061</td>\n",
              "      <td>2427</td>\n",
              "      <td>2247</td>\n",
              "      <td>2319</td>\n",
              "      <td>2403</td>\n",
              "      <td>2405</td>\n",
              "      <td>2251</td>\n",
              "      <td>2679</td>\n",
              "      <td>2271</td>\n",
              "      <td>2727</td>\n",
              "      <td>2512</td>\n",
              "      <td>2719</td>\n",
              "      <td>2408</td>\n",
              "      <td>1996</td>\n",
              "      <td>2343</td>\n",
              "      <td>1849</td>\n",
              "      <td>2308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2190</td>\n",
              "      <td>1385</td>\n",
              "      <td>1415</td>\n",
              "      <td>1458</td>\n",
              "      <td>1316</td>\n",
              "      <td>2148</td>\n",
              "      <td>1425</td>\n",
              "      <td>1480</td>\n",
              "      <td>1459</td>\n",
              "      <td>2175</td>\n",
              "      <td>2258</td>\n",
              "      <td>1213</td>\n",
              "      <td>1292</td>\n",
              "      <td>1142</td>\n",
              "      <td>2005</td>\n",
              "      <td>1946</td>\n",
              "      <td>1434</td>\n",
              "      <td>2124</td>\n",
              "      <td>2363</td>\n",
              "      <td>2252</td>\n",
              "      <td>2021</td>\n",
              "      <td>2056</td>\n",
              "      <td>1868</td>\n",
              "      <td>1770</td>\n",
              "      <td>2012</td>\n",
              "      <td>1948</td>\n",
              "      <td>2218</td>\n",
              "      <td>1993</td>\n",
              "      <td>2022</td>\n",
              "      <td>2450</td>\n",
              "      <td>2236</td>\n",
              "      <td>2223</td>\n",
              "      <td>1817</td>\n",
              "      <td>1771</td>\n",
              "      <td>1685</td>\n",
              "      <td>1897</td>\n",
              "      <td>1747</td>\n",
              "      <td>1530</td>\n",
              "      <td>2153</td>\n",
              "      <td>2197</td>\n",
              "      <td>...</td>\n",
              "      <td>2338</td>\n",
              "      <td>2273</td>\n",
              "      <td>2446</td>\n",
              "      <td>2339</td>\n",
              "      <td>2249</td>\n",
              "      <td>2269.0</td>\n",
              "      <td>2538</td>\n",
              "      <td>2185</td>\n",
              "      <td>1904</td>\n",
              "      <td>1897</td>\n",
              "      <td>2027</td>\n",
              "      <td>1793</td>\n",
              "      <td>1859</td>\n",
              "      <td>1777</td>\n",
              "      <td>1715</td>\n",
              "      <td>1936</td>\n",
              "      <td>2018</td>\n",
              "      <td>2017</td>\n",
              "      <td>1947</td>\n",
              "      <td>1797</td>\n",
              "      <td>2371</td>\n",
              "      <td>1847</td>\n",
              "      <td>2090</td>\n",
              "      <td>2054</td>\n",
              "      <td>2432</td>\n",
              "      <td>2254</td>\n",
              "      <td>2315</td>\n",
              "      <td>2416</td>\n",
              "      <td>2402</td>\n",
              "      <td>2254</td>\n",
              "      <td>2668</td>\n",
              "      <td>2274</td>\n",
              "      <td>2728</td>\n",
              "      <td>2502</td>\n",
              "      <td>2724</td>\n",
              "      <td>2404</td>\n",
              "      <td>1999</td>\n",
              "      <td>2349</td>\n",
              "      <td>1882</td>\n",
              "      <td>2314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2197</td>\n",
              "      <td>1383</td>\n",
              "      <td>1412</td>\n",
              "      <td>1456</td>\n",
              "      <td>1310</td>\n",
              "      <td>2153</td>\n",
              "      <td>1424</td>\n",
              "      <td>1485</td>\n",
              "      <td>1462</td>\n",
              "      <td>2181</td>\n",
              "      <td>2264</td>\n",
              "      <td>1204</td>\n",
              "      <td>1292</td>\n",
              "      <td>1154</td>\n",
              "      <td>2029</td>\n",
              "      <td>1940</td>\n",
              "      <td>1433</td>\n",
              "      <td>2119</td>\n",
              "      <td>2366</td>\n",
              "      <td>2267</td>\n",
              "      <td>2033</td>\n",
              "      <td>2050</td>\n",
              "      <td>1875</td>\n",
              "      <td>1773</td>\n",
              "      <td>2014</td>\n",
              "      <td>1948</td>\n",
              "      <td>2212</td>\n",
              "      <td>1998</td>\n",
              "      <td>2019</td>\n",
              "      <td>2452</td>\n",
              "      <td>2240</td>\n",
              "      <td>2228</td>\n",
              "      <td>1811</td>\n",
              "      <td>1741</td>\n",
              "      <td>1681</td>\n",
              "      <td>1913</td>\n",
              "      <td>1752</td>\n",
              "      <td>1535</td>\n",
              "      <td>2157</td>\n",
              "      <td>2189</td>\n",
              "      <td>...</td>\n",
              "      <td>2334</td>\n",
              "      <td>2269</td>\n",
              "      <td>2452</td>\n",
              "      <td>2334</td>\n",
              "      <td>2251</td>\n",
              "      <td>2283.0</td>\n",
              "      <td>2546</td>\n",
              "      <td>2197</td>\n",
              "      <td>1878</td>\n",
              "      <td>1922</td>\n",
              "      <td>2003</td>\n",
              "      <td>1787</td>\n",
              "      <td>1853</td>\n",
              "      <td>1769</td>\n",
              "      <td>1707</td>\n",
              "      <td>1930</td>\n",
              "      <td>2028</td>\n",
              "      <td>2024</td>\n",
              "      <td>1942</td>\n",
              "      <td>1788</td>\n",
              "      <td>2374</td>\n",
              "      <td>1854</td>\n",
              "      <td>2096</td>\n",
              "      <td>2060</td>\n",
              "      <td>2430</td>\n",
              "      <td>2255</td>\n",
              "      <td>2314</td>\n",
              "      <td>2413</td>\n",
              "      <td>2392</td>\n",
              "      <td>2261</td>\n",
              "      <td>2672</td>\n",
              "      <td>2277</td>\n",
              "      <td>2722</td>\n",
              "      <td>2496</td>\n",
              "      <td>2719</td>\n",
              "      <td>2407</td>\n",
              "      <td>1997</td>\n",
              "      <td>2360</td>\n",
              "      <td>1878</td>\n",
              "      <td>2305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2191</td>\n",
              "      <td>1388</td>\n",
              "      <td>1413</td>\n",
              "      <td>1459</td>\n",
              "      <td>1311</td>\n",
              "      <td>2152</td>\n",
              "      <td>1423</td>\n",
              "      <td>1490</td>\n",
              "      <td>1465</td>\n",
              "      <td>2188</td>\n",
              "      <td>2271</td>\n",
              "      <td>1188</td>\n",
              "      <td>1281</td>\n",
              "      <td>1150</td>\n",
              "      <td>2020</td>\n",
              "      <td>1937</td>\n",
              "      <td>1427</td>\n",
              "      <td>2130</td>\n",
              "      <td>2364</td>\n",
              "      <td>2266</td>\n",
              "      <td>2031</td>\n",
              "      <td>2053</td>\n",
              "      <td>1872</td>\n",
              "      <td>1772</td>\n",
              "      <td>2016</td>\n",
              "      <td>1951</td>\n",
              "      <td>2194</td>\n",
              "      <td>2004</td>\n",
              "      <td>2017</td>\n",
              "      <td>2451</td>\n",
              "      <td>2238</td>\n",
              "      <td>2232</td>\n",
              "      <td>1803</td>\n",
              "      <td>1767</td>\n",
              "      <td>1671</td>\n",
              "      <td>1911</td>\n",
              "      <td>1743</td>\n",
              "      <td>1533</td>\n",
              "      <td>2153</td>\n",
              "      <td>2176</td>\n",
              "      <td>...</td>\n",
              "      <td>2326</td>\n",
              "      <td>2272</td>\n",
              "      <td>2453</td>\n",
              "      <td>2327</td>\n",
              "      <td>2248</td>\n",
              "      <td>2288.0</td>\n",
              "      <td>2548</td>\n",
              "      <td>2204</td>\n",
              "      <td>1888</td>\n",
              "      <td>1900</td>\n",
              "      <td>2012</td>\n",
              "      <td>1786</td>\n",
              "      <td>1866</td>\n",
              "      <td>1765</td>\n",
              "      <td>1707</td>\n",
              "      <td>1938</td>\n",
              "      <td>2028</td>\n",
              "      <td>2019</td>\n",
              "      <td>1930</td>\n",
              "      <td>1791</td>\n",
              "      <td>2389</td>\n",
              "      <td>1851</td>\n",
              "      <td>2096</td>\n",
              "      <td>2060</td>\n",
              "      <td>2441</td>\n",
              "      <td>2256</td>\n",
              "      <td>2314</td>\n",
              "      <td>2423</td>\n",
              "      <td>2397</td>\n",
              "      <td>2262</td>\n",
              "      <td>2671</td>\n",
              "      <td>2278</td>\n",
              "      <td>2718</td>\n",
              "      <td>2498</td>\n",
              "      <td>2712</td>\n",
              "      <td>2406</td>\n",
              "      <td>2005</td>\n",
              "      <td>2368</td>\n",
              "      <td>1871</td>\n",
              "      <td>2327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2192</td>\n",
              "      <td>1388</td>\n",
              "      <td>1412</td>\n",
              "      <td>1463</td>\n",
              "      <td>1314</td>\n",
              "      <td>2153</td>\n",
              "      <td>1422</td>\n",
              "      <td>1494</td>\n",
              "      <td>1465</td>\n",
              "      <td>2191</td>\n",
              "      <td>2274</td>\n",
              "      <td>1212</td>\n",
              "      <td>1285</td>\n",
              "      <td>1152</td>\n",
              "      <td>1997</td>\n",
              "      <td>1938</td>\n",
              "      <td>1431</td>\n",
              "      <td>2128</td>\n",
              "      <td>2368</td>\n",
              "      <td>2269</td>\n",
              "      <td>2028</td>\n",
              "      <td>2052</td>\n",
              "      <td>1873</td>\n",
              "      <td>1774</td>\n",
              "      <td>2018</td>\n",
              "      <td>1947</td>\n",
              "      <td>2184</td>\n",
              "      <td>2007</td>\n",
              "      <td>2022</td>\n",
              "      <td>2452</td>\n",
              "      <td>2240</td>\n",
              "      <td>2247</td>\n",
              "      <td>1813</td>\n",
              "      <td>1762</td>\n",
              "      <td>1695</td>\n",
              "      <td>1892</td>\n",
              "      <td>1743</td>\n",
              "      <td>1538</td>\n",
              "      <td>2161</td>\n",
              "      <td>2182</td>\n",
              "      <td>...</td>\n",
              "      <td>2317</td>\n",
              "      <td>2275</td>\n",
              "      <td>2450</td>\n",
              "      <td>2321</td>\n",
              "      <td>2244</td>\n",
              "      <td>2301.0</td>\n",
              "      <td>2546</td>\n",
              "      <td>2212</td>\n",
              "      <td>1879</td>\n",
              "      <td>1927</td>\n",
              "      <td>2018</td>\n",
              "      <td>1799</td>\n",
              "      <td>1866</td>\n",
              "      <td>1766</td>\n",
              "      <td>1713</td>\n",
              "      <td>1950</td>\n",
              "      <td>2035</td>\n",
              "      <td>2012</td>\n",
              "      <td>1932</td>\n",
              "      <td>1798</td>\n",
              "      <td>2390</td>\n",
              "      <td>1846</td>\n",
              "      <td>2098</td>\n",
              "      <td>2076</td>\n",
              "      <td>2439</td>\n",
              "      <td>2260</td>\n",
              "      <td>2321</td>\n",
              "      <td>2440</td>\n",
              "      <td>2400</td>\n",
              "      <td>2270</td>\n",
              "      <td>2669</td>\n",
              "      <td>2279</td>\n",
              "      <td>2727</td>\n",
              "      <td>2497</td>\n",
              "      <td>2722</td>\n",
              "      <td>2412</td>\n",
              "      <td>2006</td>\n",
              "      <td>2382</td>\n",
              "      <td>1861</td>\n",
              "      <td>2332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2201</td>\n",
              "      <td>1389</td>\n",
              "      <td>1417</td>\n",
              "      <td>1465</td>\n",
              "      <td>1313</td>\n",
              "      <td>2156</td>\n",
              "      <td>1431</td>\n",
              "      <td>1500</td>\n",
              "      <td>1472</td>\n",
              "      <td>2200</td>\n",
              "      <td>2291</td>\n",
              "      <td>1188</td>\n",
              "      <td>1288</td>\n",
              "      <td>1147</td>\n",
              "      <td>1999</td>\n",
              "      <td>1974</td>\n",
              "      <td>1439</td>\n",
              "      <td>2134</td>\n",
              "      <td>2357</td>\n",
              "      <td>2270</td>\n",
              "      <td>2024</td>\n",
              "      <td>2053</td>\n",
              "      <td>1872</td>\n",
              "      <td>1768</td>\n",
              "      <td>2022</td>\n",
              "      <td>1947</td>\n",
              "      <td>2172</td>\n",
              "      <td>2009</td>\n",
              "      <td>2019</td>\n",
              "      <td>2459</td>\n",
              "      <td>2244</td>\n",
              "      <td>2251</td>\n",
              "      <td>1812</td>\n",
              "      <td>1761</td>\n",
              "      <td>1693</td>\n",
              "      <td>1902</td>\n",
              "      <td>1728</td>\n",
              "      <td>1547</td>\n",
              "      <td>2158</td>\n",
              "      <td>2188</td>\n",
              "      <td>...</td>\n",
              "      <td>2323</td>\n",
              "      <td>2280</td>\n",
              "      <td>2449</td>\n",
              "      <td>2317</td>\n",
              "      <td>2245</td>\n",
              "      <td>2315.0</td>\n",
              "      <td>2547</td>\n",
              "      <td>2227</td>\n",
              "      <td>1898</td>\n",
              "      <td>1924</td>\n",
              "      <td>2027</td>\n",
              "      <td>1803</td>\n",
              "      <td>1870</td>\n",
              "      <td>1768</td>\n",
              "      <td>1714</td>\n",
              "      <td>1955</td>\n",
              "      <td>2035</td>\n",
              "      <td>2010</td>\n",
              "      <td>1924</td>\n",
              "      <td>1794</td>\n",
              "      <td>2362</td>\n",
              "      <td>1857</td>\n",
              "      <td>2103</td>\n",
              "      <td>2079</td>\n",
              "      <td>2445</td>\n",
              "      <td>2260</td>\n",
              "      <td>2322</td>\n",
              "      <td>2445</td>\n",
              "      <td>2390</td>\n",
              "      <td>2271</td>\n",
              "      <td>2665</td>\n",
              "      <td>2284</td>\n",
              "      <td>2727</td>\n",
              "      <td>2494</td>\n",
              "      <td>2725</td>\n",
              "      <td>2416</td>\n",
              "      <td>2010</td>\n",
              "      <td>2388</td>\n",
              "      <td>1880</td>\n",
              "      <td>2341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2211</td>\n",
              "      <td>1392</td>\n",
              "      <td>1424</td>\n",
              "      <td>1472</td>\n",
              "      <td>1314</td>\n",
              "      <td>2159</td>\n",
              "      <td>1438</td>\n",
              "      <td>1516</td>\n",
              "      <td>1483</td>\n",
              "      <td>2205</td>\n",
              "      <td>2305</td>\n",
              "      <td>1203</td>\n",
              "      <td>1287</td>\n",
              "      <td>1157</td>\n",
              "      <td>2011</td>\n",
              "      <td>1974</td>\n",
              "      <td>1439</td>\n",
              "      <td>2140</td>\n",
              "      <td>2369</td>\n",
              "      <td>2271</td>\n",
              "      <td>2025</td>\n",
              "      <td>2054</td>\n",
              "      <td>1871</td>\n",
              "      <td>1772</td>\n",
              "      <td>2024</td>\n",
              "      <td>1947</td>\n",
              "      <td>2158</td>\n",
              "      <td>2014</td>\n",
              "      <td>2025</td>\n",
              "      <td>2460</td>\n",
              "      <td>2249</td>\n",
              "      <td>2252</td>\n",
              "      <td>1789</td>\n",
              "      <td>1744</td>\n",
              "      <td>1691</td>\n",
              "      <td>1922</td>\n",
              "      <td>1729</td>\n",
              "      <td>1560</td>\n",
              "      <td>2162</td>\n",
              "      <td>2186</td>\n",
              "      <td>...</td>\n",
              "      <td>2327</td>\n",
              "      <td>2274</td>\n",
              "      <td>2456</td>\n",
              "      <td>2315</td>\n",
              "      <td>2247</td>\n",
              "      <td>2334.0</td>\n",
              "      <td>2545</td>\n",
              "      <td>2236</td>\n",
              "      <td>1893</td>\n",
              "      <td>1931</td>\n",
              "      <td>2020</td>\n",
              "      <td>1807</td>\n",
              "      <td>1877</td>\n",
              "      <td>1768</td>\n",
              "      <td>1730</td>\n",
              "      <td>1947</td>\n",
              "      <td>2039</td>\n",
              "      <td>2008</td>\n",
              "      <td>1913</td>\n",
              "      <td>1803</td>\n",
              "      <td>2361</td>\n",
              "      <td>1858</td>\n",
              "      <td>2099</td>\n",
              "      <td>2080</td>\n",
              "      <td>2455</td>\n",
              "      <td>2262</td>\n",
              "      <td>2320</td>\n",
              "      <td>2450</td>\n",
              "      <td>2401</td>\n",
              "      <td>2277</td>\n",
              "      <td>2661</td>\n",
              "      <td>2279</td>\n",
              "      <td>2741</td>\n",
              "      <td>2489</td>\n",
              "      <td>2723</td>\n",
              "      <td>2422</td>\n",
              "      <td>2012</td>\n",
              "      <td>2399</td>\n",
              "      <td>1884</td>\n",
              "      <td>2343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2223</td>\n",
              "      <td>1403</td>\n",
              "      <td>1432</td>\n",
              "      <td>1485</td>\n",
              "      <td>1315</td>\n",
              "      <td>2160</td>\n",
              "      <td>1448</td>\n",
              "      <td>1530</td>\n",
              "      <td>1496</td>\n",
              "      <td>2221</td>\n",
              "      <td>2320</td>\n",
              "      <td>1207</td>\n",
              "      <td>1299</td>\n",
              "      <td>1138</td>\n",
              "      <td>1989</td>\n",
              "      <td>1970</td>\n",
              "      <td>1450</td>\n",
              "      <td>2151</td>\n",
              "      <td>2357</td>\n",
              "      <td>2268</td>\n",
              "      <td>2024</td>\n",
              "      <td>2056</td>\n",
              "      <td>1867</td>\n",
              "      <td>1774</td>\n",
              "      <td>2022</td>\n",
              "      <td>1946</td>\n",
              "      <td>2171</td>\n",
              "      <td>2018</td>\n",
              "      <td>2024</td>\n",
              "      <td>2460</td>\n",
              "      <td>2254</td>\n",
              "      <td>2250</td>\n",
              "      <td>1816</td>\n",
              "      <td>1750</td>\n",
              "      <td>1688</td>\n",
              "      <td>1904</td>\n",
              "      <td>1723</td>\n",
              "      <td>1562</td>\n",
              "      <td>2165</td>\n",
              "      <td>2184</td>\n",
              "      <td>...</td>\n",
              "      <td>2328</td>\n",
              "      <td>2285</td>\n",
              "      <td>2459</td>\n",
              "      <td>2307</td>\n",
              "      <td>2235</td>\n",
              "      <td>2346.0</td>\n",
              "      <td>2547</td>\n",
              "      <td>2246</td>\n",
              "      <td>1897</td>\n",
              "      <td>1935</td>\n",
              "      <td>2028</td>\n",
              "      <td>1806</td>\n",
              "      <td>1877</td>\n",
              "      <td>1779</td>\n",
              "      <td>1739</td>\n",
              "      <td>1952</td>\n",
              "      <td>2040</td>\n",
              "      <td>2009</td>\n",
              "      <td>1916</td>\n",
              "      <td>1805</td>\n",
              "      <td>2359</td>\n",
              "      <td>1859</td>\n",
              "      <td>2106</td>\n",
              "      <td>2072</td>\n",
              "      <td>2456</td>\n",
              "      <td>2254</td>\n",
              "      <td>2320</td>\n",
              "      <td>2436</td>\n",
              "      <td>2381</td>\n",
              "      <td>2286</td>\n",
              "      <td>2660</td>\n",
              "      <td>2284</td>\n",
              "      <td>2717</td>\n",
              "      <td>2491</td>\n",
              "      <td>2716</td>\n",
              "      <td>2430</td>\n",
              "      <td>2018</td>\n",
              "      <td>2409</td>\n",
              "      <td>1895</td>\n",
              "      <td>2352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2249</td>\n",
              "      <td>1423</td>\n",
              "      <td>1457</td>\n",
              "      <td>1502</td>\n",
              "      <td>1320</td>\n",
              "      <td>2167</td>\n",
              "      <td>1478</td>\n",
              "      <td>1568</td>\n",
              "      <td>1526</td>\n",
              "      <td>2230</td>\n",
              "      <td>2339</td>\n",
              "      <td>1218</td>\n",
              "      <td>1319</td>\n",
              "      <td>1163</td>\n",
              "      <td>2018</td>\n",
              "      <td>1995</td>\n",
              "      <td>1469</td>\n",
              "      <td>2164</td>\n",
              "      <td>2367</td>\n",
              "      <td>2276</td>\n",
              "      <td>2026</td>\n",
              "      <td>2053</td>\n",
              "      <td>1866</td>\n",
              "      <td>1770</td>\n",
              "      <td>2029</td>\n",
              "      <td>1953</td>\n",
              "      <td>2171</td>\n",
              "      <td>2030</td>\n",
              "      <td>2025</td>\n",
              "      <td>2467</td>\n",
              "      <td>2263</td>\n",
              "      <td>2256</td>\n",
              "      <td>1790</td>\n",
              "      <td>1754</td>\n",
              "      <td>1688</td>\n",
              "      <td>1914</td>\n",
              "      <td>1724</td>\n",
              "      <td>1571</td>\n",
              "      <td>2171</td>\n",
              "      <td>2189</td>\n",
              "      <td>...</td>\n",
              "      <td>2332</td>\n",
              "      <td>2287</td>\n",
              "      <td>2466</td>\n",
              "      <td>2307</td>\n",
              "      <td>2243</td>\n",
              "      <td>2365.0</td>\n",
              "      <td>2551</td>\n",
              "      <td>2256</td>\n",
              "      <td>1872</td>\n",
              "      <td>1929</td>\n",
              "      <td>2028</td>\n",
              "      <td>1813</td>\n",
              "      <td>1883</td>\n",
              "      <td>1768</td>\n",
              "      <td>1745</td>\n",
              "      <td>1962</td>\n",
              "      <td>2043</td>\n",
              "      <td>2018</td>\n",
              "      <td>1915</td>\n",
              "      <td>1803</td>\n",
              "      <td>2367</td>\n",
              "      <td>1853</td>\n",
              "      <td>2108</td>\n",
              "      <td>2074</td>\n",
              "      <td>2456</td>\n",
              "      <td>2265</td>\n",
              "      <td>2325</td>\n",
              "      <td>2445</td>\n",
              "      <td>2406</td>\n",
              "      <td>2290</td>\n",
              "      <td>2655</td>\n",
              "      <td>2280</td>\n",
              "      <td>2709</td>\n",
              "      <td>2485</td>\n",
              "      <td>2722</td>\n",
              "      <td>2431</td>\n",
              "      <td>2019</td>\n",
              "      <td>2412</td>\n",
              "      <td>1893</td>\n",
              "      <td>2365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2288</td>\n",
              "      <td>1462</td>\n",
              "      <td>1505</td>\n",
              "      <td>1549</td>\n",
              "      <td>1330</td>\n",
              "      <td>2177</td>\n",
              "      <td>1528</td>\n",
              "      <td>1632</td>\n",
              "      <td>1578</td>\n",
              "      <td>2245</td>\n",
              "      <td>2350</td>\n",
              "      <td>1241</td>\n",
              "      <td>1345</td>\n",
              "      <td>1175</td>\n",
              "      <td>2032</td>\n",
              "      <td>1980</td>\n",
              "      <td>1505</td>\n",
              "      <td>2212</td>\n",
              "      <td>2378</td>\n",
              "      <td>2290</td>\n",
              "      <td>2029</td>\n",
              "      <td>2055</td>\n",
              "      <td>1868</td>\n",
              "      <td>1777</td>\n",
              "      <td>2032</td>\n",
              "      <td>1951</td>\n",
              "      <td>2178</td>\n",
              "      <td>2028</td>\n",
              "      <td>2034</td>\n",
              "      <td>2469</td>\n",
              "      <td>2274</td>\n",
              "      <td>2271</td>\n",
              "      <td>1813</td>\n",
              "      <td>1740</td>\n",
              "      <td>1671</td>\n",
              "      <td>1932</td>\n",
              "      <td>1722</td>\n",
              "      <td>1587</td>\n",
              "      <td>2170</td>\n",
              "      <td>2184</td>\n",
              "      <td>...</td>\n",
              "      <td>2336</td>\n",
              "      <td>2293</td>\n",
              "      <td>2474</td>\n",
              "      <td>2306</td>\n",
              "      <td>2240</td>\n",
              "      <td>2378.0</td>\n",
              "      <td>2551</td>\n",
              "      <td>2270</td>\n",
              "      <td>1898</td>\n",
              "      <td>1933</td>\n",
              "      <td>2001</td>\n",
              "      <td>1819</td>\n",
              "      <td>1883</td>\n",
              "      <td>1772</td>\n",
              "      <td>1735</td>\n",
              "      <td>1960</td>\n",
              "      <td>2044</td>\n",
              "      <td>2007</td>\n",
              "      <td>1908</td>\n",
              "      <td>1806</td>\n",
              "      <td>2370</td>\n",
              "      <td>1853</td>\n",
              "      <td>2106</td>\n",
              "      <td>2082</td>\n",
              "      <td>2465</td>\n",
              "      <td>2260</td>\n",
              "      <td>2323</td>\n",
              "      <td>2452</td>\n",
              "      <td>2406</td>\n",
              "      <td>2296</td>\n",
              "      <td>2656</td>\n",
              "      <td>2285</td>\n",
              "      <td>2719</td>\n",
              "      <td>2484</td>\n",
              "      <td>2720</td>\n",
              "      <td>2429</td>\n",
              "      <td>2026</td>\n",
              "      <td>2418</td>\n",
              "      <td>1900</td>\n",
              "      <td>2375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2374</td>\n",
              "      <td>1535</td>\n",
              "      <td>1586</td>\n",
              "      <td>1635</td>\n",
              "      <td>1346</td>\n",
              "      <td>2197</td>\n",
              "      <td>1611</td>\n",
              "      <td>1745</td>\n",
              "      <td>1673</td>\n",
              "      <td>2275</td>\n",
              "      <td>2367</td>\n",
              "      <td>1319</td>\n",
              "      <td>1390</td>\n",
              "      <td>1239</td>\n",
              "      <td>2031</td>\n",
              "      <td>2025</td>\n",
              "      <td>1571</td>\n",
              "      <td>2272</td>\n",
              "      <td>2389</td>\n",
              "      <td>2307</td>\n",
              "      <td>2033</td>\n",
              "      <td>2059</td>\n",
              "      <td>1865</td>\n",
              "      <td>1777</td>\n",
              "      <td>2033</td>\n",
              "      <td>1957</td>\n",
              "      <td>2179</td>\n",
              "      <td>2032</td>\n",
              "      <td>2040</td>\n",
              "      <td>2478</td>\n",
              "      <td>2268</td>\n",
              "      <td>2277</td>\n",
              "      <td>1821</td>\n",
              "      <td>1755</td>\n",
              "      <td>1676</td>\n",
              "      <td>1928</td>\n",
              "      <td>1725</td>\n",
              "      <td>1592</td>\n",
              "      <td>2172</td>\n",
              "      <td>2189</td>\n",
              "      <td>...</td>\n",
              "      <td>2338</td>\n",
              "      <td>2295</td>\n",
              "      <td>2483</td>\n",
              "      <td>2297</td>\n",
              "      <td>2242</td>\n",
              "      <td>2382.0</td>\n",
              "      <td>2554</td>\n",
              "      <td>2280</td>\n",
              "      <td>1899</td>\n",
              "      <td>1920</td>\n",
              "      <td>2001</td>\n",
              "      <td>1827</td>\n",
              "      <td>1878</td>\n",
              "      <td>1777</td>\n",
              "      <td>1742</td>\n",
              "      <td>1972</td>\n",
              "      <td>2045</td>\n",
              "      <td>2013</td>\n",
              "      <td>1904</td>\n",
              "      <td>1807</td>\n",
              "      <td>2375</td>\n",
              "      <td>1860</td>\n",
              "      <td>2114</td>\n",
              "      <td>2092</td>\n",
              "      <td>2462</td>\n",
              "      <td>2256</td>\n",
              "      <td>2331</td>\n",
              "      <td>2462</td>\n",
              "      <td>2400</td>\n",
              "      <td>2297</td>\n",
              "      <td>2660</td>\n",
              "      <td>2291</td>\n",
              "      <td>2716</td>\n",
              "      <td>2484</td>\n",
              "      <td>2727</td>\n",
              "      <td>2433</td>\n",
              "      <td>2029</td>\n",
              "      <td>2419</td>\n",
              "      <td>1876</td>\n",
              "      <td>2376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>2512</td>\n",
              "      <td>1662</td>\n",
              "      <td>1719</td>\n",
              "      <td>1773</td>\n",
              "      <td>1381</td>\n",
              "      <td>2224</td>\n",
              "      <td>1752</td>\n",
              "      <td>1931</td>\n",
              "      <td>1834</td>\n",
              "      <td>2326</td>\n",
              "      <td>2406</td>\n",
              "      <td>1418</td>\n",
              "      <td>1473</td>\n",
              "      <td>1283</td>\n",
              "      <td>2064</td>\n",
              "      <td>2044</td>\n",
              "      <td>1672</td>\n",
              "      <td>2379</td>\n",
              "      <td>2411</td>\n",
              "      <td>2336</td>\n",
              "      <td>2041</td>\n",
              "      <td>2060</td>\n",
              "      <td>1868</td>\n",
              "      <td>1778</td>\n",
              "      <td>2034</td>\n",
              "      <td>1967</td>\n",
              "      <td>2187</td>\n",
              "      <td>2040</td>\n",
              "      <td>2040</td>\n",
              "      <td>2477</td>\n",
              "      <td>2277</td>\n",
              "      <td>2285</td>\n",
              "      <td>1813</td>\n",
              "      <td>1750</td>\n",
              "      <td>1676</td>\n",
              "      <td>1931</td>\n",
              "      <td>1749</td>\n",
              "      <td>1591</td>\n",
              "      <td>2181</td>\n",
              "      <td>2194</td>\n",
              "      <td>...</td>\n",
              "      <td>2340</td>\n",
              "      <td>2301</td>\n",
              "      <td>2488</td>\n",
              "      <td>2297</td>\n",
              "      <td>2258</td>\n",
              "      <td>2393.0</td>\n",
              "      <td>2556</td>\n",
              "      <td>2292</td>\n",
              "      <td>1889</td>\n",
              "      <td>1931</td>\n",
              "      <td>2024</td>\n",
              "      <td>1832</td>\n",
              "      <td>1883</td>\n",
              "      <td>1776</td>\n",
              "      <td>1746</td>\n",
              "      <td>1964</td>\n",
              "      <td>2045</td>\n",
              "      <td>2015</td>\n",
              "      <td>1905</td>\n",
              "      <td>1803</td>\n",
              "      <td>2374</td>\n",
              "      <td>1854</td>\n",
              "      <td>2109</td>\n",
              "      <td>2093</td>\n",
              "      <td>2473</td>\n",
              "      <td>2258</td>\n",
              "      <td>2340</td>\n",
              "      <td>2466</td>\n",
              "      <td>2397</td>\n",
              "      <td>2307</td>\n",
              "      <td>2658</td>\n",
              "      <td>2292</td>\n",
              "      <td>2702</td>\n",
              "      <td>2494</td>\n",
              "      <td>2731</td>\n",
              "      <td>2430</td>\n",
              "      <td>2032</td>\n",
              "      <td>2426</td>\n",
              "      <td>1904</td>\n",
              "      <td>2368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>2711</td>\n",
              "      <td>1847</td>\n",
              "      <td>1919</td>\n",
              "      <td>1976</td>\n",
              "      <td>1442</td>\n",
              "      <td>2281</td>\n",
              "      <td>1975</td>\n",
              "      <td>2221</td>\n",
              "      <td>2066</td>\n",
              "      <td>2411</td>\n",
              "      <td>2479</td>\n",
              "      <td>1583</td>\n",
              "      <td>1622</td>\n",
              "      <td>1411</td>\n",
              "      <td>2105</td>\n",
              "      <td>2076</td>\n",
              "      <td>1831</td>\n",
              "      <td>2537</td>\n",
              "      <td>2472</td>\n",
              "      <td>2417</td>\n",
              "      <td>2045</td>\n",
              "      <td>2061</td>\n",
              "      <td>1875</td>\n",
              "      <td>1783</td>\n",
              "      <td>2037</td>\n",
              "      <td>1983</td>\n",
              "      <td>2187</td>\n",
              "      <td>2044</td>\n",
              "      <td>2039</td>\n",
              "      <td>2482</td>\n",
              "      <td>2280</td>\n",
              "      <td>2303</td>\n",
              "      <td>1834</td>\n",
              "      <td>1766</td>\n",
              "      <td>1686</td>\n",
              "      <td>1945</td>\n",
              "      <td>1730</td>\n",
              "      <td>1612</td>\n",
              "      <td>2183</td>\n",
              "      <td>2209</td>\n",
              "      <td>...</td>\n",
              "      <td>2334</td>\n",
              "      <td>2307</td>\n",
              "      <td>2485</td>\n",
              "      <td>2296</td>\n",
              "      <td>2265</td>\n",
              "      <td>2405.0</td>\n",
              "      <td>2551</td>\n",
              "      <td>2303</td>\n",
              "      <td>1877</td>\n",
              "      <td>1936</td>\n",
              "      <td>2016</td>\n",
              "      <td>1845</td>\n",
              "      <td>1879</td>\n",
              "      <td>1770</td>\n",
              "      <td>1735</td>\n",
              "      <td>1978</td>\n",
              "      <td>2054</td>\n",
              "      <td>2020</td>\n",
              "      <td>1902</td>\n",
              "      <td>1807</td>\n",
              "      <td>2387</td>\n",
              "      <td>1855</td>\n",
              "      <td>2117</td>\n",
              "      <td>2101</td>\n",
              "      <td>2457</td>\n",
              "      <td>2263</td>\n",
              "      <td>2342</td>\n",
              "      <td>2470</td>\n",
              "      <td>2394</td>\n",
              "      <td>2313</td>\n",
              "      <td>2659</td>\n",
              "      <td>2293</td>\n",
              "      <td>2705</td>\n",
              "      <td>2495</td>\n",
              "      <td>2731</td>\n",
              "      <td>2424</td>\n",
              "      <td>2037</td>\n",
              "      <td>2431</td>\n",
              "      <td>1900</td>\n",
              "      <td>2361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>2959</td>\n",
              "      <td>2084</td>\n",
              "      <td>2172</td>\n",
              "      <td>2250</td>\n",
              "      <td>1556</td>\n",
              "      <td>2371</td>\n",
              "      <td>2235</td>\n",
              "      <td>2587</td>\n",
              "      <td>2359</td>\n",
              "      <td>2558</td>\n",
              "      <td>2584</td>\n",
              "      <td>1764</td>\n",
              "      <td>1810</td>\n",
              "      <td>1562</td>\n",
              "      <td>2205</td>\n",
              "      <td>2146</td>\n",
              "      <td>2055</td>\n",
              "      <td>2756</td>\n",
              "      <td>2573</td>\n",
              "      <td>2558</td>\n",
              "      <td>2054</td>\n",
              "      <td>2072</td>\n",
              "      <td>1878</td>\n",
              "      <td>1789</td>\n",
              "      <td>2038</td>\n",
              "      <td>2018</td>\n",
              "      <td>2195</td>\n",
              "      <td>2060</td>\n",
              "      <td>2040</td>\n",
              "      <td>2489</td>\n",
              "      <td>2288</td>\n",
              "      <td>2331</td>\n",
              "      <td>1836</td>\n",
              "      <td>1769</td>\n",
              "      <td>1699</td>\n",
              "      <td>1942</td>\n",
              "      <td>1753</td>\n",
              "      <td>1645</td>\n",
              "      <td>2198</td>\n",
              "      <td>2205</td>\n",
              "      <td>...</td>\n",
              "      <td>2328</td>\n",
              "      <td>2312</td>\n",
              "      <td>2498</td>\n",
              "      <td>2297</td>\n",
              "      <td>2257</td>\n",
              "      <td>2411.0</td>\n",
              "      <td>2554</td>\n",
              "      <td>2316</td>\n",
              "      <td>1901</td>\n",
              "      <td>1936</td>\n",
              "      <td>2011</td>\n",
              "      <td>1842</td>\n",
              "      <td>1882</td>\n",
              "      <td>1773</td>\n",
              "      <td>1745</td>\n",
              "      <td>1982</td>\n",
              "      <td>2045</td>\n",
              "      <td>2018</td>\n",
              "      <td>1905</td>\n",
              "      <td>1807</td>\n",
              "      <td>2395</td>\n",
              "      <td>1867</td>\n",
              "      <td>2119</td>\n",
              "      <td>2100</td>\n",
              "      <td>2470</td>\n",
              "      <td>2264</td>\n",
              "      <td>2363</td>\n",
              "      <td>2486</td>\n",
              "      <td>2391</td>\n",
              "      <td>2313</td>\n",
              "      <td>2659</td>\n",
              "      <td>2301</td>\n",
              "      <td>2700</td>\n",
              "      <td>2496</td>\n",
              "      <td>2737</td>\n",
              "      <td>2433</td>\n",
              "      <td>2040</td>\n",
              "      <td>2436</td>\n",
              "      <td>1905</td>\n",
              "      <td>2371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>3234</td>\n",
              "      <td>2344</td>\n",
              "      <td>2456</td>\n",
              "      <td>2545</td>\n",
              "      <td>1736</td>\n",
              "      <td>2543</td>\n",
              "      <td>2548</td>\n",
              "      <td>2998</td>\n",
              "      <td>2684</td>\n",
              "      <td>2787</td>\n",
              "      <td>2763</td>\n",
              "      <td>1961</td>\n",
              "      <td>2040</td>\n",
              "      <td>1776</td>\n",
              "      <td>2352</td>\n",
              "      <td>2254</td>\n",
              "      <td>2296</td>\n",
              "      <td>2973</td>\n",
              "      <td>2746</td>\n",
              "      <td>2756</td>\n",
              "      <td>2073</td>\n",
              "      <td>2083</td>\n",
              "      <td>1882</td>\n",
              "      <td>1792</td>\n",
              "      <td>2045</td>\n",
              "      <td>2073</td>\n",
              "      <td>2223</td>\n",
              "      <td>2080</td>\n",
              "      <td>2053</td>\n",
              "      <td>2493</td>\n",
              "      <td>2290</td>\n",
              "      <td>2394</td>\n",
              "      <td>1842</td>\n",
              "      <td>1788</td>\n",
              "      <td>1707</td>\n",
              "      <td>1953</td>\n",
              "      <td>1734</td>\n",
              "      <td>1673</td>\n",
              "      <td>2209</td>\n",
              "      <td>2229</td>\n",
              "      <td>...</td>\n",
              "      <td>2334</td>\n",
              "      <td>2320</td>\n",
              "      <td>2509</td>\n",
              "      <td>2295</td>\n",
              "      <td>2261</td>\n",
              "      <td>2426.0</td>\n",
              "      <td>2549</td>\n",
              "      <td>2325</td>\n",
              "      <td>1884</td>\n",
              "      <td>1934</td>\n",
              "      <td>2003</td>\n",
              "      <td>1852</td>\n",
              "      <td>1882</td>\n",
              "      <td>1776</td>\n",
              "      <td>1739</td>\n",
              "      <td>1982</td>\n",
              "      <td>2057</td>\n",
              "      <td>2014</td>\n",
              "      <td>1899</td>\n",
              "      <td>1810</td>\n",
              "      <td>2403</td>\n",
              "      <td>1866</td>\n",
              "      <td>2120</td>\n",
              "      <td>2098</td>\n",
              "      <td>2482</td>\n",
              "      <td>2263</td>\n",
              "      <td>2367</td>\n",
              "      <td>2485</td>\n",
              "      <td>2393</td>\n",
              "      <td>2327</td>\n",
              "      <td>2662</td>\n",
              "      <td>2305</td>\n",
              "      <td>2700</td>\n",
              "      <td>2500</td>\n",
              "      <td>2744</td>\n",
              "      <td>2431</td>\n",
              "      <td>2045</td>\n",
              "      <td>2437</td>\n",
              "      <td>1904</td>\n",
              "      <td>2394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>3516</td>\n",
              "      <td>2610</td>\n",
              "      <td>2736</td>\n",
              "      <td>2836</td>\n",
              "      <td>1971</td>\n",
              "      <td>2782</td>\n",
              "      <td>2874</td>\n",
              "      <td>3387</td>\n",
              "      <td>3006</td>\n",
              "      <td>3082</td>\n",
              "      <td>3021</td>\n",
              "      <td>2166</td>\n",
              "      <td>2276</td>\n",
              "      <td>1992</td>\n",
              "      <td>2546</td>\n",
              "      <td>2459</td>\n",
              "      <td>2558</td>\n",
              "      <td>3160</td>\n",
              "      <td>2980</td>\n",
              "      <td>3001</td>\n",
              "      <td>2099</td>\n",
              "      <td>2104</td>\n",
              "      <td>1890</td>\n",
              "      <td>1804</td>\n",
              "      <td>2042</td>\n",
              "      <td>2168</td>\n",
              "      <td>2250</td>\n",
              "      <td>2111</td>\n",
              "      <td>2073</td>\n",
              "      <td>2502</td>\n",
              "      <td>2302</td>\n",
              "      <td>2485</td>\n",
              "      <td>1911</td>\n",
              "      <td>1805</td>\n",
              "      <td>1716</td>\n",
              "      <td>1952</td>\n",
              "      <td>1748</td>\n",
              "      <td>1714</td>\n",
              "      <td>2241</td>\n",
              "      <td>2250</td>\n",
              "      <td>...</td>\n",
              "      <td>2339</td>\n",
              "      <td>2324</td>\n",
              "      <td>2515</td>\n",
              "      <td>2299</td>\n",
              "      <td>2263</td>\n",
              "      <td>2436.0</td>\n",
              "      <td>2552</td>\n",
              "      <td>2334</td>\n",
              "      <td>1894</td>\n",
              "      <td>1920</td>\n",
              "      <td>2025</td>\n",
              "      <td>1869</td>\n",
              "      <td>1888</td>\n",
              "      <td>1774</td>\n",
              "      <td>1743</td>\n",
              "      <td>1991</td>\n",
              "      <td>2052</td>\n",
              "      <td>2009</td>\n",
              "      <td>1895</td>\n",
              "      <td>1812</td>\n",
              "      <td>2377</td>\n",
              "      <td>1868</td>\n",
              "      <td>2124</td>\n",
              "      <td>2114</td>\n",
              "      <td>2482</td>\n",
              "      <td>2263</td>\n",
              "      <td>2369</td>\n",
              "      <td>2488</td>\n",
              "      <td>2397</td>\n",
              "      <td>2338</td>\n",
              "      <td>2665</td>\n",
              "      <td>2315</td>\n",
              "      <td>2710</td>\n",
              "      <td>2505</td>\n",
              "      <td>2748</td>\n",
              "      <td>2426</td>\n",
              "      <td>2047</td>\n",
              "      <td>2444</td>\n",
              "      <td>1904</td>\n",
              "      <td>2392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>3800</td>\n",
              "      <td>2863</td>\n",
              "      <td>3017</td>\n",
              "      <td>3123</td>\n",
              "      <td>2241</td>\n",
              "      <td>3064</td>\n",
              "      <td>3176</td>\n",
              "      <td>3762</td>\n",
              "      <td>3315</td>\n",
              "      <td>3404</td>\n",
              "      <td>3345</td>\n",
              "      <td>2358</td>\n",
              "      <td>2497</td>\n",
              "      <td>2204</td>\n",
              "      <td>2758</td>\n",
              "      <td>2701</td>\n",
              "      <td>2810</td>\n",
              "      <td>3436</td>\n",
              "      <td>3263</td>\n",
              "      <td>3263</td>\n",
              "      <td>2158</td>\n",
              "      <td>2143</td>\n",
              "      <td>1905</td>\n",
              "      <td>1823</td>\n",
              "      <td>2055</td>\n",
              "      <td>2330</td>\n",
              "      <td>2300</td>\n",
              "      <td>2176</td>\n",
              "      <td>2108</td>\n",
              "      <td>2527</td>\n",
              "      <td>2325</td>\n",
              "      <td>2633</td>\n",
              "      <td>1979</td>\n",
              "      <td>1843</td>\n",
              "      <td>1733</td>\n",
              "      <td>1970</td>\n",
              "      <td>1774</td>\n",
              "      <td>1810</td>\n",
              "      <td>2291</td>\n",
              "      <td>2310</td>\n",
              "      <td>...</td>\n",
              "      <td>2344</td>\n",
              "      <td>2328</td>\n",
              "      <td>2517</td>\n",
              "      <td>2304</td>\n",
              "      <td>2264</td>\n",
              "      <td>2454.0</td>\n",
              "      <td>2557</td>\n",
              "      <td>2343</td>\n",
              "      <td>1875</td>\n",
              "      <td>1933</td>\n",
              "      <td>2019</td>\n",
              "      <td>1877</td>\n",
              "      <td>1892</td>\n",
              "      <td>1780</td>\n",
              "      <td>1712</td>\n",
              "      <td>2001</td>\n",
              "      <td>2059</td>\n",
              "      <td>2012</td>\n",
              "      <td>1898</td>\n",
              "      <td>1817</td>\n",
              "      <td>2383</td>\n",
              "      <td>1869</td>\n",
              "      <td>2126</td>\n",
              "      <td>2112</td>\n",
              "      <td>2482</td>\n",
              "      <td>2264</td>\n",
              "      <td>2387</td>\n",
              "      <td>2492</td>\n",
              "      <td>2401</td>\n",
              "      <td>2348</td>\n",
              "      <td>2662</td>\n",
              "      <td>2325</td>\n",
              "      <td>2731</td>\n",
              "      <td>2512</td>\n",
              "      <td>2755</td>\n",
              "      <td>2429</td>\n",
              "      <td>2052</td>\n",
              "      <td>2453</td>\n",
              "      <td>1900</td>\n",
              "      <td>2399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>4060</td>\n",
              "      <td>3101</td>\n",
              "      <td>3256</td>\n",
              "      <td>3387</td>\n",
              "      <td>2525</td>\n",
              "      <td>3364</td>\n",
              "      <td>3489</td>\n",
              "      <td>4079</td>\n",
              "      <td>3602</td>\n",
              "      <td>3760</td>\n",
              "      <td>3707</td>\n",
              "      <td>2527</td>\n",
              "      <td>2697</td>\n",
              "      <td>2402</td>\n",
              "      <td>2995</td>\n",
              "      <td>2905</td>\n",
              "      <td>3036</td>\n",
              "      <td>3658</td>\n",
              "      <td>3551</td>\n",
              "      <td>3525</td>\n",
              "      <td>2269</td>\n",
              "      <td>2220</td>\n",
              "      <td>1931</td>\n",
              "      <td>1860</td>\n",
              "      <td>2067</td>\n",
              "      <td>2556</td>\n",
              "      <td>2398</td>\n",
              "      <td>2300</td>\n",
              "      <td>2164</td>\n",
              "      <td>2572</td>\n",
              "      <td>2367</td>\n",
              "      <td>2870</td>\n",
              "      <td>2098</td>\n",
              "      <td>1940</td>\n",
              "      <td>1757</td>\n",
              "      <td>1973</td>\n",
              "      <td>1782</td>\n",
              "      <td>1969</td>\n",
              "      <td>2395</td>\n",
              "      <td>2394</td>\n",
              "      <td>...</td>\n",
              "      <td>2357</td>\n",
              "      <td>2340</td>\n",
              "      <td>2529</td>\n",
              "      <td>2305</td>\n",
              "      <td>2264</td>\n",
              "      <td>2467.0</td>\n",
              "      <td>2552</td>\n",
              "      <td>2351</td>\n",
              "      <td>1872</td>\n",
              "      <td>1939</td>\n",
              "      <td>2006</td>\n",
              "      <td>1874</td>\n",
              "      <td>1904</td>\n",
              "      <td>1776</td>\n",
              "      <td>1720</td>\n",
              "      <td>2000</td>\n",
              "      <td>2059</td>\n",
              "      <td>2020</td>\n",
              "      <td>1903</td>\n",
              "      <td>1827</td>\n",
              "      <td>2377</td>\n",
              "      <td>1876</td>\n",
              "      <td>2123</td>\n",
              "      <td>2118</td>\n",
              "      <td>2491</td>\n",
              "      <td>2269</td>\n",
              "      <td>2384</td>\n",
              "      <td>2480</td>\n",
              "      <td>2413</td>\n",
              "      <td>2356</td>\n",
              "      <td>2663</td>\n",
              "      <td>2336</td>\n",
              "      <td>2729</td>\n",
              "      <td>2518</td>\n",
              "      <td>2761</td>\n",
              "      <td>2425</td>\n",
              "      <td>2055</td>\n",
              "      <td>2458</td>\n",
              "      <td>1901</td>\n",
              "      <td>2396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>4080</td>\n",
              "      <td>3316</td>\n",
              "      <td>3484</td>\n",
              "      <td>3629</td>\n",
              "      <td>2796</td>\n",
              "      <td>3654</td>\n",
              "      <td>3770</td>\n",
              "      <td>4080</td>\n",
              "      <td>3864</td>\n",
              "      <td>4080</td>\n",
              "      <td>4076</td>\n",
              "      <td>2707</td>\n",
              "      <td>2891</td>\n",
              "      <td>2583</td>\n",
              "      <td>3176</td>\n",
              "      <td>3128</td>\n",
              "      <td>3257</td>\n",
              "      <td>3858</td>\n",
              "      <td>3857</td>\n",
              "      <td>3761</td>\n",
              "      <td>2446</td>\n",
              "      <td>2346</td>\n",
              "      <td>1986</td>\n",
              "      <td>1921</td>\n",
              "      <td>2083</td>\n",
              "      <td>2829</td>\n",
              "      <td>2591</td>\n",
              "      <td>2510</td>\n",
              "      <td>2269</td>\n",
              "      <td>2651</td>\n",
              "      <td>2428</td>\n",
              "      <td>3176</td>\n",
              "      <td>2310</td>\n",
              "      <td>2066</td>\n",
              "      <td>1820</td>\n",
              "      <td>2027</td>\n",
              "      <td>1802</td>\n",
              "      <td>2152</td>\n",
              "      <td>2551</td>\n",
              "      <td>2539</td>\n",
              "      <td>...</td>\n",
              "      <td>2361</td>\n",
              "      <td>2352</td>\n",
              "      <td>2547</td>\n",
              "      <td>2320</td>\n",
              "      <td>2272</td>\n",
              "      <td>2476.0</td>\n",
              "      <td>2554</td>\n",
              "      <td>2365</td>\n",
              "      <td>1887</td>\n",
              "      <td>1934</td>\n",
              "      <td>2006</td>\n",
              "      <td>1889</td>\n",
              "      <td>1915</td>\n",
              "      <td>1781</td>\n",
              "      <td>1727</td>\n",
              "      <td>2006</td>\n",
              "      <td>2062</td>\n",
              "      <td>2017</td>\n",
              "      <td>1892</td>\n",
              "      <td>1830</td>\n",
              "      <td>2380</td>\n",
              "      <td>1884</td>\n",
              "      <td>2129</td>\n",
              "      <td>2120</td>\n",
              "      <td>2491</td>\n",
              "      <td>2275</td>\n",
              "      <td>2400</td>\n",
              "      <td>2491</td>\n",
              "      <td>2428</td>\n",
              "      <td>2368</td>\n",
              "      <td>2673</td>\n",
              "      <td>2348</td>\n",
              "      <td>2731</td>\n",
              "      <td>2518</td>\n",
              "      <td>2771</td>\n",
              "      <td>2428</td>\n",
              "      <td>2061</td>\n",
              "      <td>2468</td>\n",
              "      <td>1887</td>\n",
              "      <td>2384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>4080</td>\n",
              "      <td>3516</td>\n",
              "      <td>3686</td>\n",
              "      <td>3844</td>\n",
              "      <td>3047</td>\n",
              "      <td>3933</td>\n",
              "      <td>4024</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>2874</td>\n",
              "      <td>3071</td>\n",
              "      <td>2744</td>\n",
              "      <td>3363</td>\n",
              "      <td>3302</td>\n",
              "      <td>3457</td>\n",
              "      <td>4033</td>\n",
              "      <td>4080</td>\n",
              "      <td>3984</td>\n",
              "      <td>2688</td>\n",
              "      <td>2532</td>\n",
              "      <td>2068</td>\n",
              "      <td>2039</td>\n",
              "      <td>2120</td>\n",
              "      <td>3111</td>\n",
              "      <td>2881</td>\n",
              "      <td>2811</td>\n",
              "      <td>2449</td>\n",
              "      <td>2786</td>\n",
              "      <td>2533</td>\n",
              "      <td>3498</td>\n",
              "      <td>2519</td>\n",
              "      <td>2287</td>\n",
              "      <td>1909</td>\n",
              "      <td>2106</td>\n",
              "      <td>1873</td>\n",
              "      <td>2394</td>\n",
              "      <td>2756</td>\n",
              "      <td>2764</td>\n",
              "      <td>...</td>\n",
              "      <td>2375</td>\n",
              "      <td>2372</td>\n",
              "      <td>2567</td>\n",
              "      <td>2348</td>\n",
              "      <td>2280</td>\n",
              "      <td>2490.0</td>\n",
              "      <td>2564</td>\n",
              "      <td>2377</td>\n",
              "      <td>1873</td>\n",
              "      <td>1938</td>\n",
              "      <td>2040</td>\n",
              "      <td>1897</td>\n",
              "      <td>1930</td>\n",
              "      <td>1795</td>\n",
              "      <td>1766</td>\n",
              "      <td>2014</td>\n",
              "      <td>2081</td>\n",
              "      <td>2032</td>\n",
              "      <td>1895</td>\n",
              "      <td>1839</td>\n",
              "      <td>2376</td>\n",
              "      <td>1891</td>\n",
              "      <td>2132</td>\n",
              "      <td>2122</td>\n",
              "      <td>2485</td>\n",
              "      <td>2289</td>\n",
              "      <td>2416</td>\n",
              "      <td>2491</td>\n",
              "      <td>2442</td>\n",
              "      <td>2390</td>\n",
              "      <td>2690</td>\n",
              "      <td>2377</td>\n",
              "      <td>2731</td>\n",
              "      <td>2528</td>\n",
              "      <td>2788</td>\n",
              "      <td>2436</td>\n",
              "      <td>2059</td>\n",
              "      <td>2474</td>\n",
              "      <td>1898</td>\n",
              "      <td>2383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>4080</td>\n",
              "      <td>3688</td>\n",
              "      <td>3870</td>\n",
              "      <td>4053</td>\n",
              "      <td>3270</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3007</td>\n",
              "      <td>3222</td>\n",
              "      <td>2906</td>\n",
              "      <td>3551</td>\n",
              "      <td>3528</td>\n",
              "      <td>3624</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>2964</td>\n",
              "      <td>2768</td>\n",
              "      <td>2221</td>\n",
              "      <td>2220</td>\n",
              "      <td>2192</td>\n",
              "      <td>3401</td>\n",
              "      <td>3200</td>\n",
              "      <td>3173</td>\n",
              "      <td>2726</td>\n",
              "      <td>2992</td>\n",
              "      <td>2726</td>\n",
              "      <td>3825</td>\n",
              "      <td>2805</td>\n",
              "      <td>2517</td>\n",
              "      <td>2045</td>\n",
              "      <td>2215</td>\n",
              "      <td>1990</td>\n",
              "      <td>2625</td>\n",
              "      <td>2986</td>\n",
              "      <td>3015</td>\n",
              "      <td>...</td>\n",
              "      <td>2411</td>\n",
              "      <td>2412</td>\n",
              "      <td>2604</td>\n",
              "      <td>2400</td>\n",
              "      <td>2287</td>\n",
              "      <td>2513.0</td>\n",
              "      <td>2581</td>\n",
              "      <td>2390</td>\n",
              "      <td>1903</td>\n",
              "      <td>1913</td>\n",
              "      <td>2028</td>\n",
              "      <td>1920</td>\n",
              "      <td>1956</td>\n",
              "      <td>1831</td>\n",
              "      <td>1785</td>\n",
              "      <td>2027</td>\n",
              "      <td>2102</td>\n",
              "      <td>2066</td>\n",
              "      <td>1912</td>\n",
              "      <td>1846</td>\n",
              "      <td>2362</td>\n",
              "      <td>1904</td>\n",
              "      <td>2124</td>\n",
              "      <td>2120</td>\n",
              "      <td>2494</td>\n",
              "      <td>2303</td>\n",
              "      <td>2450</td>\n",
              "      <td>2492</td>\n",
              "      <td>2479</td>\n",
              "      <td>2421</td>\n",
              "      <td>2715</td>\n",
              "      <td>2424</td>\n",
              "      <td>2737</td>\n",
              "      <td>2542</td>\n",
              "      <td>2797</td>\n",
              "      <td>2451</td>\n",
              "      <td>2074</td>\n",
              "      <td>2485</td>\n",
              "      <td>1917</td>\n",
              "      <td>2392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>4080</td>\n",
              "      <td>3836</td>\n",
              "      <td>4039</td>\n",
              "      <td>4080</td>\n",
              "      <td>3467</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3131</td>\n",
              "      <td>3344</td>\n",
              "      <td>3058</td>\n",
              "      <td>3688</td>\n",
              "      <td>3692</td>\n",
              "      <td>3788</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3243</td>\n",
              "      <td>3025</td>\n",
              "      <td>2445</td>\n",
              "      <td>2469</td>\n",
              "      <td>2314</td>\n",
              "      <td>3656</td>\n",
              "      <td>3548</td>\n",
              "      <td>3557</td>\n",
              "      <td>3090</td>\n",
              "      <td>3262</td>\n",
              "      <td>2991</td>\n",
              "      <td>4080</td>\n",
              "      <td>3054</td>\n",
              "      <td>2768</td>\n",
              "      <td>2238</td>\n",
              "      <td>2425</td>\n",
              "      <td>2141</td>\n",
              "      <td>2846</td>\n",
              "      <td>3234</td>\n",
              "      <td>3303</td>\n",
              "      <td>...</td>\n",
              "      <td>2487</td>\n",
              "      <td>2483</td>\n",
              "      <td>2661</td>\n",
              "      <td>2502</td>\n",
              "      <td>2306</td>\n",
              "      <td>2550.0</td>\n",
              "      <td>2611</td>\n",
              "      <td>2419</td>\n",
              "      <td>1890</td>\n",
              "      <td>1945</td>\n",
              "      <td>2026</td>\n",
              "      <td>1978</td>\n",
              "      <td>2002</td>\n",
              "      <td>1889</td>\n",
              "      <td>1801</td>\n",
              "      <td>2059</td>\n",
              "      <td>2133</td>\n",
              "      <td>2113</td>\n",
              "      <td>1920</td>\n",
              "      <td>1865</td>\n",
              "      <td>2359</td>\n",
              "      <td>1906</td>\n",
              "      <td>2123</td>\n",
              "      <td>2125</td>\n",
              "      <td>2500</td>\n",
              "      <td>2339</td>\n",
              "      <td>2520</td>\n",
              "      <td>2506</td>\n",
              "      <td>2533</td>\n",
              "      <td>2482</td>\n",
              "      <td>2778</td>\n",
              "      <td>2517</td>\n",
              "      <td>2737</td>\n",
              "      <td>2565</td>\n",
              "      <td>2816</td>\n",
              "      <td>2457</td>\n",
              "      <td>2073</td>\n",
              "      <td>2492</td>\n",
              "      <td>1911</td>\n",
              "      <td>2392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>4080</td>\n",
              "      <td>3980</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3639</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3218</td>\n",
              "      <td>3456</td>\n",
              "      <td>3184</td>\n",
              "      <td>3806</td>\n",
              "      <td>3837</td>\n",
              "      <td>3914</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3512</td>\n",
              "      <td>3277</td>\n",
              "      <td>2728</td>\n",
              "      <td>2761</td>\n",
              "      <td>2521</td>\n",
              "      <td>3898</td>\n",
              "      <td>3880</td>\n",
              "      <td>3929</td>\n",
              "      <td>3476</td>\n",
              "      <td>3533</td>\n",
              "      <td>3320</td>\n",
              "      <td>4080</td>\n",
              "      <td>3281</td>\n",
              "      <td>2995</td>\n",
              "      <td>2455</td>\n",
              "      <td>2566</td>\n",
              "      <td>2344</td>\n",
              "      <td>3045</td>\n",
              "      <td>3502</td>\n",
              "      <td>3573</td>\n",
              "      <td>...</td>\n",
              "      <td>2595</td>\n",
              "      <td>2606</td>\n",
              "      <td>2759</td>\n",
              "      <td>2669</td>\n",
              "      <td>2345</td>\n",
              "      <td>2601.0</td>\n",
              "      <td>2678</td>\n",
              "      <td>2463</td>\n",
              "      <td>1891</td>\n",
              "      <td>1930</td>\n",
              "      <td>2030</td>\n",
              "      <td>2063</td>\n",
              "      <td>2090</td>\n",
              "      <td>1989</td>\n",
              "      <td>1843</td>\n",
              "      <td>2112</td>\n",
              "      <td>2203</td>\n",
              "      <td>2213</td>\n",
              "      <td>1942</td>\n",
              "      <td>1882</td>\n",
              "      <td>2363</td>\n",
              "      <td>1930</td>\n",
              "      <td>2132</td>\n",
              "      <td>2137</td>\n",
              "      <td>2504</td>\n",
              "      <td>2414</td>\n",
              "      <td>2635</td>\n",
              "      <td>2514</td>\n",
              "      <td>2632</td>\n",
              "      <td>2594</td>\n",
              "      <td>2886</td>\n",
              "      <td>2661</td>\n",
              "      <td>2765</td>\n",
              "      <td>2619</td>\n",
              "      <td>2848</td>\n",
              "      <td>2476</td>\n",
              "      <td>2077</td>\n",
              "      <td>2501</td>\n",
              "      <td>1900</td>\n",
              "      <td>2387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3779</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3315</td>\n",
              "      <td>3517</td>\n",
              "      <td>3290</td>\n",
              "      <td>3910</td>\n",
              "      <td>3946</td>\n",
              "      <td>4033</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3760</td>\n",
              "      <td>3518</td>\n",
              "      <td>3046</td>\n",
              "      <td>3060</td>\n",
              "      <td>2799</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3875</td>\n",
              "      <td>3838</td>\n",
              "      <td>3667</td>\n",
              "      <td>4080</td>\n",
              "      <td>3487</td>\n",
              "      <td>3224</td>\n",
              "      <td>2674</td>\n",
              "      <td>2885</td>\n",
              "      <td>2522</td>\n",
              "      <td>3230</td>\n",
              "      <td>3727</td>\n",
              "      <td>3837</td>\n",
              "      <td>...</td>\n",
              "      <td>2768</td>\n",
              "      <td>2803</td>\n",
              "      <td>2903</td>\n",
              "      <td>2925</td>\n",
              "      <td>2435</td>\n",
              "      <td>2699.0</td>\n",
              "      <td>2795</td>\n",
              "      <td>2541</td>\n",
              "      <td>1901</td>\n",
              "      <td>1944</td>\n",
              "      <td>2035</td>\n",
              "      <td>2217</td>\n",
              "      <td>2228</td>\n",
              "      <td>2143</td>\n",
              "      <td>1919</td>\n",
              "      <td>2215</td>\n",
              "      <td>2313</td>\n",
              "      <td>2352</td>\n",
              "      <td>1975</td>\n",
              "      <td>1906</td>\n",
              "      <td>2368</td>\n",
              "      <td>1972</td>\n",
              "      <td>2136</td>\n",
              "      <td>2145</td>\n",
              "      <td>2505</td>\n",
              "      <td>2528</td>\n",
              "      <td>2797</td>\n",
              "      <td>2558</td>\n",
              "      <td>2794</td>\n",
              "      <td>2749</td>\n",
              "      <td>3059</td>\n",
              "      <td>2860</td>\n",
              "      <td>2791</td>\n",
              "      <td>2716</td>\n",
              "      <td>2911</td>\n",
              "      <td>2518</td>\n",
              "      <td>2077</td>\n",
              "      <td>2508</td>\n",
              "      <td>1934</td>\n",
              "      <td>2377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3899</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3422</td>\n",
              "      <td>3622</td>\n",
              "      <td>3370</td>\n",
              "      <td>4013</td>\n",
              "      <td>4047</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3980</td>\n",
              "      <td>3729</td>\n",
              "      <td>3367</td>\n",
              "      <td>3350</td>\n",
              "      <td>3123</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4012</td>\n",
              "      <td>4080</td>\n",
              "      <td>3650</td>\n",
              "      <td>3437</td>\n",
              "      <td>2893</td>\n",
              "      <td>3151</td>\n",
              "      <td>2751</td>\n",
              "      <td>3391</td>\n",
              "      <td>3936</td>\n",
              "      <td>4055</td>\n",
              "      <td>...</td>\n",
              "      <td>3034</td>\n",
              "      <td>3070</td>\n",
              "      <td>3132</td>\n",
              "      <td>3255</td>\n",
              "      <td>2575</td>\n",
              "      <td>2833.0</td>\n",
              "      <td>2976</td>\n",
              "      <td>2662</td>\n",
              "      <td>1886</td>\n",
              "      <td>1956</td>\n",
              "      <td>2035</td>\n",
              "      <td>2423</td>\n",
              "      <td>2436</td>\n",
              "      <td>2362</td>\n",
              "      <td>2038</td>\n",
              "      <td>2367</td>\n",
              "      <td>2467</td>\n",
              "      <td>2551</td>\n",
              "      <td>2062</td>\n",
              "      <td>1977</td>\n",
              "      <td>2389</td>\n",
              "      <td>2044</td>\n",
              "      <td>2143</td>\n",
              "      <td>2145</td>\n",
              "      <td>2533</td>\n",
              "      <td>2696</td>\n",
              "      <td>3003</td>\n",
              "      <td>2612</td>\n",
              "      <td>3008</td>\n",
              "      <td>2969</td>\n",
              "      <td>3296</td>\n",
              "      <td>3111</td>\n",
              "      <td>2865</td>\n",
              "      <td>2886</td>\n",
              "      <td>3019</td>\n",
              "      <td>2580</td>\n",
              "      <td>2080</td>\n",
              "      <td>2519</td>\n",
              "      <td>1933</td>\n",
              "      <td>2383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3991</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3507</td>\n",
              "      <td>3707</td>\n",
              "      <td>3467</td>\n",
              "      <td>4079</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3920</td>\n",
              "      <td>3685</td>\n",
              "      <td>3623</td>\n",
              "      <td>3457</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3829</td>\n",
              "      <td>3629</td>\n",
              "      <td>3086</td>\n",
              "      <td>3399</td>\n",
              "      <td>2948</td>\n",
              "      <td>3531</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>...</td>\n",
              "      <td>3349</td>\n",
              "      <td>3434</td>\n",
              "      <td>3424</td>\n",
              "      <td>3632</td>\n",
              "      <td>2817</td>\n",
              "      <td>3060.0</td>\n",
              "      <td>3224</td>\n",
              "      <td>2865</td>\n",
              "      <td>1912</td>\n",
              "      <td>1981</td>\n",
              "      <td>2033</td>\n",
              "      <td>2684</td>\n",
              "      <td>2672</td>\n",
              "      <td>2616</td>\n",
              "      <td>2139</td>\n",
              "      <td>2570</td>\n",
              "      <td>2672</td>\n",
              "      <td>2778</td>\n",
              "      <td>2179</td>\n",
              "      <td>2070</td>\n",
              "      <td>2421</td>\n",
              "      <td>2171</td>\n",
              "      <td>2154</td>\n",
              "      <td>2159</td>\n",
              "      <td>2560</td>\n",
              "      <td>2892</td>\n",
              "      <td>3344</td>\n",
              "      <td>2665</td>\n",
              "      <td>3271</td>\n",
              "      <td>3219</td>\n",
              "      <td>3570</td>\n",
              "      <td>3384</td>\n",
              "      <td>2982</td>\n",
              "      <td>3126</td>\n",
              "      <td>3189</td>\n",
              "      <td>2696</td>\n",
              "      <td>2085</td>\n",
              "      <td>2521</td>\n",
              "      <td>1936</td>\n",
              "      <td>2387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4073</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3553</td>\n",
              "      <td>3763</td>\n",
              "      <td>3514</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3969</td>\n",
              "      <td>3874</td>\n",
              "      <td>3785</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3960</td>\n",
              "      <td>3809</td>\n",
              "      <td>3264</td>\n",
              "      <td>3596</td>\n",
              "      <td>3116</td>\n",
              "      <td>3662</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>...</td>\n",
              "      <td>3700</td>\n",
              "      <td>3840</td>\n",
              "      <td>3757</td>\n",
              "      <td>4024</td>\n",
              "      <td>3175</td>\n",
              "      <td>3329.0</td>\n",
              "      <td>3528</td>\n",
              "      <td>3139</td>\n",
              "      <td>1949</td>\n",
              "      <td>2027</td>\n",
              "      <td>2040</td>\n",
              "      <td>2958</td>\n",
              "      <td>2930</td>\n",
              "      <td>2873</td>\n",
              "      <td>2267</td>\n",
              "      <td>2783</td>\n",
              "      <td>2905</td>\n",
              "      <td>3009</td>\n",
              "      <td>2377</td>\n",
              "      <td>2223</td>\n",
              "      <td>2455</td>\n",
              "      <td>2353</td>\n",
              "      <td>2168</td>\n",
              "      <td>2166</td>\n",
              "      <td>2613</td>\n",
              "      <td>3109</td>\n",
              "      <td>3583</td>\n",
              "      <td>2790</td>\n",
              "      <td>3552</td>\n",
              "      <td>3482</td>\n",
              "      <td>3853</td>\n",
              "      <td>3609</td>\n",
              "      <td>3165</td>\n",
              "      <td>3429</td>\n",
              "      <td>3423</td>\n",
              "      <td>2873</td>\n",
              "      <td>2088</td>\n",
              "      <td>2524</td>\n",
              "      <td>1927</td>\n",
              "      <td>2387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3620</td>\n",
              "      <td>3822</td>\n",
              "      <td>3587</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4058</td>\n",
              "      <td>3949</td>\n",
              "      <td>3417</td>\n",
              "      <td>3765</td>\n",
              "      <td>3292</td>\n",
              "      <td>3773</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>...</td>\n",
              "      <td>4038</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3546</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3834</td>\n",
              "      <td>3449</td>\n",
              "      <td>1976</td>\n",
              "      <td>2131</td>\n",
              "      <td>2036</td>\n",
              "      <td>3225</td>\n",
              "      <td>3161</td>\n",
              "      <td>3119</td>\n",
              "      <td>2373</td>\n",
              "      <td>3002</td>\n",
              "      <td>3122</td>\n",
              "      <td>3227</td>\n",
              "      <td>2590</td>\n",
              "      <td>2421</td>\n",
              "      <td>2536</td>\n",
              "      <td>2588</td>\n",
              "      <td>2208</td>\n",
              "      <td>2178</td>\n",
              "      <td>2710</td>\n",
              "      <td>3327</td>\n",
              "      <td>3853</td>\n",
              "      <td>2904</td>\n",
              "      <td>3829</td>\n",
              "      <td>3751</td>\n",
              "      <td>4080</td>\n",
              "      <td>3913</td>\n",
              "      <td>3409</td>\n",
              "      <td>3706</td>\n",
              "      <td>3690</td>\n",
              "      <td>3128</td>\n",
              "      <td>2087</td>\n",
              "      <td>2534</td>\n",
              "      <td>1933</td>\n",
              "      <td>2395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3651</td>\n",
              "      <td>3853</td>\n",
              "      <td>3627</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4059</td>\n",
              "      <td>3556</td>\n",
              "      <td>3917</td>\n",
              "      <td>3466</td>\n",
              "      <td>3871</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>...</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3951</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4080</td>\n",
              "      <td>3772</td>\n",
              "      <td>2088</td>\n",
              "      <td>2245</td>\n",
              "      <td>2072</td>\n",
              "      <td>3483</td>\n",
              "      <td>3386</td>\n",
              "      <td>3343</td>\n",
              "      <td>2557</td>\n",
              "      <td>3224</td>\n",
              "      <td>3320</td>\n",
              "      <td>3431</td>\n",
              "      <td>2821</td>\n",
              "      <td>2634</td>\n",
              "      <td>2637</td>\n",
              "      <td>2830</td>\n",
              "      <td>2275</td>\n",
              "      <td>2209</td>\n",
              "      <td>2875</td>\n",
              "      <td>3542</td>\n",
              "      <td>4080</td>\n",
              "      <td>3062</td>\n",
              "      <td>4080</td>\n",
              "      <td>3985</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3705</td>\n",
              "      <td>4002</td>\n",
              "      <td>3976</td>\n",
              "      <td>3420</td>\n",
              "      <td>2093</td>\n",
              "      <td>2540</td>\n",
              "      <td>1932</td>\n",
              "      <td>2396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3715</td>\n",
              "      <td>3904</td>\n",
              "      <td>3680</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3661</td>\n",
              "      <td>3928</td>\n",
              "      <td>3593</td>\n",
              "      <td>3935</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>...</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4080</td>\n",
              "      <td>4076</td>\n",
              "      <td>2231</td>\n",
              "      <td>2458</td>\n",
              "      <td>2088</td>\n",
              "      <td>3703</td>\n",
              "      <td>3576</td>\n",
              "      <td>3552</td>\n",
              "      <td>2493</td>\n",
              "      <td>3417</td>\n",
              "      <td>3513</td>\n",
              "      <td>3606</td>\n",
              "      <td>3050</td>\n",
              "      <td>2846</td>\n",
              "      <td>2758</td>\n",
              "      <td>3073</td>\n",
              "      <td>2385</td>\n",
              "      <td>2253</td>\n",
              "      <td>3086</td>\n",
              "      <td>3726</td>\n",
              "      <td>4080</td>\n",
              "      <td>3212</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4040</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3733</td>\n",
              "      <td>2094</td>\n",
              "      <td>2545</td>\n",
              "      <td>1938</td>\n",
              "      <td>2408</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>40 rows × 176 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    x1_10^6  x1_10^6.1  x1_10^6.2  x1_10^6.3  ...  X1_Neg  X2_Neg  X3_Neg  X4_Neg\n",
              "0      2169       1424       1461       1489  ...    2020    2380    1948    2377\n",
              "1      2164       1416       1454       1480  ...    2003    2360    1913    2341\n",
              "2      2167       1408       1446       1478  ...    1989    2321    1917    2321\n",
              "3      2172       1409       1436       1473  ...    1997    2306    1903    2307\n",
              "4      2174       1406       1428       1472  ...    1993    2300    1896    2296\n",
              "5      2173       1402       1422       1469  ...    1995    2301    1886    2287\n",
              "6      2177       1397       1419       1465  ...    1989    2300    1880    2285\n",
              "7      2176       1392       1418       1460  ...    1994    2308    1874    2295\n",
              "8      2181       1391       1416       1457  ...    1993    2318    1879    2304\n",
              "9      2188       1388       1412       1459  ...    1996    2330    1876    2309\n",
              "10     2187       1388       1412       1458  ...    1996    2343    1849    2308\n",
              "11     2190       1385       1415       1458  ...    1999    2349    1882    2314\n",
              "12     2197       1383       1412       1456  ...    1997    2360    1878    2305\n",
              "13     2191       1388       1413       1459  ...    2005    2368    1871    2327\n",
              "14     2192       1388       1412       1463  ...    2006    2382    1861    2332\n",
              "15     2201       1389       1417       1465  ...    2010    2388    1880    2341\n",
              "16     2211       1392       1424       1472  ...    2012    2399    1884    2343\n",
              "17     2223       1403       1432       1485  ...    2018    2409    1895    2352\n",
              "18     2249       1423       1457       1502  ...    2019    2412    1893    2365\n",
              "19     2288       1462       1505       1549  ...    2026    2418    1900    2375\n",
              "20     2374       1535       1586       1635  ...    2029    2419    1876    2376\n",
              "21     2512       1662       1719       1773  ...    2032    2426    1904    2368\n",
              "22     2711       1847       1919       1976  ...    2037    2431    1900    2361\n",
              "23     2959       2084       2172       2250  ...    2040    2436    1905    2371\n",
              "24     3234       2344       2456       2545  ...    2045    2437    1904    2394\n",
              "25     3516       2610       2736       2836  ...    2047    2444    1904    2392\n",
              "26     3800       2863       3017       3123  ...    2052    2453    1900    2399\n",
              "27     4060       3101       3256       3387  ...    2055    2458    1901    2396\n",
              "28     4080       3316       3484       3629  ...    2061    2468    1887    2384\n",
              "29     4080       3516       3686       3844  ...    2059    2474    1898    2383\n",
              "30     4080       3688       3870       4053  ...    2074    2485    1917    2392\n",
              "31     4080       3836       4039       4080  ...    2073    2492    1911    2392\n",
              "32     4080       3980       4080       4080  ...    2077    2501    1900    2387\n",
              "33     4080       4080       4080       4080  ...    2077    2508    1934    2377\n",
              "34     4080       4080       4080       4080  ...    2080    2519    1933    2383\n",
              "35     4080       4080       4080       4080  ...    2085    2521    1936    2387\n",
              "36     4080       4080       4080       4080  ...    2088    2524    1927    2387\n",
              "37     4080       4080       4080       4080  ...    2087    2534    1933    2395\n",
              "38     4080       4080       4080       4080  ...    2093    2540    1932    2396\n",
              "39     4080       4080       4080       4080  ...    2094    2545    1938    2408\n",
              "\n",
              "[40 rows x 176 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuGC5YwuOnhZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2554cf24-1939-4aad-a045-134c88596242"
      },
      "source": [
        "#drop Neg data\n",
        "pcr_df = pcr_df.drop(columns=['X1_Neg', 'X2_Neg', 'X3_Neg', 'X4_Neg'])\n",
        "\n",
        "#drop 10^6 data\n",
        "pcr_df = pcr_df.drop(columns=['x1_10^6', 'x1_10^6.1',\t'x1_10^6.2',\t'x1_10^6.3',\t'x1_10^6.4',\t'x1_10^6.5'\n",
        "                              ,\t'x2_10^6',\t'x2_10^6.1',\t'x2_10^6.2',\t'x2_10^6.3',\t'x2_10^6.4'\n",
        "                              ,\t'x3_10^6',\t'x3_10^6.1',\t'x3_10^6.2',\t'x3_10^6.3',\t'x3_10^6.4'\n",
        "                              ,\t'x4_10^6',\t'x4_10^6.1',\t'x4_10^6.2',\t'x4_10^6.3'])\n",
        "\n",
        "\n",
        "#drop weird data\n",
        "sns.lineplot(data=pcr_df[['x3_10^3.2', 'x1_10^3.12', 'x1_10^3.3', 'x1_10^3', 'x4_10^4.8','x2_10^3.11', 'x4_10^3.1', 'x4_10^3', 'x3_10^3.6']], dashes=False)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "\n",
        "pcr_df = pcr_df.drop(columns=['x3_10^3.2', 'x1_10^3.12', 'x1_10^3.3', 'x1_10^3',  'x4_10^4.8','x2_10^3.11', 'x4_10^3.1', 'x4_10^3', 'x3_10^3.6'])\n",
        "\n",
        "\n",
        "display(pcr_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X1_10^5</th>\n",
              "      <th>X1_10^5.1</th>\n",
              "      <th>X1_10^5.2</th>\n",
              "      <th>X1_10^5.3</th>\n",
              "      <th>X1_10^5.4</th>\n",
              "      <th>X1_10^5.5</th>\n",
              "      <th>x2_10^5</th>\n",
              "      <th>x2_10^5.1</th>\n",
              "      <th>x2_10^5.2</th>\n",
              "      <th>x2_10^5.3</th>\n",
              "      <th>x2_10^5.4</th>\n",
              "      <th>x2_10^5.5</th>\n",
              "      <th>x3_10^5</th>\n",
              "      <th>x3_10^5.1</th>\n",
              "      <th>x3_10^5.2</th>\n",
              "      <th>x3_10^5.3</th>\n",
              "      <th>x3_10^5.4</th>\n",
              "      <th>x3_10^5.5</th>\n",
              "      <th>x4_10^5</th>\n",
              "      <th>x4_10^5.1</th>\n",
              "      <th>x4_10^5.2</th>\n",
              "      <th>x4_10^5.3</th>\n",
              "      <th>x4_10^5.4</th>\n",
              "      <th>x4_10^5.5</th>\n",
              "      <th>x1_10^4</th>\n",
              "      <th>x1_10^4.1</th>\n",
              "      <th>x1_10^4.2</th>\n",
              "      <th>x1_10^4.3</th>\n",
              "      <th>x1_10^4.4</th>\n",
              "      <th>x1_10^4.5</th>\n",
              "      <th>x1_10^4.6</th>\n",
              "      <th>x1_10^4.7</th>\n",
              "      <th>x1_10^4.8</th>\n",
              "      <th>x1_10^4.9</th>\n",
              "      <th>x1_10^4.10</th>\n",
              "      <th>x1_10^4.11</th>\n",
              "      <th>x1_10^4.12</th>\n",
              "      <th>x1_10^4.13</th>\n",
              "      <th>x1_10^4.14</th>\n",
              "      <th>x1_10^4.15</th>\n",
              "      <th>...</th>\n",
              "      <th>x1_10^3.10</th>\n",
              "      <th>x1_10^3.11</th>\n",
              "      <th>x1_10^3.13</th>\n",
              "      <th>x2_10^3</th>\n",
              "      <th>x2_10^3.1</th>\n",
              "      <th>x2_10^3.2</th>\n",
              "      <th>x2_10^3.3</th>\n",
              "      <th>x2_10^3.4</th>\n",
              "      <th>x2_10^3.5</th>\n",
              "      <th>x2_10^3.6</th>\n",
              "      <th>x2_10^3.7</th>\n",
              "      <th>x2_10^3.8</th>\n",
              "      <th>x2_10^3.9</th>\n",
              "      <th>x2_10^3.10</th>\n",
              "      <th>x2_10^3.12</th>\n",
              "      <th>x2_10^3.13</th>\n",
              "      <th>x3_10^3</th>\n",
              "      <th>x3_10^3.1</th>\n",
              "      <th>x3_10^3.3</th>\n",
              "      <th>x3_10^3.4</th>\n",
              "      <th>x3_10^3.5</th>\n",
              "      <th>x3_10^3.7</th>\n",
              "      <th>x3_10^3.8</th>\n",
              "      <th>x3_10^3.9</th>\n",
              "      <th>x3_10^3.10</th>\n",
              "      <th>x3_10^3.11</th>\n",
              "      <th>x3_10^3.12</th>\n",
              "      <th>x3_10^3.13</th>\n",
              "      <th>x4_10^3.2</th>\n",
              "      <th>x4_10^3.3</th>\n",
              "      <th>x4_10^3.4</th>\n",
              "      <th>x4_10^3.5</th>\n",
              "      <th>x4_10^3.6</th>\n",
              "      <th>x4_10^3.7</th>\n",
              "      <th>x4_10^3.8</th>\n",
              "      <th>x4_10^3.9</th>\n",
              "      <th>x4_10^3.10</th>\n",
              "      <th>x4_10^3.11</th>\n",
              "      <th>x4_10^3.12</th>\n",
              "      <th>x4_10^3.13</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2091</td>\n",
              "      <td>2105</td>\n",
              "      <td>1953</td>\n",
              "      <td>1811</td>\n",
              "      <td>2048</td>\n",
              "      <td>1996</td>\n",
              "      <td>2217</td>\n",
              "      <td>2073</td>\n",
              "      <td>2091</td>\n",
              "      <td>2534</td>\n",
              "      <td>2247</td>\n",
              "      <td>2313</td>\n",
              "      <td>1828</td>\n",
              "      <td>1811</td>\n",
              "      <td>1725</td>\n",
              "      <td>1905</td>\n",
              "      <td>1805</td>\n",
              "      <td>1561</td>\n",
              "      <td>2206</td>\n",
              "      <td>2249</td>\n",
              "      <td>2323</td>\n",
              "      <td>2273</td>\n",
              "      <td>2443</td>\n",
              "      <td>2230</td>\n",
              "      <td>1967</td>\n",
              "      <td>1997</td>\n",
              "      <td>1633</td>\n",
              "      <td>1793</td>\n",
              "      <td>1339</td>\n",
              "      <td>2241</td>\n",
              "      <td>2411</td>\n",
              "      <td>2418</td>\n",
              "      <td>1736</td>\n",
              "      <td>1940</td>\n",
              "      <td>1860</td>\n",
              "      <td>1903</td>\n",
              "      <td>1779</td>\n",
              "      <td>1979</td>\n",
              "      <td>1922</td>\n",
              "      <td>2191</td>\n",
              "      <td>...</td>\n",
              "      <td>2209</td>\n",
              "      <td>2178</td>\n",
              "      <td>2107</td>\n",
              "      <td>2193</td>\n",
              "      <td>2225</td>\n",
              "      <td>2104</td>\n",
              "      <td>2475</td>\n",
              "      <td>2023</td>\n",
              "      <td>2261</td>\n",
              "      <td>2691</td>\n",
              "      <td>2340</td>\n",
              "      <td>2543</td>\n",
              "      <td>2489</td>\n",
              "      <td>2442</td>\n",
              "      <td>2653</td>\n",
              "      <td>2212</td>\n",
              "      <td>1890</td>\n",
              "      <td>1964</td>\n",
              "      <td>1775</td>\n",
              "      <td>1922</td>\n",
              "      <td>1836</td>\n",
              "      <td>1957</td>\n",
              "      <td>2068</td>\n",
              "      <td>2108</td>\n",
              "      <td>2055</td>\n",
              "      <td>1849</td>\n",
              "      <td>2349</td>\n",
              "      <td>1921</td>\n",
              "      <td>2450</td>\n",
              "      <td>2306</td>\n",
              "      <td>2382</td>\n",
              "      <td>2348</td>\n",
              "      <td>2451</td>\n",
              "      <td>2294</td>\n",
              "      <td>2761</td>\n",
              "      <td>2368</td>\n",
              "      <td>2754</td>\n",
              "      <td>2598</td>\n",
              "      <td>2913</td>\n",
              "      <td>2494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2077</td>\n",
              "      <td>2094</td>\n",
              "      <td>1939</td>\n",
              "      <td>1810</td>\n",
              "      <td>2039</td>\n",
              "      <td>1981</td>\n",
              "      <td>2242</td>\n",
              "      <td>2053</td>\n",
              "      <td>2065</td>\n",
              "      <td>2514</td>\n",
              "      <td>2248</td>\n",
              "      <td>2275</td>\n",
              "      <td>1828</td>\n",
              "      <td>1804</td>\n",
              "      <td>1726</td>\n",
              "      <td>1911</td>\n",
              "      <td>1800</td>\n",
              "      <td>1553</td>\n",
              "      <td>2181</td>\n",
              "      <td>2225</td>\n",
              "      <td>2296</td>\n",
              "      <td>2241</td>\n",
              "      <td>2429</td>\n",
              "      <td>2215</td>\n",
              "      <td>1957</td>\n",
              "      <td>1973</td>\n",
              "      <td>1636</td>\n",
              "      <td>1777</td>\n",
              "      <td>1338</td>\n",
              "      <td>2227</td>\n",
              "      <td>2382</td>\n",
              "      <td>2402</td>\n",
              "      <td>1731</td>\n",
              "      <td>1904</td>\n",
              "      <td>1849</td>\n",
              "      <td>1878</td>\n",
              "      <td>1760</td>\n",
              "      <td>1959</td>\n",
              "      <td>1910</td>\n",
              "      <td>2164</td>\n",
              "      <td>...</td>\n",
              "      <td>2156</td>\n",
              "      <td>2159</td>\n",
              "      <td>2098</td>\n",
              "      <td>2184</td>\n",
              "      <td>2216</td>\n",
              "      <td>2093</td>\n",
              "      <td>2423</td>\n",
              "      <td>2017</td>\n",
              "      <td>2210</td>\n",
              "      <td>2551</td>\n",
              "      <td>2319</td>\n",
              "      <td>2497</td>\n",
              "      <td>2457</td>\n",
              "      <td>2412</td>\n",
              "      <td>2609</td>\n",
              "      <td>2196</td>\n",
              "      <td>1907</td>\n",
              "      <td>1941</td>\n",
              "      <td>1783</td>\n",
              "      <td>1902</td>\n",
              "      <td>1825</td>\n",
              "      <td>1927</td>\n",
              "      <td>2060</td>\n",
              "      <td>2092</td>\n",
              "      <td>2040</td>\n",
              "      <td>1839</td>\n",
              "      <td>2343</td>\n",
              "      <td>1897</td>\n",
              "      <td>2441</td>\n",
              "      <td>2289</td>\n",
              "      <td>2357</td>\n",
              "      <td>2347</td>\n",
              "      <td>2418</td>\n",
              "      <td>2288</td>\n",
              "      <td>2740</td>\n",
              "      <td>2352</td>\n",
              "      <td>2750</td>\n",
              "      <td>2567</td>\n",
              "      <td>2851</td>\n",
              "      <td>2484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2068</td>\n",
              "      <td>2085</td>\n",
              "      <td>1916</td>\n",
              "      <td>1806</td>\n",
              "      <td>2022</td>\n",
              "      <td>1982</td>\n",
              "      <td>2238</td>\n",
              "      <td>2040</td>\n",
              "      <td>2051</td>\n",
              "      <td>2478</td>\n",
              "      <td>2251</td>\n",
              "      <td>2263</td>\n",
              "      <td>1825</td>\n",
              "      <td>1787</td>\n",
              "      <td>1716</td>\n",
              "      <td>1912</td>\n",
              "      <td>1767</td>\n",
              "      <td>1545</td>\n",
              "      <td>2175</td>\n",
              "      <td>2232</td>\n",
              "      <td>2287</td>\n",
              "      <td>2237</td>\n",
              "      <td>2426</td>\n",
              "      <td>2218</td>\n",
              "      <td>1949</td>\n",
              "      <td>1993</td>\n",
              "      <td>1676</td>\n",
              "      <td>1766</td>\n",
              "      <td>1334</td>\n",
              "      <td>2223</td>\n",
              "      <td>2367</td>\n",
              "      <td>2381</td>\n",
              "      <td>1734</td>\n",
              "      <td>1893</td>\n",
              "      <td>1834</td>\n",
              "      <td>1867</td>\n",
              "      <td>1752</td>\n",
              "      <td>1938</td>\n",
              "      <td>1895</td>\n",
              "      <td>2150</td>\n",
              "      <td>...</td>\n",
              "      <td>2156</td>\n",
              "      <td>2145</td>\n",
              "      <td>2094</td>\n",
              "      <td>2176</td>\n",
              "      <td>2194</td>\n",
              "      <td>2074</td>\n",
              "      <td>2391</td>\n",
              "      <td>2009</td>\n",
              "      <td>2191</td>\n",
              "      <td>2505</td>\n",
              "      <td>2305</td>\n",
              "      <td>2484</td>\n",
              "      <td>2437</td>\n",
              "      <td>2390</td>\n",
              "      <td>2610</td>\n",
              "      <td>2190</td>\n",
              "      <td>1904</td>\n",
              "      <td>1914</td>\n",
              "      <td>1785</td>\n",
              "      <td>1902</td>\n",
              "      <td>1820</td>\n",
              "      <td>1911</td>\n",
              "      <td>2050</td>\n",
              "      <td>2090</td>\n",
              "      <td>2021</td>\n",
              "      <td>1833</td>\n",
              "      <td>2346</td>\n",
              "      <td>1897</td>\n",
              "      <td>2426</td>\n",
              "      <td>2276</td>\n",
              "      <td>2342</td>\n",
              "      <td>2355</td>\n",
              "      <td>2418</td>\n",
              "      <td>2273</td>\n",
              "      <td>2732</td>\n",
              "      <td>2327</td>\n",
              "      <td>2743</td>\n",
              "      <td>2550</td>\n",
              "      <td>2825</td>\n",
              "      <td>2477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2056</td>\n",
              "      <td>2065</td>\n",
              "      <td>1908</td>\n",
              "      <td>1797</td>\n",
              "      <td>2022</td>\n",
              "      <td>1978</td>\n",
              "      <td>2229</td>\n",
              "      <td>2023</td>\n",
              "      <td>2041</td>\n",
              "      <td>2488</td>\n",
              "      <td>2244</td>\n",
              "      <td>2250</td>\n",
              "      <td>1817</td>\n",
              "      <td>1770</td>\n",
              "      <td>1695</td>\n",
              "      <td>1905</td>\n",
              "      <td>1753</td>\n",
              "      <td>1535</td>\n",
              "      <td>2173</td>\n",
              "      <td>2227</td>\n",
              "      <td>2278</td>\n",
              "      <td>2233</td>\n",
              "      <td>2421</td>\n",
              "      <td>2202</td>\n",
              "      <td>1943</td>\n",
              "      <td>1956</td>\n",
              "      <td>1631</td>\n",
              "      <td>1757</td>\n",
              "      <td>1333</td>\n",
              "      <td>2215</td>\n",
              "      <td>2362</td>\n",
              "      <td>2389</td>\n",
              "      <td>1721</td>\n",
              "      <td>1885</td>\n",
              "      <td>1829</td>\n",
              "      <td>1856</td>\n",
              "      <td>1735</td>\n",
              "      <td>1926</td>\n",
              "      <td>1898</td>\n",
              "      <td>2126</td>\n",
              "      <td>...</td>\n",
              "      <td>2143</td>\n",
              "      <td>2143</td>\n",
              "      <td>2075</td>\n",
              "      <td>2169</td>\n",
              "      <td>2186</td>\n",
              "      <td>2071</td>\n",
              "      <td>2364</td>\n",
              "      <td>2003</td>\n",
              "      <td>2201</td>\n",
              "      <td>2468</td>\n",
              "      <td>2294</td>\n",
              "      <td>2474</td>\n",
              "      <td>2425</td>\n",
              "      <td>2362</td>\n",
              "      <td>2595</td>\n",
              "      <td>2173</td>\n",
              "      <td>1903</td>\n",
              "      <td>1914</td>\n",
              "      <td>1794</td>\n",
              "      <td>1883</td>\n",
              "      <td>1817</td>\n",
              "      <td>1902</td>\n",
              "      <td>2047</td>\n",
              "      <td>2079</td>\n",
              "      <td>2014</td>\n",
              "      <td>1821</td>\n",
              "      <td>2358</td>\n",
              "      <td>1887</td>\n",
              "      <td>2416</td>\n",
              "      <td>2270</td>\n",
              "      <td>2339</td>\n",
              "      <td>2358</td>\n",
              "      <td>2419</td>\n",
              "      <td>2265</td>\n",
              "      <td>2722</td>\n",
              "      <td>2315</td>\n",
              "      <td>2728</td>\n",
              "      <td>2540</td>\n",
              "      <td>2798</td>\n",
              "      <td>2471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2053</td>\n",
              "      <td>2056</td>\n",
              "      <td>1895</td>\n",
              "      <td>1798</td>\n",
              "      <td>2015</td>\n",
              "      <td>1975</td>\n",
              "      <td>2224</td>\n",
              "      <td>2010</td>\n",
              "      <td>2040</td>\n",
              "      <td>2463</td>\n",
              "      <td>2239</td>\n",
              "      <td>2241</td>\n",
              "      <td>1802</td>\n",
              "      <td>1760</td>\n",
              "      <td>1679</td>\n",
              "      <td>1881</td>\n",
              "      <td>1786</td>\n",
              "      <td>1537</td>\n",
              "      <td>2169</td>\n",
              "      <td>2224</td>\n",
              "      <td>2271</td>\n",
              "      <td>2228</td>\n",
              "      <td>2405</td>\n",
              "      <td>2200</td>\n",
              "      <td>1939</td>\n",
              "      <td>1938</td>\n",
              "      <td>1621</td>\n",
              "      <td>1749</td>\n",
              "      <td>1329</td>\n",
              "      <td>2208</td>\n",
              "      <td>2352</td>\n",
              "      <td>2373</td>\n",
              "      <td>1718</td>\n",
              "      <td>1871</td>\n",
              "      <td>1818</td>\n",
              "      <td>1851</td>\n",
              "      <td>1725</td>\n",
              "      <td>1911</td>\n",
              "      <td>1894</td>\n",
              "      <td>2127</td>\n",
              "      <td>...</td>\n",
              "      <td>2139</td>\n",
              "      <td>2136</td>\n",
              "      <td>2068</td>\n",
              "      <td>2165</td>\n",
              "      <td>2190</td>\n",
              "      <td>2049</td>\n",
              "      <td>2366</td>\n",
              "      <td>1997</td>\n",
              "      <td>2174</td>\n",
              "      <td>2421</td>\n",
              "      <td>2287</td>\n",
              "      <td>2477</td>\n",
              "      <td>2404</td>\n",
              "      <td>2316</td>\n",
              "      <td>2565</td>\n",
              "      <td>2159</td>\n",
              "      <td>1884</td>\n",
              "      <td>1938</td>\n",
              "      <td>1794</td>\n",
              "      <td>1865</td>\n",
              "      <td>1813</td>\n",
              "      <td>1904</td>\n",
              "      <td>2042</td>\n",
              "      <td>2075</td>\n",
              "      <td>1992</td>\n",
              "      <td>1811</td>\n",
              "      <td>2370</td>\n",
              "      <td>1876</td>\n",
              "      <td>2413</td>\n",
              "      <td>2251</td>\n",
              "      <td>2319</td>\n",
              "      <td>2375</td>\n",
              "      <td>2411</td>\n",
              "      <td>2261</td>\n",
              "      <td>2716</td>\n",
              "      <td>2313</td>\n",
              "      <td>2712</td>\n",
              "      <td>2539</td>\n",
              "      <td>2776</td>\n",
              "      <td>2445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2037</td>\n",
              "      <td>2051</td>\n",
              "      <td>1883</td>\n",
              "      <td>1795</td>\n",
              "      <td>2013</td>\n",
              "      <td>1967</td>\n",
              "      <td>2216</td>\n",
              "      <td>2009</td>\n",
              "      <td>2035</td>\n",
              "      <td>2452</td>\n",
              "      <td>2243</td>\n",
              "      <td>2237</td>\n",
              "      <td>1805</td>\n",
              "      <td>1779</td>\n",
              "      <td>1679</td>\n",
              "      <td>1892</td>\n",
              "      <td>1780</td>\n",
              "      <td>1526</td>\n",
              "      <td>2161</td>\n",
              "      <td>2208</td>\n",
              "      <td>2248</td>\n",
              "      <td>2234</td>\n",
              "      <td>2409</td>\n",
              "      <td>2175</td>\n",
              "      <td>1942</td>\n",
              "      <td>1941</td>\n",
              "      <td>1637</td>\n",
              "      <td>1751</td>\n",
              "      <td>1323</td>\n",
              "      <td>2194</td>\n",
              "      <td>2343</td>\n",
              "      <td>2377</td>\n",
              "      <td>1711</td>\n",
              "      <td>1874</td>\n",
              "      <td>1807</td>\n",
              "      <td>1844</td>\n",
              "      <td>1719</td>\n",
              "      <td>1909</td>\n",
              "      <td>1882</td>\n",
              "      <td>2120</td>\n",
              "      <td>...</td>\n",
              "      <td>2134</td>\n",
              "      <td>2123</td>\n",
              "      <td>2080</td>\n",
              "      <td>2160</td>\n",
              "      <td>2189</td>\n",
              "      <td>2045</td>\n",
              "      <td>2384</td>\n",
              "      <td>1990</td>\n",
              "      <td>2178</td>\n",
              "      <td>2399</td>\n",
              "      <td>2274</td>\n",
              "      <td>2450</td>\n",
              "      <td>2393</td>\n",
              "      <td>2293</td>\n",
              "      <td>2553</td>\n",
              "      <td>2160</td>\n",
              "      <td>1889</td>\n",
              "      <td>1925</td>\n",
              "      <td>1791</td>\n",
              "      <td>1860</td>\n",
              "      <td>1802</td>\n",
              "      <td>1910</td>\n",
              "      <td>2034</td>\n",
              "      <td>2060</td>\n",
              "      <td>1981</td>\n",
              "      <td>1810</td>\n",
              "      <td>2367</td>\n",
              "      <td>1872</td>\n",
              "      <td>2416</td>\n",
              "      <td>2258</td>\n",
              "      <td>2316</td>\n",
              "      <td>2356</td>\n",
              "      <td>2402</td>\n",
              "      <td>2246</td>\n",
              "      <td>2701</td>\n",
              "      <td>2299</td>\n",
              "      <td>2691</td>\n",
              "      <td>2532</td>\n",
              "      <td>2756</td>\n",
              "      <td>2440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2039</td>\n",
              "      <td>2058</td>\n",
              "      <td>1881</td>\n",
              "      <td>1792</td>\n",
              "      <td>2007</td>\n",
              "      <td>1962</td>\n",
              "      <td>2217</td>\n",
              "      <td>2002</td>\n",
              "      <td>2025</td>\n",
              "      <td>2447</td>\n",
              "      <td>2237</td>\n",
              "      <td>2226</td>\n",
              "      <td>1790</td>\n",
              "      <td>1766</td>\n",
              "      <td>1683</td>\n",
              "      <td>1880</td>\n",
              "      <td>1769</td>\n",
              "      <td>1532</td>\n",
              "      <td>2153</td>\n",
              "      <td>2213</td>\n",
              "      <td>2247</td>\n",
              "      <td>2234</td>\n",
              "      <td>2410</td>\n",
              "      <td>2174</td>\n",
              "      <td>1940</td>\n",
              "      <td>1935</td>\n",
              "      <td>1615</td>\n",
              "      <td>1754</td>\n",
              "      <td>1322</td>\n",
              "      <td>2183</td>\n",
              "      <td>2334</td>\n",
              "      <td>2384</td>\n",
              "      <td>1703</td>\n",
              "      <td>1868</td>\n",
              "      <td>1807</td>\n",
              "      <td>1837</td>\n",
              "      <td>1707</td>\n",
              "      <td>1903</td>\n",
              "      <td>1872</td>\n",
              "      <td>2117</td>\n",
              "      <td>...</td>\n",
              "      <td>2128</td>\n",
              "      <td>2124</td>\n",
              "      <td>2077</td>\n",
              "      <td>2156</td>\n",
              "      <td>2189</td>\n",
              "      <td>2040</td>\n",
              "      <td>2370</td>\n",
              "      <td>1983</td>\n",
              "      <td>2173</td>\n",
              "      <td>2384</td>\n",
              "      <td>2278</td>\n",
              "      <td>2459</td>\n",
              "      <td>2389</td>\n",
              "      <td>2278</td>\n",
              "      <td>2547</td>\n",
              "      <td>2159</td>\n",
              "      <td>1897</td>\n",
              "      <td>1928</td>\n",
              "      <td>1791</td>\n",
              "      <td>1866</td>\n",
              "      <td>1796</td>\n",
              "      <td>1918</td>\n",
              "      <td>2024</td>\n",
              "      <td>2061</td>\n",
              "      <td>1981</td>\n",
              "      <td>1802</td>\n",
              "      <td>2358</td>\n",
              "      <td>1863</td>\n",
              "      <td>2420</td>\n",
              "      <td>2260</td>\n",
              "      <td>2318</td>\n",
              "      <td>2370</td>\n",
              "      <td>2394</td>\n",
              "      <td>2249</td>\n",
              "      <td>2694</td>\n",
              "      <td>2285</td>\n",
              "      <td>2701</td>\n",
              "      <td>2523</td>\n",
              "      <td>2739</td>\n",
              "      <td>2434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2033</td>\n",
              "      <td>2066</td>\n",
              "      <td>1876</td>\n",
              "      <td>1787</td>\n",
              "      <td>2010</td>\n",
              "      <td>1957</td>\n",
              "      <td>2226</td>\n",
              "      <td>1998</td>\n",
              "      <td>2026</td>\n",
              "      <td>2454</td>\n",
              "      <td>2245</td>\n",
              "      <td>2214</td>\n",
              "      <td>1801</td>\n",
              "      <td>1779</td>\n",
              "      <td>1688</td>\n",
              "      <td>1892</td>\n",
              "      <td>1750</td>\n",
              "      <td>1532</td>\n",
              "      <td>2143</td>\n",
              "      <td>2213</td>\n",
              "      <td>2245</td>\n",
              "      <td>2242</td>\n",
              "      <td>2412</td>\n",
              "      <td>2168</td>\n",
              "      <td>1936</td>\n",
              "      <td>1956</td>\n",
              "      <td>1622</td>\n",
              "      <td>1758</td>\n",
              "      <td>1318</td>\n",
              "      <td>2176</td>\n",
              "      <td>2326</td>\n",
              "      <td>2370</td>\n",
              "      <td>1694</td>\n",
              "      <td>1862</td>\n",
              "      <td>1804</td>\n",
              "      <td>1830</td>\n",
              "      <td>1706</td>\n",
              "      <td>1895</td>\n",
              "      <td>1869</td>\n",
              "      <td>2114</td>\n",
              "      <td>...</td>\n",
              "      <td>2124</td>\n",
              "      <td>2116</td>\n",
              "      <td>2075</td>\n",
              "      <td>2162</td>\n",
              "      <td>2191</td>\n",
              "      <td>2043</td>\n",
              "      <td>2374</td>\n",
              "      <td>1985</td>\n",
              "      <td>2173</td>\n",
              "      <td>2357</td>\n",
              "      <td>2273</td>\n",
              "      <td>2458</td>\n",
              "      <td>2364</td>\n",
              "      <td>2275</td>\n",
              "      <td>2544</td>\n",
              "      <td>2162</td>\n",
              "      <td>1884</td>\n",
              "      <td>1925</td>\n",
              "      <td>1781</td>\n",
              "      <td>1859</td>\n",
              "      <td>1785</td>\n",
              "      <td>1925</td>\n",
              "      <td>2010</td>\n",
              "      <td>2051</td>\n",
              "      <td>1965</td>\n",
              "      <td>1803</td>\n",
              "      <td>2363</td>\n",
              "      <td>1856</td>\n",
              "      <td>2415</td>\n",
              "      <td>2263</td>\n",
              "      <td>2320</td>\n",
              "      <td>2377</td>\n",
              "      <td>2391</td>\n",
              "      <td>2245</td>\n",
              "      <td>2692</td>\n",
              "      <td>2285</td>\n",
              "      <td>2710</td>\n",
              "      <td>2515</td>\n",
              "      <td>2727</td>\n",
              "      <td>2430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2028</td>\n",
              "      <td>2062</td>\n",
              "      <td>1872</td>\n",
              "      <td>1782</td>\n",
              "      <td>2012</td>\n",
              "      <td>1954</td>\n",
              "      <td>2214</td>\n",
              "      <td>1992</td>\n",
              "      <td>2018</td>\n",
              "      <td>2455</td>\n",
              "      <td>2238</td>\n",
              "      <td>2217</td>\n",
              "      <td>1815</td>\n",
              "      <td>1773</td>\n",
              "      <td>1676</td>\n",
              "      <td>1890</td>\n",
              "      <td>1767</td>\n",
              "      <td>1525</td>\n",
              "      <td>2148</td>\n",
              "      <td>2197</td>\n",
              "      <td>2250</td>\n",
              "      <td>2243</td>\n",
              "      <td>2409</td>\n",
              "      <td>2168</td>\n",
              "      <td>1939</td>\n",
              "      <td>1938</td>\n",
              "      <td>1649</td>\n",
              "      <td>1760</td>\n",
              "      <td>1321</td>\n",
              "      <td>2174</td>\n",
              "      <td>2327</td>\n",
              "      <td>2362</td>\n",
              "      <td>1693</td>\n",
              "      <td>1866</td>\n",
              "      <td>1801</td>\n",
              "      <td>1823</td>\n",
              "      <td>1697</td>\n",
              "      <td>1895</td>\n",
              "      <td>1867</td>\n",
              "      <td>2109</td>\n",
              "      <td>...</td>\n",
              "      <td>2098</td>\n",
              "      <td>2109</td>\n",
              "      <td>2063</td>\n",
              "      <td>2158</td>\n",
              "      <td>2192</td>\n",
              "      <td>2032</td>\n",
              "      <td>2374</td>\n",
              "      <td>1981</td>\n",
              "      <td>2176</td>\n",
              "      <td>2350</td>\n",
              "      <td>2274</td>\n",
              "      <td>2451</td>\n",
              "      <td>2354</td>\n",
              "      <td>2267</td>\n",
              "      <td>2540</td>\n",
              "      <td>2166</td>\n",
              "      <td>1894</td>\n",
              "      <td>1909</td>\n",
              "      <td>1787</td>\n",
              "      <td>1858</td>\n",
              "      <td>1786</td>\n",
              "      <td>1914</td>\n",
              "      <td>2021</td>\n",
              "      <td>2043</td>\n",
              "      <td>1955</td>\n",
              "      <td>1803</td>\n",
              "      <td>2356</td>\n",
              "      <td>1854</td>\n",
              "      <td>2420</td>\n",
              "      <td>2251</td>\n",
              "      <td>2311</td>\n",
              "      <td>2395</td>\n",
              "      <td>2406</td>\n",
              "      <td>2246</td>\n",
              "      <td>2691</td>\n",
              "      <td>2276</td>\n",
              "      <td>2727</td>\n",
              "      <td>2514</td>\n",
              "      <td>2722</td>\n",
              "      <td>2417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2025</td>\n",
              "      <td>2061</td>\n",
              "      <td>1871</td>\n",
              "      <td>1780</td>\n",
              "      <td>2012</td>\n",
              "      <td>1952</td>\n",
              "      <td>2215</td>\n",
              "      <td>1996</td>\n",
              "      <td>2017</td>\n",
              "      <td>2457</td>\n",
              "      <td>2238</td>\n",
              "      <td>2219</td>\n",
              "      <td>1785</td>\n",
              "      <td>1774</td>\n",
              "      <td>1686</td>\n",
              "      <td>1895</td>\n",
              "      <td>1760</td>\n",
              "      <td>1530</td>\n",
              "      <td>2144</td>\n",
              "      <td>2183</td>\n",
              "      <td>2254</td>\n",
              "      <td>2250</td>\n",
              "      <td>2406</td>\n",
              "      <td>2158</td>\n",
              "      <td>1935</td>\n",
              "      <td>1962</td>\n",
              "      <td>1674</td>\n",
              "      <td>1767</td>\n",
              "      <td>1321</td>\n",
              "      <td>2167</td>\n",
              "      <td>2322</td>\n",
              "      <td>2359</td>\n",
              "      <td>1688</td>\n",
              "      <td>1860</td>\n",
              "      <td>1795</td>\n",
              "      <td>1817</td>\n",
              "      <td>1698</td>\n",
              "      <td>1888</td>\n",
              "      <td>1861</td>\n",
              "      <td>2106</td>\n",
              "      <td>...</td>\n",
              "      <td>2109</td>\n",
              "      <td>2108</td>\n",
              "      <td>2066</td>\n",
              "      <td>2163</td>\n",
              "      <td>2193</td>\n",
              "      <td>2030</td>\n",
              "      <td>2378</td>\n",
              "      <td>1980</td>\n",
              "      <td>2177</td>\n",
              "      <td>2329</td>\n",
              "      <td>2276</td>\n",
              "      <td>2442</td>\n",
              "      <td>2344</td>\n",
              "      <td>2263</td>\n",
              "      <td>2539</td>\n",
              "      <td>2162</td>\n",
              "      <td>1892</td>\n",
              "      <td>1919</td>\n",
              "      <td>1786</td>\n",
              "      <td>1856</td>\n",
              "      <td>1785</td>\n",
              "      <td>1919</td>\n",
              "      <td>2020</td>\n",
              "      <td>2040</td>\n",
              "      <td>1948</td>\n",
              "      <td>1801</td>\n",
              "      <td>2361</td>\n",
              "      <td>1860</td>\n",
              "      <td>2426</td>\n",
              "      <td>2256</td>\n",
              "      <td>2316</td>\n",
              "      <td>2413</td>\n",
              "      <td>2405</td>\n",
              "      <td>2246</td>\n",
              "      <td>2687</td>\n",
              "      <td>2277</td>\n",
              "      <td>2730</td>\n",
              "      <td>2512</td>\n",
              "      <td>2720</td>\n",
              "      <td>2418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2023</td>\n",
              "      <td>2054</td>\n",
              "      <td>1870</td>\n",
              "      <td>1775</td>\n",
              "      <td>2011</td>\n",
              "      <td>1950</td>\n",
              "      <td>2219</td>\n",
              "      <td>1991</td>\n",
              "      <td>2020</td>\n",
              "      <td>2451</td>\n",
              "      <td>2232</td>\n",
              "      <td>2220</td>\n",
              "      <td>1812</td>\n",
              "      <td>1768</td>\n",
              "      <td>1671</td>\n",
              "      <td>1886</td>\n",
              "      <td>1759</td>\n",
              "      <td>1524</td>\n",
              "      <td>2153</td>\n",
              "      <td>2202</td>\n",
              "      <td>2245</td>\n",
              "      <td>2251</td>\n",
              "      <td>2405</td>\n",
              "      <td>2148</td>\n",
              "      <td>1941</td>\n",
              "      <td>1954</td>\n",
              "      <td>1647</td>\n",
              "      <td>1772</td>\n",
              "      <td>1318</td>\n",
              "      <td>2166</td>\n",
              "      <td>2317</td>\n",
              "      <td>2346</td>\n",
              "      <td>1679</td>\n",
              "      <td>1869</td>\n",
              "      <td>1794</td>\n",
              "      <td>1810</td>\n",
              "      <td>1695</td>\n",
              "      <td>1885</td>\n",
              "      <td>1858</td>\n",
              "      <td>2109</td>\n",
              "      <td>...</td>\n",
              "      <td>2095</td>\n",
              "      <td>2105</td>\n",
              "      <td>2065</td>\n",
              "      <td>2162</td>\n",
              "      <td>2198</td>\n",
              "      <td>2032</td>\n",
              "      <td>2383</td>\n",
              "      <td>1982</td>\n",
              "      <td>2178</td>\n",
              "      <td>2330</td>\n",
              "      <td>2276</td>\n",
              "      <td>2445</td>\n",
              "      <td>2337</td>\n",
              "      <td>2256</td>\n",
              "      <td>2541</td>\n",
              "      <td>2177</td>\n",
              "      <td>1907</td>\n",
              "      <td>1914</td>\n",
              "      <td>1788</td>\n",
              "      <td>1863</td>\n",
              "      <td>1773</td>\n",
              "      <td>1922</td>\n",
              "      <td>2017</td>\n",
              "      <td>2028</td>\n",
              "      <td>1942</td>\n",
              "      <td>1796</td>\n",
              "      <td>2368</td>\n",
              "      <td>1854</td>\n",
              "      <td>2427</td>\n",
              "      <td>2247</td>\n",
              "      <td>2319</td>\n",
              "      <td>2403</td>\n",
              "      <td>2405</td>\n",
              "      <td>2251</td>\n",
              "      <td>2679</td>\n",
              "      <td>2271</td>\n",
              "      <td>2727</td>\n",
              "      <td>2512</td>\n",
              "      <td>2719</td>\n",
              "      <td>2408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2021</td>\n",
              "      <td>2056</td>\n",
              "      <td>1868</td>\n",
              "      <td>1770</td>\n",
              "      <td>2012</td>\n",
              "      <td>1948</td>\n",
              "      <td>2218</td>\n",
              "      <td>1993</td>\n",
              "      <td>2022</td>\n",
              "      <td>2450</td>\n",
              "      <td>2236</td>\n",
              "      <td>2223</td>\n",
              "      <td>1817</td>\n",
              "      <td>1771</td>\n",
              "      <td>1685</td>\n",
              "      <td>1897</td>\n",
              "      <td>1747</td>\n",
              "      <td>1530</td>\n",
              "      <td>2153</td>\n",
              "      <td>2197</td>\n",
              "      <td>2252</td>\n",
              "      <td>2255</td>\n",
              "      <td>2404</td>\n",
              "      <td>2145</td>\n",
              "      <td>1944</td>\n",
              "      <td>1930</td>\n",
              "      <td>1667</td>\n",
              "      <td>1774</td>\n",
              "      <td>1319</td>\n",
              "      <td>2161</td>\n",
              "      <td>2311</td>\n",
              "      <td>2357</td>\n",
              "      <td>1683</td>\n",
              "      <td>1868</td>\n",
              "      <td>1796</td>\n",
              "      <td>1814</td>\n",
              "      <td>1696</td>\n",
              "      <td>1877</td>\n",
              "      <td>1855</td>\n",
              "      <td>2114</td>\n",
              "      <td>...</td>\n",
              "      <td>2103</td>\n",
              "      <td>2104</td>\n",
              "      <td>2061</td>\n",
              "      <td>2165</td>\n",
              "      <td>2200</td>\n",
              "      <td>2032</td>\n",
              "      <td>2389</td>\n",
              "      <td>1980</td>\n",
              "      <td>2184</td>\n",
              "      <td>2338</td>\n",
              "      <td>2273</td>\n",
              "      <td>2446</td>\n",
              "      <td>2339</td>\n",
              "      <td>2249</td>\n",
              "      <td>2538</td>\n",
              "      <td>2185</td>\n",
              "      <td>1904</td>\n",
              "      <td>1897</td>\n",
              "      <td>1793</td>\n",
              "      <td>1859</td>\n",
              "      <td>1777</td>\n",
              "      <td>1936</td>\n",
              "      <td>2018</td>\n",
              "      <td>2017</td>\n",
              "      <td>1947</td>\n",
              "      <td>1797</td>\n",
              "      <td>2371</td>\n",
              "      <td>1847</td>\n",
              "      <td>2432</td>\n",
              "      <td>2254</td>\n",
              "      <td>2315</td>\n",
              "      <td>2416</td>\n",
              "      <td>2402</td>\n",
              "      <td>2254</td>\n",
              "      <td>2668</td>\n",
              "      <td>2274</td>\n",
              "      <td>2728</td>\n",
              "      <td>2502</td>\n",
              "      <td>2724</td>\n",
              "      <td>2404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2033</td>\n",
              "      <td>2050</td>\n",
              "      <td>1875</td>\n",
              "      <td>1773</td>\n",
              "      <td>2014</td>\n",
              "      <td>1948</td>\n",
              "      <td>2212</td>\n",
              "      <td>1998</td>\n",
              "      <td>2019</td>\n",
              "      <td>2452</td>\n",
              "      <td>2240</td>\n",
              "      <td>2228</td>\n",
              "      <td>1811</td>\n",
              "      <td>1741</td>\n",
              "      <td>1681</td>\n",
              "      <td>1913</td>\n",
              "      <td>1752</td>\n",
              "      <td>1535</td>\n",
              "      <td>2157</td>\n",
              "      <td>2189</td>\n",
              "      <td>2256</td>\n",
              "      <td>2252</td>\n",
              "      <td>2412</td>\n",
              "      <td>2136</td>\n",
              "      <td>1946</td>\n",
              "      <td>1950</td>\n",
              "      <td>1678</td>\n",
              "      <td>1776</td>\n",
              "      <td>1317</td>\n",
              "      <td>2164</td>\n",
              "      <td>2311</td>\n",
              "      <td>2344</td>\n",
              "      <td>1678</td>\n",
              "      <td>1869</td>\n",
              "      <td>1794</td>\n",
              "      <td>1810</td>\n",
              "      <td>1693</td>\n",
              "      <td>1872</td>\n",
              "      <td>1843</td>\n",
              "      <td>2120</td>\n",
              "      <td>...</td>\n",
              "      <td>2097</td>\n",
              "      <td>2104</td>\n",
              "      <td>2061</td>\n",
              "      <td>2174</td>\n",
              "      <td>2209</td>\n",
              "      <td>2037</td>\n",
              "      <td>2389</td>\n",
              "      <td>1982</td>\n",
              "      <td>2179</td>\n",
              "      <td>2334</td>\n",
              "      <td>2269</td>\n",
              "      <td>2452</td>\n",
              "      <td>2334</td>\n",
              "      <td>2251</td>\n",
              "      <td>2546</td>\n",
              "      <td>2197</td>\n",
              "      <td>1878</td>\n",
              "      <td>1922</td>\n",
              "      <td>1787</td>\n",
              "      <td>1853</td>\n",
              "      <td>1769</td>\n",
              "      <td>1930</td>\n",
              "      <td>2028</td>\n",
              "      <td>2024</td>\n",
              "      <td>1942</td>\n",
              "      <td>1788</td>\n",
              "      <td>2374</td>\n",
              "      <td>1854</td>\n",
              "      <td>2430</td>\n",
              "      <td>2255</td>\n",
              "      <td>2314</td>\n",
              "      <td>2413</td>\n",
              "      <td>2392</td>\n",
              "      <td>2261</td>\n",
              "      <td>2672</td>\n",
              "      <td>2277</td>\n",
              "      <td>2722</td>\n",
              "      <td>2496</td>\n",
              "      <td>2719</td>\n",
              "      <td>2407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2031</td>\n",
              "      <td>2053</td>\n",
              "      <td>1872</td>\n",
              "      <td>1772</td>\n",
              "      <td>2016</td>\n",
              "      <td>1951</td>\n",
              "      <td>2194</td>\n",
              "      <td>2004</td>\n",
              "      <td>2017</td>\n",
              "      <td>2451</td>\n",
              "      <td>2238</td>\n",
              "      <td>2232</td>\n",
              "      <td>1803</td>\n",
              "      <td>1767</td>\n",
              "      <td>1671</td>\n",
              "      <td>1911</td>\n",
              "      <td>1743</td>\n",
              "      <td>1533</td>\n",
              "      <td>2153</td>\n",
              "      <td>2176</td>\n",
              "      <td>2253</td>\n",
              "      <td>2263</td>\n",
              "      <td>2399</td>\n",
              "      <td>2144</td>\n",
              "      <td>1944</td>\n",
              "      <td>1934</td>\n",
              "      <td>1687</td>\n",
              "      <td>1778</td>\n",
              "      <td>1316</td>\n",
              "      <td>2155</td>\n",
              "      <td>2310</td>\n",
              "      <td>2359</td>\n",
              "      <td>1681</td>\n",
              "      <td>1872</td>\n",
              "      <td>1799</td>\n",
              "      <td>1802</td>\n",
              "      <td>1687</td>\n",
              "      <td>1867</td>\n",
              "      <td>1850</td>\n",
              "      <td>2122</td>\n",
              "      <td>...</td>\n",
              "      <td>2098</td>\n",
              "      <td>2103</td>\n",
              "      <td>2060</td>\n",
              "      <td>2181</td>\n",
              "      <td>2208</td>\n",
              "      <td>2036</td>\n",
              "      <td>2395</td>\n",
              "      <td>1978</td>\n",
              "      <td>2183</td>\n",
              "      <td>2326</td>\n",
              "      <td>2272</td>\n",
              "      <td>2453</td>\n",
              "      <td>2327</td>\n",
              "      <td>2248</td>\n",
              "      <td>2548</td>\n",
              "      <td>2204</td>\n",
              "      <td>1888</td>\n",
              "      <td>1900</td>\n",
              "      <td>1786</td>\n",
              "      <td>1866</td>\n",
              "      <td>1765</td>\n",
              "      <td>1938</td>\n",
              "      <td>2028</td>\n",
              "      <td>2019</td>\n",
              "      <td>1930</td>\n",
              "      <td>1791</td>\n",
              "      <td>2389</td>\n",
              "      <td>1851</td>\n",
              "      <td>2441</td>\n",
              "      <td>2256</td>\n",
              "      <td>2314</td>\n",
              "      <td>2423</td>\n",
              "      <td>2397</td>\n",
              "      <td>2262</td>\n",
              "      <td>2671</td>\n",
              "      <td>2278</td>\n",
              "      <td>2718</td>\n",
              "      <td>2498</td>\n",
              "      <td>2712</td>\n",
              "      <td>2406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2028</td>\n",
              "      <td>2052</td>\n",
              "      <td>1873</td>\n",
              "      <td>1774</td>\n",
              "      <td>2018</td>\n",
              "      <td>1947</td>\n",
              "      <td>2184</td>\n",
              "      <td>2007</td>\n",
              "      <td>2022</td>\n",
              "      <td>2452</td>\n",
              "      <td>2240</td>\n",
              "      <td>2247</td>\n",
              "      <td>1813</td>\n",
              "      <td>1762</td>\n",
              "      <td>1695</td>\n",
              "      <td>1892</td>\n",
              "      <td>1743</td>\n",
              "      <td>1538</td>\n",
              "      <td>2161</td>\n",
              "      <td>2182</td>\n",
              "      <td>2256</td>\n",
              "      <td>2270</td>\n",
              "      <td>2407</td>\n",
              "      <td>2144</td>\n",
              "      <td>1943</td>\n",
              "      <td>1947</td>\n",
              "      <td>1672</td>\n",
              "      <td>1785</td>\n",
              "      <td>1318</td>\n",
              "      <td>2156</td>\n",
              "      <td>2309</td>\n",
              "      <td>2354</td>\n",
              "      <td>1681</td>\n",
              "      <td>1874</td>\n",
              "      <td>1795</td>\n",
              "      <td>1801</td>\n",
              "      <td>1686</td>\n",
              "      <td>1866</td>\n",
              "      <td>1847</td>\n",
              "      <td>2125</td>\n",
              "      <td>...</td>\n",
              "      <td>2093</td>\n",
              "      <td>2107</td>\n",
              "      <td>2056</td>\n",
              "      <td>2188</td>\n",
              "      <td>2226</td>\n",
              "      <td>2037</td>\n",
              "      <td>2399</td>\n",
              "      <td>1979</td>\n",
              "      <td>2188</td>\n",
              "      <td>2317</td>\n",
              "      <td>2275</td>\n",
              "      <td>2450</td>\n",
              "      <td>2321</td>\n",
              "      <td>2244</td>\n",
              "      <td>2546</td>\n",
              "      <td>2212</td>\n",
              "      <td>1879</td>\n",
              "      <td>1927</td>\n",
              "      <td>1799</td>\n",
              "      <td>1866</td>\n",
              "      <td>1766</td>\n",
              "      <td>1950</td>\n",
              "      <td>2035</td>\n",
              "      <td>2012</td>\n",
              "      <td>1932</td>\n",
              "      <td>1798</td>\n",
              "      <td>2390</td>\n",
              "      <td>1846</td>\n",
              "      <td>2439</td>\n",
              "      <td>2260</td>\n",
              "      <td>2321</td>\n",
              "      <td>2440</td>\n",
              "      <td>2400</td>\n",
              "      <td>2270</td>\n",
              "      <td>2669</td>\n",
              "      <td>2279</td>\n",
              "      <td>2727</td>\n",
              "      <td>2497</td>\n",
              "      <td>2722</td>\n",
              "      <td>2412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2024</td>\n",
              "      <td>2053</td>\n",
              "      <td>1872</td>\n",
              "      <td>1768</td>\n",
              "      <td>2022</td>\n",
              "      <td>1947</td>\n",
              "      <td>2172</td>\n",
              "      <td>2009</td>\n",
              "      <td>2019</td>\n",
              "      <td>2459</td>\n",
              "      <td>2244</td>\n",
              "      <td>2251</td>\n",
              "      <td>1812</td>\n",
              "      <td>1761</td>\n",
              "      <td>1693</td>\n",
              "      <td>1902</td>\n",
              "      <td>1728</td>\n",
              "      <td>1547</td>\n",
              "      <td>2158</td>\n",
              "      <td>2188</td>\n",
              "      <td>2257</td>\n",
              "      <td>2274</td>\n",
              "      <td>2411</td>\n",
              "      <td>2145</td>\n",
              "      <td>1941</td>\n",
              "      <td>1939</td>\n",
              "      <td>1681</td>\n",
              "      <td>1786</td>\n",
              "      <td>1316</td>\n",
              "      <td>2156</td>\n",
              "      <td>2308</td>\n",
              "      <td>2337</td>\n",
              "      <td>1678</td>\n",
              "      <td>1879</td>\n",
              "      <td>1804</td>\n",
              "      <td>1796</td>\n",
              "      <td>1679</td>\n",
              "      <td>1864</td>\n",
              "      <td>1845</td>\n",
              "      <td>2130</td>\n",
              "      <td>...</td>\n",
              "      <td>2090</td>\n",
              "      <td>2104</td>\n",
              "      <td>2055</td>\n",
              "      <td>2189</td>\n",
              "      <td>2232</td>\n",
              "      <td>2038</td>\n",
              "      <td>2402</td>\n",
              "      <td>1980</td>\n",
              "      <td>2193</td>\n",
              "      <td>2323</td>\n",
              "      <td>2280</td>\n",
              "      <td>2449</td>\n",
              "      <td>2317</td>\n",
              "      <td>2245</td>\n",
              "      <td>2547</td>\n",
              "      <td>2227</td>\n",
              "      <td>1898</td>\n",
              "      <td>1924</td>\n",
              "      <td>1803</td>\n",
              "      <td>1870</td>\n",
              "      <td>1768</td>\n",
              "      <td>1955</td>\n",
              "      <td>2035</td>\n",
              "      <td>2010</td>\n",
              "      <td>1924</td>\n",
              "      <td>1794</td>\n",
              "      <td>2362</td>\n",
              "      <td>1857</td>\n",
              "      <td>2445</td>\n",
              "      <td>2260</td>\n",
              "      <td>2322</td>\n",
              "      <td>2445</td>\n",
              "      <td>2390</td>\n",
              "      <td>2271</td>\n",
              "      <td>2665</td>\n",
              "      <td>2284</td>\n",
              "      <td>2727</td>\n",
              "      <td>2494</td>\n",
              "      <td>2725</td>\n",
              "      <td>2416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2025</td>\n",
              "      <td>2054</td>\n",
              "      <td>1871</td>\n",
              "      <td>1772</td>\n",
              "      <td>2024</td>\n",
              "      <td>1947</td>\n",
              "      <td>2158</td>\n",
              "      <td>2014</td>\n",
              "      <td>2025</td>\n",
              "      <td>2460</td>\n",
              "      <td>2249</td>\n",
              "      <td>2252</td>\n",
              "      <td>1789</td>\n",
              "      <td>1744</td>\n",
              "      <td>1691</td>\n",
              "      <td>1922</td>\n",
              "      <td>1729</td>\n",
              "      <td>1560</td>\n",
              "      <td>2162</td>\n",
              "      <td>2186</td>\n",
              "      <td>2254</td>\n",
              "      <td>2274</td>\n",
              "      <td>2416</td>\n",
              "      <td>2142</td>\n",
              "      <td>1940</td>\n",
              "      <td>1920</td>\n",
              "      <td>1702</td>\n",
              "      <td>1794</td>\n",
              "      <td>1313</td>\n",
              "      <td>2154</td>\n",
              "      <td>2307</td>\n",
              "      <td>2346</td>\n",
              "      <td>1680</td>\n",
              "      <td>1880</td>\n",
              "      <td>1803</td>\n",
              "      <td>1800</td>\n",
              "      <td>1675</td>\n",
              "      <td>1866</td>\n",
              "      <td>1844</td>\n",
              "      <td>2138</td>\n",
              "      <td>...</td>\n",
              "      <td>2083</td>\n",
              "      <td>2105</td>\n",
              "      <td>2056</td>\n",
              "      <td>2204</td>\n",
              "      <td>2243</td>\n",
              "      <td>2038</td>\n",
              "      <td>2404</td>\n",
              "      <td>1983</td>\n",
              "      <td>2199</td>\n",
              "      <td>2327</td>\n",
              "      <td>2274</td>\n",
              "      <td>2456</td>\n",
              "      <td>2315</td>\n",
              "      <td>2247</td>\n",
              "      <td>2545</td>\n",
              "      <td>2236</td>\n",
              "      <td>1893</td>\n",
              "      <td>1931</td>\n",
              "      <td>1807</td>\n",
              "      <td>1877</td>\n",
              "      <td>1768</td>\n",
              "      <td>1947</td>\n",
              "      <td>2039</td>\n",
              "      <td>2008</td>\n",
              "      <td>1913</td>\n",
              "      <td>1803</td>\n",
              "      <td>2361</td>\n",
              "      <td>1858</td>\n",
              "      <td>2455</td>\n",
              "      <td>2262</td>\n",
              "      <td>2320</td>\n",
              "      <td>2450</td>\n",
              "      <td>2401</td>\n",
              "      <td>2277</td>\n",
              "      <td>2661</td>\n",
              "      <td>2279</td>\n",
              "      <td>2741</td>\n",
              "      <td>2489</td>\n",
              "      <td>2723</td>\n",
              "      <td>2422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2024</td>\n",
              "      <td>2056</td>\n",
              "      <td>1867</td>\n",
              "      <td>1774</td>\n",
              "      <td>2022</td>\n",
              "      <td>1946</td>\n",
              "      <td>2171</td>\n",
              "      <td>2018</td>\n",
              "      <td>2024</td>\n",
              "      <td>2460</td>\n",
              "      <td>2254</td>\n",
              "      <td>2250</td>\n",
              "      <td>1816</td>\n",
              "      <td>1750</td>\n",
              "      <td>1688</td>\n",
              "      <td>1904</td>\n",
              "      <td>1723</td>\n",
              "      <td>1562</td>\n",
              "      <td>2165</td>\n",
              "      <td>2184</td>\n",
              "      <td>2259</td>\n",
              "      <td>2281</td>\n",
              "      <td>2419</td>\n",
              "      <td>2136</td>\n",
              "      <td>1945</td>\n",
              "      <td>1913</td>\n",
              "      <td>1715</td>\n",
              "      <td>1792</td>\n",
              "      <td>1316</td>\n",
              "      <td>2155</td>\n",
              "      <td>2310</td>\n",
              "      <td>2355</td>\n",
              "      <td>1677</td>\n",
              "      <td>1879</td>\n",
              "      <td>1804</td>\n",
              "      <td>1797</td>\n",
              "      <td>1673</td>\n",
              "      <td>1861</td>\n",
              "      <td>1843</td>\n",
              "      <td>2141</td>\n",
              "      <td>...</td>\n",
              "      <td>2077</td>\n",
              "      <td>2106</td>\n",
              "      <td>2059</td>\n",
              "      <td>2209</td>\n",
              "      <td>2243</td>\n",
              "      <td>2041</td>\n",
              "      <td>2408</td>\n",
              "      <td>1989</td>\n",
              "      <td>2202</td>\n",
              "      <td>2328</td>\n",
              "      <td>2285</td>\n",
              "      <td>2459</td>\n",
              "      <td>2307</td>\n",
              "      <td>2235</td>\n",
              "      <td>2547</td>\n",
              "      <td>2246</td>\n",
              "      <td>1897</td>\n",
              "      <td>1935</td>\n",
              "      <td>1806</td>\n",
              "      <td>1877</td>\n",
              "      <td>1779</td>\n",
              "      <td>1952</td>\n",
              "      <td>2040</td>\n",
              "      <td>2009</td>\n",
              "      <td>1916</td>\n",
              "      <td>1805</td>\n",
              "      <td>2359</td>\n",
              "      <td>1859</td>\n",
              "      <td>2456</td>\n",
              "      <td>2254</td>\n",
              "      <td>2320</td>\n",
              "      <td>2436</td>\n",
              "      <td>2381</td>\n",
              "      <td>2286</td>\n",
              "      <td>2660</td>\n",
              "      <td>2284</td>\n",
              "      <td>2717</td>\n",
              "      <td>2491</td>\n",
              "      <td>2716</td>\n",
              "      <td>2430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2026</td>\n",
              "      <td>2053</td>\n",
              "      <td>1866</td>\n",
              "      <td>1770</td>\n",
              "      <td>2029</td>\n",
              "      <td>1953</td>\n",
              "      <td>2171</td>\n",
              "      <td>2030</td>\n",
              "      <td>2025</td>\n",
              "      <td>2467</td>\n",
              "      <td>2263</td>\n",
              "      <td>2256</td>\n",
              "      <td>1790</td>\n",
              "      <td>1754</td>\n",
              "      <td>1688</td>\n",
              "      <td>1914</td>\n",
              "      <td>1724</td>\n",
              "      <td>1571</td>\n",
              "      <td>2171</td>\n",
              "      <td>2189</td>\n",
              "      <td>2268</td>\n",
              "      <td>2289</td>\n",
              "      <td>2417</td>\n",
              "      <td>2141</td>\n",
              "      <td>1939</td>\n",
              "      <td>1895</td>\n",
              "      <td>1710</td>\n",
              "      <td>1797</td>\n",
              "      <td>1313</td>\n",
              "      <td>2151</td>\n",
              "      <td>2314</td>\n",
              "      <td>2354</td>\n",
              "      <td>1680</td>\n",
              "      <td>1885</td>\n",
              "      <td>1800</td>\n",
              "      <td>1795</td>\n",
              "      <td>1672</td>\n",
              "      <td>1859</td>\n",
              "      <td>1845</td>\n",
              "      <td>2149</td>\n",
              "      <td>...</td>\n",
              "      <td>2077</td>\n",
              "      <td>2101</td>\n",
              "      <td>2060</td>\n",
              "      <td>2214</td>\n",
              "      <td>2241</td>\n",
              "      <td>2046</td>\n",
              "      <td>2411</td>\n",
              "      <td>1990</td>\n",
              "      <td>2199</td>\n",
              "      <td>2332</td>\n",
              "      <td>2287</td>\n",
              "      <td>2466</td>\n",
              "      <td>2307</td>\n",
              "      <td>2243</td>\n",
              "      <td>2551</td>\n",
              "      <td>2256</td>\n",
              "      <td>1872</td>\n",
              "      <td>1929</td>\n",
              "      <td>1813</td>\n",
              "      <td>1883</td>\n",
              "      <td>1768</td>\n",
              "      <td>1962</td>\n",
              "      <td>2043</td>\n",
              "      <td>2018</td>\n",
              "      <td>1915</td>\n",
              "      <td>1803</td>\n",
              "      <td>2367</td>\n",
              "      <td>1853</td>\n",
              "      <td>2456</td>\n",
              "      <td>2265</td>\n",
              "      <td>2325</td>\n",
              "      <td>2445</td>\n",
              "      <td>2406</td>\n",
              "      <td>2290</td>\n",
              "      <td>2655</td>\n",
              "      <td>2280</td>\n",
              "      <td>2709</td>\n",
              "      <td>2485</td>\n",
              "      <td>2722</td>\n",
              "      <td>2431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2029</td>\n",
              "      <td>2055</td>\n",
              "      <td>1868</td>\n",
              "      <td>1777</td>\n",
              "      <td>2032</td>\n",
              "      <td>1951</td>\n",
              "      <td>2178</td>\n",
              "      <td>2028</td>\n",
              "      <td>2034</td>\n",
              "      <td>2469</td>\n",
              "      <td>2274</td>\n",
              "      <td>2271</td>\n",
              "      <td>1813</td>\n",
              "      <td>1740</td>\n",
              "      <td>1671</td>\n",
              "      <td>1932</td>\n",
              "      <td>1722</td>\n",
              "      <td>1587</td>\n",
              "      <td>2170</td>\n",
              "      <td>2184</td>\n",
              "      <td>2268</td>\n",
              "      <td>2291</td>\n",
              "      <td>2411</td>\n",
              "      <td>2144</td>\n",
              "      <td>1945</td>\n",
              "      <td>1948</td>\n",
              "      <td>1698</td>\n",
              "      <td>1798</td>\n",
              "      <td>1317</td>\n",
              "      <td>2154</td>\n",
              "      <td>2315</td>\n",
              "      <td>2348</td>\n",
              "      <td>1680</td>\n",
              "      <td>1888</td>\n",
              "      <td>1803</td>\n",
              "      <td>1794</td>\n",
              "      <td>1668</td>\n",
              "      <td>1861</td>\n",
              "      <td>1845</td>\n",
              "      <td>2153</td>\n",
              "      <td>...</td>\n",
              "      <td>2071</td>\n",
              "      <td>2109</td>\n",
              "      <td>2061</td>\n",
              "      <td>2218</td>\n",
              "      <td>2248</td>\n",
              "      <td>2046</td>\n",
              "      <td>2416</td>\n",
              "      <td>1994</td>\n",
              "      <td>2209</td>\n",
              "      <td>2336</td>\n",
              "      <td>2293</td>\n",
              "      <td>2474</td>\n",
              "      <td>2306</td>\n",
              "      <td>2240</td>\n",
              "      <td>2551</td>\n",
              "      <td>2270</td>\n",
              "      <td>1898</td>\n",
              "      <td>1933</td>\n",
              "      <td>1819</td>\n",
              "      <td>1883</td>\n",
              "      <td>1772</td>\n",
              "      <td>1960</td>\n",
              "      <td>2044</td>\n",
              "      <td>2007</td>\n",
              "      <td>1908</td>\n",
              "      <td>1806</td>\n",
              "      <td>2370</td>\n",
              "      <td>1853</td>\n",
              "      <td>2465</td>\n",
              "      <td>2260</td>\n",
              "      <td>2323</td>\n",
              "      <td>2452</td>\n",
              "      <td>2406</td>\n",
              "      <td>2296</td>\n",
              "      <td>2656</td>\n",
              "      <td>2285</td>\n",
              "      <td>2719</td>\n",
              "      <td>2484</td>\n",
              "      <td>2720</td>\n",
              "      <td>2429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2033</td>\n",
              "      <td>2059</td>\n",
              "      <td>1865</td>\n",
              "      <td>1777</td>\n",
              "      <td>2033</td>\n",
              "      <td>1957</td>\n",
              "      <td>2179</td>\n",
              "      <td>2032</td>\n",
              "      <td>2040</td>\n",
              "      <td>2478</td>\n",
              "      <td>2268</td>\n",
              "      <td>2277</td>\n",
              "      <td>1821</td>\n",
              "      <td>1755</td>\n",
              "      <td>1676</td>\n",
              "      <td>1928</td>\n",
              "      <td>1725</td>\n",
              "      <td>1592</td>\n",
              "      <td>2172</td>\n",
              "      <td>2189</td>\n",
              "      <td>2276</td>\n",
              "      <td>2296</td>\n",
              "      <td>2420</td>\n",
              "      <td>2144</td>\n",
              "      <td>1949</td>\n",
              "      <td>1902</td>\n",
              "      <td>1703</td>\n",
              "      <td>1800</td>\n",
              "      <td>1315</td>\n",
              "      <td>2158</td>\n",
              "      <td>2318</td>\n",
              "      <td>2348</td>\n",
              "      <td>1673</td>\n",
              "      <td>1890</td>\n",
              "      <td>1801</td>\n",
              "      <td>1794</td>\n",
              "      <td>1668</td>\n",
              "      <td>1860</td>\n",
              "      <td>1847</td>\n",
              "      <td>2159</td>\n",
              "      <td>...</td>\n",
              "      <td>2071</td>\n",
              "      <td>2112</td>\n",
              "      <td>2061</td>\n",
              "      <td>2220</td>\n",
              "      <td>2250</td>\n",
              "      <td>2051</td>\n",
              "      <td>2423</td>\n",
              "      <td>1993</td>\n",
              "      <td>2215</td>\n",
              "      <td>2338</td>\n",
              "      <td>2295</td>\n",
              "      <td>2483</td>\n",
              "      <td>2297</td>\n",
              "      <td>2242</td>\n",
              "      <td>2554</td>\n",
              "      <td>2280</td>\n",
              "      <td>1899</td>\n",
              "      <td>1920</td>\n",
              "      <td>1827</td>\n",
              "      <td>1878</td>\n",
              "      <td>1777</td>\n",
              "      <td>1972</td>\n",
              "      <td>2045</td>\n",
              "      <td>2013</td>\n",
              "      <td>1904</td>\n",
              "      <td>1807</td>\n",
              "      <td>2375</td>\n",
              "      <td>1860</td>\n",
              "      <td>2462</td>\n",
              "      <td>2256</td>\n",
              "      <td>2331</td>\n",
              "      <td>2462</td>\n",
              "      <td>2400</td>\n",
              "      <td>2297</td>\n",
              "      <td>2660</td>\n",
              "      <td>2291</td>\n",
              "      <td>2716</td>\n",
              "      <td>2484</td>\n",
              "      <td>2727</td>\n",
              "      <td>2433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>2041</td>\n",
              "      <td>2060</td>\n",
              "      <td>1868</td>\n",
              "      <td>1778</td>\n",
              "      <td>2034</td>\n",
              "      <td>1967</td>\n",
              "      <td>2187</td>\n",
              "      <td>2040</td>\n",
              "      <td>2040</td>\n",
              "      <td>2477</td>\n",
              "      <td>2277</td>\n",
              "      <td>2285</td>\n",
              "      <td>1813</td>\n",
              "      <td>1750</td>\n",
              "      <td>1676</td>\n",
              "      <td>1931</td>\n",
              "      <td>1749</td>\n",
              "      <td>1591</td>\n",
              "      <td>2181</td>\n",
              "      <td>2194</td>\n",
              "      <td>2285</td>\n",
              "      <td>2301</td>\n",
              "      <td>2420</td>\n",
              "      <td>2151</td>\n",
              "      <td>1950</td>\n",
              "      <td>1914</td>\n",
              "      <td>1739</td>\n",
              "      <td>1804</td>\n",
              "      <td>1319</td>\n",
              "      <td>2164</td>\n",
              "      <td>2319</td>\n",
              "      <td>2349</td>\n",
              "      <td>1676</td>\n",
              "      <td>1898</td>\n",
              "      <td>1803</td>\n",
              "      <td>1795</td>\n",
              "      <td>1667</td>\n",
              "      <td>1851</td>\n",
              "      <td>1846</td>\n",
              "      <td>2163</td>\n",
              "      <td>...</td>\n",
              "      <td>2077</td>\n",
              "      <td>2114</td>\n",
              "      <td>2062</td>\n",
              "      <td>2225</td>\n",
              "      <td>2256</td>\n",
              "      <td>2062</td>\n",
              "      <td>2422</td>\n",
              "      <td>1999</td>\n",
              "      <td>2217</td>\n",
              "      <td>2340</td>\n",
              "      <td>2301</td>\n",
              "      <td>2488</td>\n",
              "      <td>2297</td>\n",
              "      <td>2258</td>\n",
              "      <td>2556</td>\n",
              "      <td>2292</td>\n",
              "      <td>1889</td>\n",
              "      <td>1931</td>\n",
              "      <td>1832</td>\n",
              "      <td>1883</td>\n",
              "      <td>1776</td>\n",
              "      <td>1964</td>\n",
              "      <td>2045</td>\n",
              "      <td>2015</td>\n",
              "      <td>1905</td>\n",
              "      <td>1803</td>\n",
              "      <td>2374</td>\n",
              "      <td>1854</td>\n",
              "      <td>2473</td>\n",
              "      <td>2258</td>\n",
              "      <td>2340</td>\n",
              "      <td>2466</td>\n",
              "      <td>2397</td>\n",
              "      <td>2307</td>\n",
              "      <td>2658</td>\n",
              "      <td>2292</td>\n",
              "      <td>2702</td>\n",
              "      <td>2494</td>\n",
              "      <td>2731</td>\n",
              "      <td>2430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>2045</td>\n",
              "      <td>2061</td>\n",
              "      <td>1875</td>\n",
              "      <td>1783</td>\n",
              "      <td>2037</td>\n",
              "      <td>1983</td>\n",
              "      <td>2187</td>\n",
              "      <td>2044</td>\n",
              "      <td>2039</td>\n",
              "      <td>2482</td>\n",
              "      <td>2280</td>\n",
              "      <td>2303</td>\n",
              "      <td>1834</td>\n",
              "      <td>1766</td>\n",
              "      <td>1686</td>\n",
              "      <td>1945</td>\n",
              "      <td>1730</td>\n",
              "      <td>1612</td>\n",
              "      <td>2183</td>\n",
              "      <td>2209</td>\n",
              "      <td>2280</td>\n",
              "      <td>2304</td>\n",
              "      <td>2423</td>\n",
              "      <td>2160</td>\n",
              "      <td>1953</td>\n",
              "      <td>1906</td>\n",
              "      <td>1720</td>\n",
              "      <td>1808</td>\n",
              "      <td>1322</td>\n",
              "      <td>2167</td>\n",
              "      <td>2319</td>\n",
              "      <td>2353</td>\n",
              "      <td>1676</td>\n",
              "      <td>1899</td>\n",
              "      <td>1805</td>\n",
              "      <td>1795</td>\n",
              "      <td>1662</td>\n",
              "      <td>1854</td>\n",
              "      <td>1848</td>\n",
              "      <td>2165</td>\n",
              "      <td>...</td>\n",
              "      <td>2068</td>\n",
              "      <td>2112</td>\n",
              "      <td>2061</td>\n",
              "      <td>2238</td>\n",
              "      <td>2262</td>\n",
              "      <td>2067</td>\n",
              "      <td>2430</td>\n",
              "      <td>2002</td>\n",
              "      <td>2219</td>\n",
              "      <td>2334</td>\n",
              "      <td>2307</td>\n",
              "      <td>2485</td>\n",
              "      <td>2296</td>\n",
              "      <td>2265</td>\n",
              "      <td>2551</td>\n",
              "      <td>2303</td>\n",
              "      <td>1877</td>\n",
              "      <td>1936</td>\n",
              "      <td>1845</td>\n",
              "      <td>1879</td>\n",
              "      <td>1770</td>\n",
              "      <td>1978</td>\n",
              "      <td>2054</td>\n",
              "      <td>2020</td>\n",
              "      <td>1902</td>\n",
              "      <td>1807</td>\n",
              "      <td>2387</td>\n",
              "      <td>1855</td>\n",
              "      <td>2457</td>\n",
              "      <td>2263</td>\n",
              "      <td>2342</td>\n",
              "      <td>2470</td>\n",
              "      <td>2394</td>\n",
              "      <td>2313</td>\n",
              "      <td>2659</td>\n",
              "      <td>2293</td>\n",
              "      <td>2705</td>\n",
              "      <td>2495</td>\n",
              "      <td>2731</td>\n",
              "      <td>2424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>2054</td>\n",
              "      <td>2072</td>\n",
              "      <td>1878</td>\n",
              "      <td>1789</td>\n",
              "      <td>2038</td>\n",
              "      <td>2018</td>\n",
              "      <td>2195</td>\n",
              "      <td>2060</td>\n",
              "      <td>2040</td>\n",
              "      <td>2489</td>\n",
              "      <td>2288</td>\n",
              "      <td>2331</td>\n",
              "      <td>1836</td>\n",
              "      <td>1769</td>\n",
              "      <td>1699</td>\n",
              "      <td>1942</td>\n",
              "      <td>1753</td>\n",
              "      <td>1645</td>\n",
              "      <td>2198</td>\n",
              "      <td>2205</td>\n",
              "      <td>2289</td>\n",
              "      <td>2305</td>\n",
              "      <td>2421</td>\n",
              "      <td>2182</td>\n",
              "      <td>1959</td>\n",
              "      <td>1880</td>\n",
              "      <td>1750</td>\n",
              "      <td>1815</td>\n",
              "      <td>1324</td>\n",
              "      <td>2173</td>\n",
              "      <td>2330</td>\n",
              "      <td>2332</td>\n",
              "      <td>1680</td>\n",
              "      <td>1915</td>\n",
              "      <td>1812</td>\n",
              "      <td>1789</td>\n",
              "      <td>1656</td>\n",
              "      <td>1853</td>\n",
              "      <td>1846</td>\n",
              "      <td>2173</td>\n",
              "      <td>...</td>\n",
              "      <td>2065</td>\n",
              "      <td>2117</td>\n",
              "      <td>2061</td>\n",
              "      <td>2243</td>\n",
              "      <td>2265</td>\n",
              "      <td>2078</td>\n",
              "      <td>2434</td>\n",
              "      <td>2006</td>\n",
              "      <td>2222</td>\n",
              "      <td>2328</td>\n",
              "      <td>2312</td>\n",
              "      <td>2498</td>\n",
              "      <td>2297</td>\n",
              "      <td>2257</td>\n",
              "      <td>2554</td>\n",
              "      <td>2316</td>\n",
              "      <td>1901</td>\n",
              "      <td>1936</td>\n",
              "      <td>1842</td>\n",
              "      <td>1882</td>\n",
              "      <td>1773</td>\n",
              "      <td>1982</td>\n",
              "      <td>2045</td>\n",
              "      <td>2018</td>\n",
              "      <td>1905</td>\n",
              "      <td>1807</td>\n",
              "      <td>2395</td>\n",
              "      <td>1867</td>\n",
              "      <td>2470</td>\n",
              "      <td>2264</td>\n",
              "      <td>2363</td>\n",
              "      <td>2486</td>\n",
              "      <td>2391</td>\n",
              "      <td>2313</td>\n",
              "      <td>2659</td>\n",
              "      <td>2301</td>\n",
              "      <td>2700</td>\n",
              "      <td>2496</td>\n",
              "      <td>2737</td>\n",
              "      <td>2433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>2073</td>\n",
              "      <td>2083</td>\n",
              "      <td>1882</td>\n",
              "      <td>1792</td>\n",
              "      <td>2045</td>\n",
              "      <td>2073</td>\n",
              "      <td>2223</td>\n",
              "      <td>2080</td>\n",
              "      <td>2053</td>\n",
              "      <td>2493</td>\n",
              "      <td>2290</td>\n",
              "      <td>2394</td>\n",
              "      <td>1842</td>\n",
              "      <td>1788</td>\n",
              "      <td>1707</td>\n",
              "      <td>1953</td>\n",
              "      <td>1734</td>\n",
              "      <td>1673</td>\n",
              "      <td>2209</td>\n",
              "      <td>2229</td>\n",
              "      <td>2305</td>\n",
              "      <td>2318</td>\n",
              "      <td>2434</td>\n",
              "      <td>2226</td>\n",
              "      <td>1963</td>\n",
              "      <td>1878</td>\n",
              "      <td>1759</td>\n",
              "      <td>1817</td>\n",
              "      <td>1323</td>\n",
              "      <td>2182</td>\n",
              "      <td>2336</td>\n",
              "      <td>2348</td>\n",
              "      <td>1679</td>\n",
              "      <td>1926</td>\n",
              "      <td>1818</td>\n",
              "      <td>1797</td>\n",
              "      <td>1662</td>\n",
              "      <td>1852</td>\n",
              "      <td>1849</td>\n",
              "      <td>2176</td>\n",
              "      <td>...</td>\n",
              "      <td>2064</td>\n",
              "      <td>2119</td>\n",
              "      <td>2061</td>\n",
              "      <td>2246</td>\n",
              "      <td>2274</td>\n",
              "      <td>2082</td>\n",
              "      <td>2435</td>\n",
              "      <td>2012</td>\n",
              "      <td>2228</td>\n",
              "      <td>2334</td>\n",
              "      <td>2320</td>\n",
              "      <td>2509</td>\n",
              "      <td>2295</td>\n",
              "      <td>2261</td>\n",
              "      <td>2549</td>\n",
              "      <td>2325</td>\n",
              "      <td>1884</td>\n",
              "      <td>1934</td>\n",
              "      <td>1852</td>\n",
              "      <td>1882</td>\n",
              "      <td>1776</td>\n",
              "      <td>1982</td>\n",
              "      <td>2057</td>\n",
              "      <td>2014</td>\n",
              "      <td>1899</td>\n",
              "      <td>1810</td>\n",
              "      <td>2403</td>\n",
              "      <td>1866</td>\n",
              "      <td>2482</td>\n",
              "      <td>2263</td>\n",
              "      <td>2367</td>\n",
              "      <td>2485</td>\n",
              "      <td>2393</td>\n",
              "      <td>2327</td>\n",
              "      <td>2662</td>\n",
              "      <td>2305</td>\n",
              "      <td>2700</td>\n",
              "      <td>2500</td>\n",
              "      <td>2744</td>\n",
              "      <td>2431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2099</td>\n",
              "      <td>2104</td>\n",
              "      <td>1890</td>\n",
              "      <td>1804</td>\n",
              "      <td>2042</td>\n",
              "      <td>2168</td>\n",
              "      <td>2250</td>\n",
              "      <td>2111</td>\n",
              "      <td>2073</td>\n",
              "      <td>2502</td>\n",
              "      <td>2302</td>\n",
              "      <td>2485</td>\n",
              "      <td>1911</td>\n",
              "      <td>1805</td>\n",
              "      <td>1716</td>\n",
              "      <td>1952</td>\n",
              "      <td>1748</td>\n",
              "      <td>1714</td>\n",
              "      <td>2241</td>\n",
              "      <td>2250</td>\n",
              "      <td>2312</td>\n",
              "      <td>2325</td>\n",
              "      <td>2441</td>\n",
              "      <td>2306</td>\n",
              "      <td>1968</td>\n",
              "      <td>1875</td>\n",
              "      <td>1755</td>\n",
              "      <td>1826</td>\n",
              "      <td>1326</td>\n",
              "      <td>2196</td>\n",
              "      <td>2345</td>\n",
              "      <td>2352</td>\n",
              "      <td>1679</td>\n",
              "      <td>1936</td>\n",
              "      <td>1824</td>\n",
              "      <td>1793</td>\n",
              "      <td>1652</td>\n",
              "      <td>1858</td>\n",
              "      <td>1852</td>\n",
              "      <td>2191</td>\n",
              "      <td>...</td>\n",
              "      <td>2061</td>\n",
              "      <td>2122</td>\n",
              "      <td>2064</td>\n",
              "      <td>2254</td>\n",
              "      <td>2279</td>\n",
              "      <td>2090</td>\n",
              "      <td>2442</td>\n",
              "      <td>2015</td>\n",
              "      <td>2232</td>\n",
              "      <td>2339</td>\n",
              "      <td>2324</td>\n",
              "      <td>2515</td>\n",
              "      <td>2299</td>\n",
              "      <td>2263</td>\n",
              "      <td>2552</td>\n",
              "      <td>2334</td>\n",
              "      <td>1894</td>\n",
              "      <td>1920</td>\n",
              "      <td>1869</td>\n",
              "      <td>1888</td>\n",
              "      <td>1774</td>\n",
              "      <td>1991</td>\n",
              "      <td>2052</td>\n",
              "      <td>2009</td>\n",
              "      <td>1895</td>\n",
              "      <td>1812</td>\n",
              "      <td>2377</td>\n",
              "      <td>1868</td>\n",
              "      <td>2482</td>\n",
              "      <td>2263</td>\n",
              "      <td>2369</td>\n",
              "      <td>2488</td>\n",
              "      <td>2397</td>\n",
              "      <td>2338</td>\n",
              "      <td>2665</td>\n",
              "      <td>2315</td>\n",
              "      <td>2710</td>\n",
              "      <td>2505</td>\n",
              "      <td>2748</td>\n",
              "      <td>2426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>2158</td>\n",
              "      <td>2143</td>\n",
              "      <td>1905</td>\n",
              "      <td>1823</td>\n",
              "      <td>2055</td>\n",
              "      <td>2330</td>\n",
              "      <td>2300</td>\n",
              "      <td>2176</td>\n",
              "      <td>2108</td>\n",
              "      <td>2527</td>\n",
              "      <td>2325</td>\n",
              "      <td>2633</td>\n",
              "      <td>1979</td>\n",
              "      <td>1843</td>\n",
              "      <td>1733</td>\n",
              "      <td>1970</td>\n",
              "      <td>1774</td>\n",
              "      <td>1810</td>\n",
              "      <td>2291</td>\n",
              "      <td>2310</td>\n",
              "      <td>2333</td>\n",
              "      <td>2336</td>\n",
              "      <td>2453</td>\n",
              "      <td>2444</td>\n",
              "      <td>1974</td>\n",
              "      <td>1880</td>\n",
              "      <td>1753</td>\n",
              "      <td>1830</td>\n",
              "      <td>1326</td>\n",
              "      <td>2226</td>\n",
              "      <td>2361</td>\n",
              "      <td>2355</td>\n",
              "      <td>1684</td>\n",
              "      <td>1943</td>\n",
              "      <td>1831</td>\n",
              "      <td>1795</td>\n",
              "      <td>1674</td>\n",
              "      <td>1858</td>\n",
              "      <td>1858</td>\n",
              "      <td>2200</td>\n",
              "      <td>...</td>\n",
              "      <td>2052</td>\n",
              "      <td>2122</td>\n",
              "      <td>2064</td>\n",
              "      <td>2259</td>\n",
              "      <td>2281</td>\n",
              "      <td>2095</td>\n",
              "      <td>2438</td>\n",
              "      <td>2019</td>\n",
              "      <td>2232</td>\n",
              "      <td>2344</td>\n",
              "      <td>2328</td>\n",
              "      <td>2517</td>\n",
              "      <td>2304</td>\n",
              "      <td>2264</td>\n",
              "      <td>2557</td>\n",
              "      <td>2343</td>\n",
              "      <td>1875</td>\n",
              "      <td>1933</td>\n",
              "      <td>1877</td>\n",
              "      <td>1892</td>\n",
              "      <td>1780</td>\n",
              "      <td>2001</td>\n",
              "      <td>2059</td>\n",
              "      <td>2012</td>\n",
              "      <td>1898</td>\n",
              "      <td>1817</td>\n",
              "      <td>2383</td>\n",
              "      <td>1869</td>\n",
              "      <td>2482</td>\n",
              "      <td>2264</td>\n",
              "      <td>2387</td>\n",
              "      <td>2492</td>\n",
              "      <td>2401</td>\n",
              "      <td>2348</td>\n",
              "      <td>2662</td>\n",
              "      <td>2325</td>\n",
              "      <td>2731</td>\n",
              "      <td>2512</td>\n",
              "      <td>2755</td>\n",
              "      <td>2429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2269</td>\n",
              "      <td>2220</td>\n",
              "      <td>1931</td>\n",
              "      <td>1860</td>\n",
              "      <td>2067</td>\n",
              "      <td>2556</td>\n",
              "      <td>2398</td>\n",
              "      <td>2300</td>\n",
              "      <td>2164</td>\n",
              "      <td>2572</td>\n",
              "      <td>2367</td>\n",
              "      <td>2870</td>\n",
              "      <td>2098</td>\n",
              "      <td>1940</td>\n",
              "      <td>1757</td>\n",
              "      <td>1973</td>\n",
              "      <td>1782</td>\n",
              "      <td>1969</td>\n",
              "      <td>2395</td>\n",
              "      <td>2394</td>\n",
              "      <td>2359</td>\n",
              "      <td>2360</td>\n",
              "      <td>2481</td>\n",
              "      <td>2651</td>\n",
              "      <td>1983</td>\n",
              "      <td>1870</td>\n",
              "      <td>1811</td>\n",
              "      <td>1832</td>\n",
              "      <td>1327</td>\n",
              "      <td>2279</td>\n",
              "      <td>2390</td>\n",
              "      <td>2363</td>\n",
              "      <td>1692</td>\n",
              "      <td>1958</td>\n",
              "      <td>1841</td>\n",
              "      <td>1799</td>\n",
              "      <td>1685</td>\n",
              "      <td>1874</td>\n",
              "      <td>1871</td>\n",
              "      <td>2220</td>\n",
              "      <td>...</td>\n",
              "      <td>2050</td>\n",
              "      <td>2121</td>\n",
              "      <td>2067</td>\n",
              "      <td>2267</td>\n",
              "      <td>2290</td>\n",
              "      <td>2100</td>\n",
              "      <td>2441</td>\n",
              "      <td>2026</td>\n",
              "      <td>2239</td>\n",
              "      <td>2357</td>\n",
              "      <td>2340</td>\n",
              "      <td>2529</td>\n",
              "      <td>2305</td>\n",
              "      <td>2264</td>\n",
              "      <td>2552</td>\n",
              "      <td>2351</td>\n",
              "      <td>1872</td>\n",
              "      <td>1939</td>\n",
              "      <td>1874</td>\n",
              "      <td>1904</td>\n",
              "      <td>1776</td>\n",
              "      <td>2000</td>\n",
              "      <td>2059</td>\n",
              "      <td>2020</td>\n",
              "      <td>1903</td>\n",
              "      <td>1827</td>\n",
              "      <td>2377</td>\n",
              "      <td>1876</td>\n",
              "      <td>2491</td>\n",
              "      <td>2269</td>\n",
              "      <td>2384</td>\n",
              "      <td>2480</td>\n",
              "      <td>2413</td>\n",
              "      <td>2356</td>\n",
              "      <td>2663</td>\n",
              "      <td>2336</td>\n",
              "      <td>2729</td>\n",
              "      <td>2518</td>\n",
              "      <td>2761</td>\n",
              "      <td>2425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2446</td>\n",
              "      <td>2346</td>\n",
              "      <td>1986</td>\n",
              "      <td>1921</td>\n",
              "      <td>2083</td>\n",
              "      <td>2829</td>\n",
              "      <td>2591</td>\n",
              "      <td>2510</td>\n",
              "      <td>2269</td>\n",
              "      <td>2651</td>\n",
              "      <td>2428</td>\n",
              "      <td>3176</td>\n",
              "      <td>2310</td>\n",
              "      <td>2066</td>\n",
              "      <td>1820</td>\n",
              "      <td>2027</td>\n",
              "      <td>1802</td>\n",
              "      <td>2152</td>\n",
              "      <td>2551</td>\n",
              "      <td>2539</td>\n",
              "      <td>2422</td>\n",
              "      <td>2408</td>\n",
              "      <td>2527</td>\n",
              "      <td>2911</td>\n",
              "      <td>1996</td>\n",
              "      <td>1915</td>\n",
              "      <td>1833</td>\n",
              "      <td>1836</td>\n",
              "      <td>1334</td>\n",
              "      <td>2375</td>\n",
              "      <td>2454</td>\n",
              "      <td>2388</td>\n",
              "      <td>1712</td>\n",
              "      <td>1973</td>\n",
              "      <td>1855</td>\n",
              "      <td>1813</td>\n",
              "      <td>1705</td>\n",
              "      <td>1901</td>\n",
              "      <td>1895</td>\n",
              "      <td>2247</td>\n",
              "      <td>...</td>\n",
              "      <td>2055</td>\n",
              "      <td>2121</td>\n",
              "      <td>2066</td>\n",
              "      <td>2275</td>\n",
              "      <td>2300</td>\n",
              "      <td>2116</td>\n",
              "      <td>2445</td>\n",
              "      <td>2040</td>\n",
              "      <td>2256</td>\n",
              "      <td>2361</td>\n",
              "      <td>2352</td>\n",
              "      <td>2547</td>\n",
              "      <td>2320</td>\n",
              "      <td>2272</td>\n",
              "      <td>2554</td>\n",
              "      <td>2365</td>\n",
              "      <td>1887</td>\n",
              "      <td>1934</td>\n",
              "      <td>1889</td>\n",
              "      <td>1915</td>\n",
              "      <td>1781</td>\n",
              "      <td>2006</td>\n",
              "      <td>2062</td>\n",
              "      <td>2017</td>\n",
              "      <td>1892</td>\n",
              "      <td>1830</td>\n",
              "      <td>2380</td>\n",
              "      <td>1884</td>\n",
              "      <td>2491</td>\n",
              "      <td>2275</td>\n",
              "      <td>2400</td>\n",
              "      <td>2491</td>\n",
              "      <td>2428</td>\n",
              "      <td>2368</td>\n",
              "      <td>2673</td>\n",
              "      <td>2348</td>\n",
              "      <td>2731</td>\n",
              "      <td>2518</td>\n",
              "      <td>2771</td>\n",
              "      <td>2428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2688</td>\n",
              "      <td>2532</td>\n",
              "      <td>2068</td>\n",
              "      <td>2039</td>\n",
              "      <td>2120</td>\n",
              "      <td>3111</td>\n",
              "      <td>2881</td>\n",
              "      <td>2811</td>\n",
              "      <td>2449</td>\n",
              "      <td>2786</td>\n",
              "      <td>2533</td>\n",
              "      <td>3498</td>\n",
              "      <td>2519</td>\n",
              "      <td>2287</td>\n",
              "      <td>1909</td>\n",
              "      <td>2106</td>\n",
              "      <td>1873</td>\n",
              "      <td>2394</td>\n",
              "      <td>2756</td>\n",
              "      <td>2764</td>\n",
              "      <td>2521</td>\n",
              "      <td>2515</td>\n",
              "      <td>2612</td>\n",
              "      <td>3181</td>\n",
              "      <td>2019</td>\n",
              "      <td>1873</td>\n",
              "      <td>1842</td>\n",
              "      <td>1842</td>\n",
              "      <td>1339</td>\n",
              "      <td>2533</td>\n",
              "      <td>2543</td>\n",
              "      <td>2436</td>\n",
              "      <td>1748</td>\n",
              "      <td>2002</td>\n",
              "      <td>1897</td>\n",
              "      <td>1835</td>\n",
              "      <td>1715</td>\n",
              "      <td>1946</td>\n",
              "      <td>1927</td>\n",
              "      <td>2321</td>\n",
              "      <td>...</td>\n",
              "      <td>2055</td>\n",
              "      <td>2127</td>\n",
              "      <td>2068</td>\n",
              "      <td>2281</td>\n",
              "      <td>2304</td>\n",
              "      <td>2106</td>\n",
              "      <td>2450</td>\n",
              "      <td>2066</td>\n",
              "      <td>2274</td>\n",
              "      <td>2375</td>\n",
              "      <td>2372</td>\n",
              "      <td>2567</td>\n",
              "      <td>2348</td>\n",
              "      <td>2280</td>\n",
              "      <td>2564</td>\n",
              "      <td>2377</td>\n",
              "      <td>1873</td>\n",
              "      <td>1938</td>\n",
              "      <td>1897</td>\n",
              "      <td>1930</td>\n",
              "      <td>1795</td>\n",
              "      <td>2014</td>\n",
              "      <td>2081</td>\n",
              "      <td>2032</td>\n",
              "      <td>1895</td>\n",
              "      <td>1839</td>\n",
              "      <td>2376</td>\n",
              "      <td>1891</td>\n",
              "      <td>2485</td>\n",
              "      <td>2289</td>\n",
              "      <td>2416</td>\n",
              "      <td>2491</td>\n",
              "      <td>2442</td>\n",
              "      <td>2390</td>\n",
              "      <td>2690</td>\n",
              "      <td>2377</td>\n",
              "      <td>2731</td>\n",
              "      <td>2528</td>\n",
              "      <td>2788</td>\n",
              "      <td>2436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>2964</td>\n",
              "      <td>2768</td>\n",
              "      <td>2221</td>\n",
              "      <td>2220</td>\n",
              "      <td>2192</td>\n",
              "      <td>3401</td>\n",
              "      <td>3200</td>\n",
              "      <td>3173</td>\n",
              "      <td>2726</td>\n",
              "      <td>2992</td>\n",
              "      <td>2726</td>\n",
              "      <td>3825</td>\n",
              "      <td>2805</td>\n",
              "      <td>2517</td>\n",
              "      <td>2045</td>\n",
              "      <td>2215</td>\n",
              "      <td>1990</td>\n",
              "      <td>2625</td>\n",
              "      <td>2986</td>\n",
              "      <td>3015</td>\n",
              "      <td>2678</td>\n",
              "      <td>2659</td>\n",
              "      <td>2746</td>\n",
              "      <td>3459</td>\n",
              "      <td>2055</td>\n",
              "      <td>1888</td>\n",
              "      <td>1826</td>\n",
              "      <td>1851</td>\n",
              "      <td>1355</td>\n",
              "      <td>2774</td>\n",
              "      <td>2702</td>\n",
              "      <td>2534</td>\n",
              "      <td>1815</td>\n",
              "      <td>2055</td>\n",
              "      <td>1969</td>\n",
              "      <td>1875</td>\n",
              "      <td>1766</td>\n",
              "      <td>2044</td>\n",
              "      <td>1996</td>\n",
              "      <td>2398</td>\n",
              "      <td>...</td>\n",
              "      <td>2060</td>\n",
              "      <td>2135</td>\n",
              "      <td>2079</td>\n",
              "      <td>2286</td>\n",
              "      <td>2315</td>\n",
              "      <td>2114</td>\n",
              "      <td>2480</td>\n",
              "      <td>2112</td>\n",
              "      <td>2314</td>\n",
              "      <td>2411</td>\n",
              "      <td>2412</td>\n",
              "      <td>2604</td>\n",
              "      <td>2400</td>\n",
              "      <td>2287</td>\n",
              "      <td>2581</td>\n",
              "      <td>2390</td>\n",
              "      <td>1903</td>\n",
              "      <td>1913</td>\n",
              "      <td>1920</td>\n",
              "      <td>1956</td>\n",
              "      <td>1831</td>\n",
              "      <td>2027</td>\n",
              "      <td>2102</td>\n",
              "      <td>2066</td>\n",
              "      <td>1912</td>\n",
              "      <td>1846</td>\n",
              "      <td>2362</td>\n",
              "      <td>1904</td>\n",
              "      <td>2494</td>\n",
              "      <td>2303</td>\n",
              "      <td>2450</td>\n",
              "      <td>2492</td>\n",
              "      <td>2479</td>\n",
              "      <td>2421</td>\n",
              "      <td>2715</td>\n",
              "      <td>2424</td>\n",
              "      <td>2737</td>\n",
              "      <td>2542</td>\n",
              "      <td>2797</td>\n",
              "      <td>2451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>3243</td>\n",
              "      <td>3025</td>\n",
              "      <td>2445</td>\n",
              "      <td>2469</td>\n",
              "      <td>2314</td>\n",
              "      <td>3656</td>\n",
              "      <td>3548</td>\n",
              "      <td>3557</td>\n",
              "      <td>3090</td>\n",
              "      <td>3262</td>\n",
              "      <td>2991</td>\n",
              "      <td>4080</td>\n",
              "      <td>3054</td>\n",
              "      <td>2768</td>\n",
              "      <td>2238</td>\n",
              "      <td>2425</td>\n",
              "      <td>2141</td>\n",
              "      <td>2846</td>\n",
              "      <td>3234</td>\n",
              "      <td>3303</td>\n",
              "      <td>2907</td>\n",
              "      <td>2866</td>\n",
              "      <td>2942</td>\n",
              "      <td>3715</td>\n",
              "      <td>2139</td>\n",
              "      <td>1965</td>\n",
              "      <td>1856</td>\n",
              "      <td>1863</td>\n",
              "      <td>1390</td>\n",
              "      <td>3073</td>\n",
              "      <td>2925</td>\n",
              "      <td>2699</td>\n",
              "      <td>1932</td>\n",
              "      <td>2152</td>\n",
              "      <td>2085</td>\n",
              "      <td>1949</td>\n",
              "      <td>1837</td>\n",
              "      <td>2197</td>\n",
              "      <td>2111</td>\n",
              "      <td>2572</td>\n",
              "      <td>...</td>\n",
              "      <td>2073</td>\n",
              "      <td>2141</td>\n",
              "      <td>2095</td>\n",
              "      <td>2295</td>\n",
              "      <td>2321</td>\n",
              "      <td>2129</td>\n",
              "      <td>2504</td>\n",
              "      <td>2193</td>\n",
              "      <td>2383</td>\n",
              "      <td>2487</td>\n",
              "      <td>2483</td>\n",
              "      <td>2661</td>\n",
              "      <td>2502</td>\n",
              "      <td>2306</td>\n",
              "      <td>2611</td>\n",
              "      <td>2419</td>\n",
              "      <td>1890</td>\n",
              "      <td>1945</td>\n",
              "      <td>1978</td>\n",
              "      <td>2002</td>\n",
              "      <td>1889</td>\n",
              "      <td>2059</td>\n",
              "      <td>2133</td>\n",
              "      <td>2113</td>\n",
              "      <td>1920</td>\n",
              "      <td>1865</td>\n",
              "      <td>2359</td>\n",
              "      <td>1906</td>\n",
              "      <td>2500</td>\n",
              "      <td>2339</td>\n",
              "      <td>2520</td>\n",
              "      <td>2506</td>\n",
              "      <td>2533</td>\n",
              "      <td>2482</td>\n",
              "      <td>2778</td>\n",
              "      <td>2517</td>\n",
              "      <td>2737</td>\n",
              "      <td>2565</td>\n",
              "      <td>2816</td>\n",
              "      <td>2457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>3512</td>\n",
              "      <td>3277</td>\n",
              "      <td>2728</td>\n",
              "      <td>2761</td>\n",
              "      <td>2521</td>\n",
              "      <td>3898</td>\n",
              "      <td>3880</td>\n",
              "      <td>3929</td>\n",
              "      <td>3476</td>\n",
              "      <td>3533</td>\n",
              "      <td>3320</td>\n",
              "      <td>4080</td>\n",
              "      <td>3281</td>\n",
              "      <td>2995</td>\n",
              "      <td>2455</td>\n",
              "      <td>2566</td>\n",
              "      <td>2344</td>\n",
              "      <td>3045</td>\n",
              "      <td>3502</td>\n",
              "      <td>3573</td>\n",
              "      <td>3165</td>\n",
              "      <td>3110</td>\n",
              "      <td>3181</td>\n",
              "      <td>3942</td>\n",
              "      <td>2272</td>\n",
              "      <td>1975</td>\n",
              "      <td>1922</td>\n",
              "      <td>1882</td>\n",
              "      <td>1450</td>\n",
              "      <td>3395</td>\n",
              "      <td>3186</td>\n",
              "      <td>2935</td>\n",
              "      <td>2115</td>\n",
              "      <td>2308</td>\n",
              "      <td>2274</td>\n",
              "      <td>2081</td>\n",
              "      <td>2000</td>\n",
              "      <td>2417</td>\n",
              "      <td>2303</td>\n",
              "      <td>2800</td>\n",
              "      <td>...</td>\n",
              "      <td>2100</td>\n",
              "      <td>2159</td>\n",
              "      <td>2128</td>\n",
              "      <td>2302</td>\n",
              "      <td>2328</td>\n",
              "      <td>2148</td>\n",
              "      <td>2581</td>\n",
              "      <td>2334</td>\n",
              "      <td>2509</td>\n",
              "      <td>2595</td>\n",
              "      <td>2606</td>\n",
              "      <td>2759</td>\n",
              "      <td>2669</td>\n",
              "      <td>2345</td>\n",
              "      <td>2678</td>\n",
              "      <td>2463</td>\n",
              "      <td>1891</td>\n",
              "      <td>1930</td>\n",
              "      <td>2063</td>\n",
              "      <td>2090</td>\n",
              "      <td>1989</td>\n",
              "      <td>2112</td>\n",
              "      <td>2203</td>\n",
              "      <td>2213</td>\n",
              "      <td>1942</td>\n",
              "      <td>1882</td>\n",
              "      <td>2363</td>\n",
              "      <td>1930</td>\n",
              "      <td>2504</td>\n",
              "      <td>2414</td>\n",
              "      <td>2635</td>\n",
              "      <td>2514</td>\n",
              "      <td>2632</td>\n",
              "      <td>2594</td>\n",
              "      <td>2886</td>\n",
              "      <td>2661</td>\n",
              "      <td>2765</td>\n",
              "      <td>2619</td>\n",
              "      <td>2848</td>\n",
              "      <td>2476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>3760</td>\n",
              "      <td>3518</td>\n",
              "      <td>3046</td>\n",
              "      <td>3060</td>\n",
              "      <td>2799</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3875</td>\n",
              "      <td>3838</td>\n",
              "      <td>3667</td>\n",
              "      <td>4080</td>\n",
              "      <td>3487</td>\n",
              "      <td>3224</td>\n",
              "      <td>2674</td>\n",
              "      <td>2885</td>\n",
              "      <td>2522</td>\n",
              "      <td>3230</td>\n",
              "      <td>3727</td>\n",
              "      <td>3837</td>\n",
              "      <td>3484</td>\n",
              "      <td>3376</td>\n",
              "      <td>3438</td>\n",
              "      <td>4080</td>\n",
              "      <td>2476</td>\n",
              "      <td>2070</td>\n",
              "      <td>2038</td>\n",
              "      <td>1925</td>\n",
              "      <td>1556</td>\n",
              "      <td>3707</td>\n",
              "      <td>3445</td>\n",
              "      <td>3232</td>\n",
              "      <td>2357</td>\n",
              "      <td>2528</td>\n",
              "      <td>2534</td>\n",
              "      <td>2282</td>\n",
              "      <td>2213</td>\n",
              "      <td>2676</td>\n",
              "      <td>2563</td>\n",
              "      <td>3063</td>\n",
              "      <td>...</td>\n",
              "      <td>2149</td>\n",
              "      <td>2191</td>\n",
              "      <td>2185</td>\n",
              "      <td>2319</td>\n",
              "      <td>2312</td>\n",
              "      <td>2171</td>\n",
              "      <td>2716</td>\n",
              "      <td>2576</td>\n",
              "      <td>2719</td>\n",
              "      <td>2768</td>\n",
              "      <td>2803</td>\n",
              "      <td>2903</td>\n",
              "      <td>2925</td>\n",
              "      <td>2435</td>\n",
              "      <td>2795</td>\n",
              "      <td>2541</td>\n",
              "      <td>1901</td>\n",
              "      <td>1944</td>\n",
              "      <td>2217</td>\n",
              "      <td>2228</td>\n",
              "      <td>2143</td>\n",
              "      <td>2215</td>\n",
              "      <td>2313</td>\n",
              "      <td>2352</td>\n",
              "      <td>1975</td>\n",
              "      <td>1906</td>\n",
              "      <td>2368</td>\n",
              "      <td>1972</td>\n",
              "      <td>2505</td>\n",
              "      <td>2528</td>\n",
              "      <td>2797</td>\n",
              "      <td>2558</td>\n",
              "      <td>2794</td>\n",
              "      <td>2749</td>\n",
              "      <td>3059</td>\n",
              "      <td>2860</td>\n",
              "      <td>2791</td>\n",
              "      <td>2716</td>\n",
              "      <td>2911</td>\n",
              "      <td>2518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>3980</td>\n",
              "      <td>3729</td>\n",
              "      <td>3367</td>\n",
              "      <td>3350</td>\n",
              "      <td>3123</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4012</td>\n",
              "      <td>4080</td>\n",
              "      <td>3650</td>\n",
              "      <td>3437</td>\n",
              "      <td>2893</td>\n",
              "      <td>3151</td>\n",
              "      <td>2751</td>\n",
              "      <td>3391</td>\n",
              "      <td>3936</td>\n",
              "      <td>4055</td>\n",
              "      <td>3752</td>\n",
              "      <td>3648</td>\n",
              "      <td>3697</td>\n",
              "      <td>4080</td>\n",
              "      <td>2747</td>\n",
              "      <td>2196</td>\n",
              "      <td>2217</td>\n",
              "      <td>2007</td>\n",
              "      <td>1730</td>\n",
              "      <td>4002</td>\n",
              "      <td>3705</td>\n",
              "      <td>3558</td>\n",
              "      <td>2632</td>\n",
              "      <td>2769</td>\n",
              "      <td>2811</td>\n",
              "      <td>2529</td>\n",
              "      <td>2488</td>\n",
              "      <td>2951</td>\n",
              "      <td>2864</td>\n",
              "      <td>3345</td>\n",
              "      <td>...</td>\n",
              "      <td>2236</td>\n",
              "      <td>2251</td>\n",
              "      <td>2287</td>\n",
              "      <td>2340</td>\n",
              "      <td>2349</td>\n",
              "      <td>2212</td>\n",
              "      <td>2933</td>\n",
              "      <td>2915</td>\n",
              "      <td>3003</td>\n",
              "      <td>3034</td>\n",
              "      <td>3070</td>\n",
              "      <td>3132</td>\n",
              "      <td>3255</td>\n",
              "      <td>2575</td>\n",
              "      <td>2976</td>\n",
              "      <td>2662</td>\n",
              "      <td>1886</td>\n",
              "      <td>1956</td>\n",
              "      <td>2423</td>\n",
              "      <td>2436</td>\n",
              "      <td>2362</td>\n",
              "      <td>2367</td>\n",
              "      <td>2467</td>\n",
              "      <td>2551</td>\n",
              "      <td>2062</td>\n",
              "      <td>1977</td>\n",
              "      <td>2389</td>\n",
              "      <td>2044</td>\n",
              "      <td>2533</td>\n",
              "      <td>2696</td>\n",
              "      <td>3003</td>\n",
              "      <td>2612</td>\n",
              "      <td>3008</td>\n",
              "      <td>2969</td>\n",
              "      <td>3296</td>\n",
              "      <td>3111</td>\n",
              "      <td>2865</td>\n",
              "      <td>2886</td>\n",
              "      <td>3019</td>\n",
              "      <td>2580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>4080</td>\n",
              "      <td>3920</td>\n",
              "      <td>3685</td>\n",
              "      <td>3623</td>\n",
              "      <td>3457</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3829</td>\n",
              "      <td>3629</td>\n",
              "      <td>3086</td>\n",
              "      <td>3399</td>\n",
              "      <td>2948</td>\n",
              "      <td>3531</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4004</td>\n",
              "      <td>3899</td>\n",
              "      <td>3929</td>\n",
              "      <td>4080</td>\n",
              "      <td>3058</td>\n",
              "      <td>2344</td>\n",
              "      <td>2464</td>\n",
              "      <td>2155</td>\n",
              "      <td>1971</td>\n",
              "      <td>4080</td>\n",
              "      <td>3940</td>\n",
              "      <td>3865</td>\n",
              "      <td>2905</td>\n",
              "      <td>3046</td>\n",
              "      <td>3085</td>\n",
              "      <td>2802</td>\n",
              "      <td>2726</td>\n",
              "      <td>3214</td>\n",
              "      <td>3170</td>\n",
              "      <td>3619</td>\n",
              "      <td>...</td>\n",
              "      <td>2391</td>\n",
              "      <td>2357</td>\n",
              "      <td>2452</td>\n",
              "      <td>2385</td>\n",
              "      <td>2367</td>\n",
              "      <td>2286</td>\n",
              "      <td>3223</td>\n",
              "      <td>3314</td>\n",
              "      <td>3342</td>\n",
              "      <td>3349</td>\n",
              "      <td>3434</td>\n",
              "      <td>3424</td>\n",
              "      <td>3632</td>\n",
              "      <td>2817</td>\n",
              "      <td>3224</td>\n",
              "      <td>2865</td>\n",
              "      <td>1912</td>\n",
              "      <td>1981</td>\n",
              "      <td>2684</td>\n",
              "      <td>2672</td>\n",
              "      <td>2616</td>\n",
              "      <td>2570</td>\n",
              "      <td>2672</td>\n",
              "      <td>2778</td>\n",
              "      <td>2179</td>\n",
              "      <td>2070</td>\n",
              "      <td>2421</td>\n",
              "      <td>2171</td>\n",
              "      <td>2560</td>\n",
              "      <td>2892</td>\n",
              "      <td>3344</td>\n",
              "      <td>2665</td>\n",
              "      <td>3271</td>\n",
              "      <td>3219</td>\n",
              "      <td>3570</td>\n",
              "      <td>3384</td>\n",
              "      <td>2982</td>\n",
              "      <td>3126</td>\n",
              "      <td>3189</td>\n",
              "      <td>2696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3969</td>\n",
              "      <td>3874</td>\n",
              "      <td>3785</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3960</td>\n",
              "      <td>3809</td>\n",
              "      <td>3264</td>\n",
              "      <td>3596</td>\n",
              "      <td>3116</td>\n",
              "      <td>3662</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3362</td>\n",
              "      <td>2491</td>\n",
              "      <td>2704</td>\n",
              "      <td>2373</td>\n",
              "      <td>2256</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3160</td>\n",
              "      <td>3319</td>\n",
              "      <td>3398</td>\n",
              "      <td>3073</td>\n",
              "      <td>3035</td>\n",
              "      <td>3469</td>\n",
              "      <td>3448</td>\n",
              "      <td>3862</td>\n",
              "      <td>...</td>\n",
              "      <td>2632</td>\n",
              "      <td>2528</td>\n",
              "      <td>2696</td>\n",
              "      <td>2452</td>\n",
              "      <td>2397</td>\n",
              "      <td>2425</td>\n",
              "      <td>3550</td>\n",
              "      <td>3735</td>\n",
              "      <td>3694</td>\n",
              "      <td>3700</td>\n",
              "      <td>3840</td>\n",
              "      <td>3757</td>\n",
              "      <td>4024</td>\n",
              "      <td>3175</td>\n",
              "      <td>3528</td>\n",
              "      <td>3139</td>\n",
              "      <td>1949</td>\n",
              "      <td>2027</td>\n",
              "      <td>2958</td>\n",
              "      <td>2930</td>\n",
              "      <td>2873</td>\n",
              "      <td>2783</td>\n",
              "      <td>2905</td>\n",
              "      <td>3009</td>\n",
              "      <td>2377</td>\n",
              "      <td>2223</td>\n",
              "      <td>2455</td>\n",
              "      <td>2353</td>\n",
              "      <td>2613</td>\n",
              "      <td>3109</td>\n",
              "      <td>3583</td>\n",
              "      <td>2790</td>\n",
              "      <td>3552</td>\n",
              "      <td>3482</td>\n",
              "      <td>3853</td>\n",
              "      <td>3609</td>\n",
              "      <td>3165</td>\n",
              "      <td>3429</td>\n",
              "      <td>3423</td>\n",
              "      <td>2873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4058</td>\n",
              "      <td>3949</td>\n",
              "      <td>3417</td>\n",
              "      <td>3765</td>\n",
              "      <td>3292</td>\n",
              "      <td>3773</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3654</td>\n",
              "      <td>2680</td>\n",
              "      <td>2927</td>\n",
              "      <td>2647</td>\n",
              "      <td>2548</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3399</td>\n",
              "      <td>3568</td>\n",
              "      <td>3638</td>\n",
              "      <td>3335</td>\n",
              "      <td>3268</td>\n",
              "      <td>3686</td>\n",
              "      <td>3704</td>\n",
              "      <td>4080</td>\n",
              "      <td>...</td>\n",
              "      <td>2924</td>\n",
              "      <td>2741</td>\n",
              "      <td>2985</td>\n",
              "      <td>2575</td>\n",
              "      <td>2447</td>\n",
              "      <td>2649</td>\n",
              "      <td>3883</td>\n",
              "      <td>4080</td>\n",
              "      <td>4036</td>\n",
              "      <td>4038</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3546</td>\n",
              "      <td>3834</td>\n",
              "      <td>3449</td>\n",
              "      <td>1976</td>\n",
              "      <td>2131</td>\n",
              "      <td>3225</td>\n",
              "      <td>3161</td>\n",
              "      <td>3119</td>\n",
              "      <td>3002</td>\n",
              "      <td>3122</td>\n",
              "      <td>3227</td>\n",
              "      <td>2590</td>\n",
              "      <td>2421</td>\n",
              "      <td>2536</td>\n",
              "      <td>2588</td>\n",
              "      <td>2710</td>\n",
              "      <td>3327</td>\n",
              "      <td>3853</td>\n",
              "      <td>2904</td>\n",
              "      <td>3829</td>\n",
              "      <td>3751</td>\n",
              "      <td>4080</td>\n",
              "      <td>3913</td>\n",
              "      <td>3409</td>\n",
              "      <td>3706</td>\n",
              "      <td>3690</td>\n",
              "      <td>3128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4059</td>\n",
              "      <td>3556</td>\n",
              "      <td>3917</td>\n",
              "      <td>3466</td>\n",
              "      <td>3871</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3907</td>\n",
              "      <td>2822</td>\n",
              "      <td>3164</td>\n",
              "      <td>2925</td>\n",
              "      <td>2838</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3603</td>\n",
              "      <td>3795</td>\n",
              "      <td>3852</td>\n",
              "      <td>3572</td>\n",
              "      <td>3493</td>\n",
              "      <td>3892</td>\n",
              "      <td>3922</td>\n",
              "      <td>4080</td>\n",
              "      <td>...</td>\n",
              "      <td>3170</td>\n",
              "      <td>2983</td>\n",
              "      <td>3289</td>\n",
              "      <td>2787</td>\n",
              "      <td>2542</td>\n",
              "      <td>2972</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3951</td>\n",
              "      <td>4080</td>\n",
              "      <td>3772</td>\n",
              "      <td>2088</td>\n",
              "      <td>2245</td>\n",
              "      <td>3483</td>\n",
              "      <td>3386</td>\n",
              "      <td>3343</td>\n",
              "      <td>3224</td>\n",
              "      <td>3320</td>\n",
              "      <td>3431</td>\n",
              "      <td>2821</td>\n",
              "      <td>2634</td>\n",
              "      <td>2637</td>\n",
              "      <td>2830</td>\n",
              "      <td>2875</td>\n",
              "      <td>3542</td>\n",
              "      <td>4080</td>\n",
              "      <td>3062</td>\n",
              "      <td>4080</td>\n",
              "      <td>3985</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3705</td>\n",
              "      <td>4002</td>\n",
              "      <td>3976</td>\n",
              "      <td>3420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3661</td>\n",
              "      <td>3928</td>\n",
              "      <td>3593</td>\n",
              "      <td>3935</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>2988</td>\n",
              "      <td>3402</td>\n",
              "      <td>3194</td>\n",
              "      <td>3093</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3779</td>\n",
              "      <td>3985</td>\n",
              "      <td>4044</td>\n",
              "      <td>3780</td>\n",
              "      <td>3682</td>\n",
              "      <td>4077</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>...</td>\n",
              "      <td>3546</td>\n",
              "      <td>3221</td>\n",
              "      <td>3595</td>\n",
              "      <td>3091</td>\n",
              "      <td>2705</td>\n",
              "      <td>3346</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4076</td>\n",
              "      <td>2231</td>\n",
              "      <td>2458</td>\n",
              "      <td>3703</td>\n",
              "      <td>3576</td>\n",
              "      <td>3552</td>\n",
              "      <td>3417</td>\n",
              "      <td>3513</td>\n",
              "      <td>3606</td>\n",
              "      <td>3050</td>\n",
              "      <td>2846</td>\n",
              "      <td>2758</td>\n",
              "      <td>3073</td>\n",
              "      <td>3086</td>\n",
              "      <td>3726</td>\n",
              "      <td>4080</td>\n",
              "      <td>3212</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4040</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>3733</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>40 rows × 143 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    X1_10^5  X1_10^5.1  X1_10^5.2  ...  x4_10^3.11  x4_10^3.12  x4_10^3.13\n",
              "0      2091       2105       1953  ...        2598        2913        2494\n",
              "1      2077       2094       1939  ...        2567        2851        2484\n",
              "2      2068       2085       1916  ...        2550        2825        2477\n",
              "3      2056       2065       1908  ...        2540        2798        2471\n",
              "4      2053       2056       1895  ...        2539        2776        2445\n",
              "5      2037       2051       1883  ...        2532        2756        2440\n",
              "6      2039       2058       1881  ...        2523        2739        2434\n",
              "7      2033       2066       1876  ...        2515        2727        2430\n",
              "8      2028       2062       1872  ...        2514        2722        2417\n",
              "9      2025       2061       1871  ...        2512        2720        2418\n",
              "10     2023       2054       1870  ...        2512        2719        2408\n",
              "11     2021       2056       1868  ...        2502        2724        2404\n",
              "12     2033       2050       1875  ...        2496        2719        2407\n",
              "13     2031       2053       1872  ...        2498        2712        2406\n",
              "14     2028       2052       1873  ...        2497        2722        2412\n",
              "15     2024       2053       1872  ...        2494        2725        2416\n",
              "16     2025       2054       1871  ...        2489        2723        2422\n",
              "17     2024       2056       1867  ...        2491        2716        2430\n",
              "18     2026       2053       1866  ...        2485        2722        2431\n",
              "19     2029       2055       1868  ...        2484        2720        2429\n",
              "20     2033       2059       1865  ...        2484        2727        2433\n",
              "21     2041       2060       1868  ...        2494        2731        2430\n",
              "22     2045       2061       1875  ...        2495        2731        2424\n",
              "23     2054       2072       1878  ...        2496        2737        2433\n",
              "24     2073       2083       1882  ...        2500        2744        2431\n",
              "25     2099       2104       1890  ...        2505        2748        2426\n",
              "26     2158       2143       1905  ...        2512        2755        2429\n",
              "27     2269       2220       1931  ...        2518        2761        2425\n",
              "28     2446       2346       1986  ...        2518        2771        2428\n",
              "29     2688       2532       2068  ...        2528        2788        2436\n",
              "30     2964       2768       2221  ...        2542        2797        2451\n",
              "31     3243       3025       2445  ...        2565        2816        2457\n",
              "32     3512       3277       2728  ...        2619        2848        2476\n",
              "33     3760       3518       3046  ...        2716        2911        2518\n",
              "34     3980       3729       3367  ...        2886        3019        2580\n",
              "35     4080       3920       3685  ...        3126        3189        2696\n",
              "36     4080       4080       3969  ...        3429        3423        2873\n",
              "37     4080       4080       4080  ...        3706        3690        3128\n",
              "38     4080       4080       4080  ...        4002        3976        3420\n",
              "39     4080       4080       4080  ...        4080        4080        3733\n",
              "\n",
              "[40 rows x 143 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8xRCSWvO6x6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeb3e8e5-efd2-4607-faa7-d6bcf643a061"
      },
      "source": [
        "pcr_5_df = pd.DataFrame()\n",
        "pcr_4_df = pd.DataFrame()\n",
        "pcr_3_df = pd.DataFrame()\n",
        "\n",
        "#Seperate each concentration(5, 4, 3)\n",
        "for col in pcr_df.columns:\n",
        "  if col[6] == '5':\n",
        "    pcr_5_df = pcr_5_df.append(pcr_df[col])\n",
        "  elif col[6] == '4':\n",
        "    pcr_4_df = pcr_4_df.append(pcr_df[col])\n",
        "  elif col[6] == '3':\n",
        "    pcr_3_df = pcr_3_df.append(pcr_df[col])\n",
        "\n",
        "\n",
        "print(f'10^5:{pcr_5_df.shape[0], pcr_5_df.shape[1]}, 10^4:{pcr_4_df.shape[0], pcr_4_df.shape[1]}, 10^3:{pcr_3_df.shape[0], pcr_3_df.shape[1]}')\n",
        "#display(pcr_5_df)\n",
        "#display(pcr_4_df)\n",
        "#display(pcr_3_df)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10^5:(24, 40), 10^4:(71, 40), 10^3:(48, 40)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_wdzBgSPPrD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "f962f16b-4f6a-45ab-88d3-60956365e7f3"
      },
      "source": [
        "pcr_df = pcr_df.T\n",
        "\n",
        "display(pcr_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>X1_10^5</th>\n",
              "      <td>2091</td>\n",
              "      <td>2077</td>\n",
              "      <td>2068</td>\n",
              "      <td>2056</td>\n",
              "      <td>2053</td>\n",
              "      <td>2037</td>\n",
              "      <td>2039</td>\n",
              "      <td>2033</td>\n",
              "      <td>2028</td>\n",
              "      <td>2025</td>\n",
              "      <td>2023</td>\n",
              "      <td>2021</td>\n",
              "      <td>2033</td>\n",
              "      <td>2031</td>\n",
              "      <td>2028</td>\n",
              "      <td>2024</td>\n",
              "      <td>2025</td>\n",
              "      <td>2024</td>\n",
              "      <td>2026</td>\n",
              "      <td>2029</td>\n",
              "      <td>2033</td>\n",
              "      <td>2041</td>\n",
              "      <td>2045</td>\n",
              "      <td>2054</td>\n",
              "      <td>2073</td>\n",
              "      <td>2099</td>\n",
              "      <td>2158</td>\n",
              "      <td>2269</td>\n",
              "      <td>2446</td>\n",
              "      <td>2688</td>\n",
              "      <td>2964</td>\n",
              "      <td>3243</td>\n",
              "      <td>3512</td>\n",
              "      <td>3760</td>\n",
              "      <td>3980</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X1_10^5.1</th>\n",
              "      <td>2105</td>\n",
              "      <td>2094</td>\n",
              "      <td>2085</td>\n",
              "      <td>2065</td>\n",
              "      <td>2056</td>\n",
              "      <td>2051</td>\n",
              "      <td>2058</td>\n",
              "      <td>2066</td>\n",
              "      <td>2062</td>\n",
              "      <td>2061</td>\n",
              "      <td>2054</td>\n",
              "      <td>2056</td>\n",
              "      <td>2050</td>\n",
              "      <td>2053</td>\n",
              "      <td>2052</td>\n",
              "      <td>2053</td>\n",
              "      <td>2054</td>\n",
              "      <td>2056</td>\n",
              "      <td>2053</td>\n",
              "      <td>2055</td>\n",
              "      <td>2059</td>\n",
              "      <td>2060</td>\n",
              "      <td>2061</td>\n",
              "      <td>2072</td>\n",
              "      <td>2083</td>\n",
              "      <td>2104</td>\n",
              "      <td>2143</td>\n",
              "      <td>2220</td>\n",
              "      <td>2346</td>\n",
              "      <td>2532</td>\n",
              "      <td>2768</td>\n",
              "      <td>3025</td>\n",
              "      <td>3277</td>\n",
              "      <td>3518</td>\n",
              "      <td>3729</td>\n",
              "      <td>3920</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X1_10^5.2</th>\n",
              "      <td>1953</td>\n",
              "      <td>1939</td>\n",
              "      <td>1916</td>\n",
              "      <td>1908</td>\n",
              "      <td>1895</td>\n",
              "      <td>1883</td>\n",
              "      <td>1881</td>\n",
              "      <td>1876</td>\n",
              "      <td>1872</td>\n",
              "      <td>1871</td>\n",
              "      <td>1870</td>\n",
              "      <td>1868</td>\n",
              "      <td>1875</td>\n",
              "      <td>1872</td>\n",
              "      <td>1873</td>\n",
              "      <td>1872</td>\n",
              "      <td>1871</td>\n",
              "      <td>1867</td>\n",
              "      <td>1866</td>\n",
              "      <td>1868</td>\n",
              "      <td>1865</td>\n",
              "      <td>1868</td>\n",
              "      <td>1875</td>\n",
              "      <td>1878</td>\n",
              "      <td>1882</td>\n",
              "      <td>1890</td>\n",
              "      <td>1905</td>\n",
              "      <td>1931</td>\n",
              "      <td>1986</td>\n",
              "      <td>2068</td>\n",
              "      <td>2221</td>\n",
              "      <td>2445</td>\n",
              "      <td>2728</td>\n",
              "      <td>3046</td>\n",
              "      <td>3367</td>\n",
              "      <td>3685</td>\n",
              "      <td>3969</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X1_10^5.3</th>\n",
              "      <td>1811</td>\n",
              "      <td>1810</td>\n",
              "      <td>1806</td>\n",
              "      <td>1797</td>\n",
              "      <td>1798</td>\n",
              "      <td>1795</td>\n",
              "      <td>1792</td>\n",
              "      <td>1787</td>\n",
              "      <td>1782</td>\n",
              "      <td>1780</td>\n",
              "      <td>1775</td>\n",
              "      <td>1770</td>\n",
              "      <td>1773</td>\n",
              "      <td>1772</td>\n",
              "      <td>1774</td>\n",
              "      <td>1768</td>\n",
              "      <td>1772</td>\n",
              "      <td>1774</td>\n",
              "      <td>1770</td>\n",
              "      <td>1777</td>\n",
              "      <td>1777</td>\n",
              "      <td>1778</td>\n",
              "      <td>1783</td>\n",
              "      <td>1789</td>\n",
              "      <td>1792</td>\n",
              "      <td>1804</td>\n",
              "      <td>1823</td>\n",
              "      <td>1860</td>\n",
              "      <td>1921</td>\n",
              "      <td>2039</td>\n",
              "      <td>2220</td>\n",
              "      <td>2469</td>\n",
              "      <td>2761</td>\n",
              "      <td>3060</td>\n",
              "      <td>3350</td>\n",
              "      <td>3623</td>\n",
              "      <td>3874</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X1_10^5.4</th>\n",
              "      <td>2048</td>\n",
              "      <td>2039</td>\n",
              "      <td>2022</td>\n",
              "      <td>2022</td>\n",
              "      <td>2015</td>\n",
              "      <td>2013</td>\n",
              "      <td>2007</td>\n",
              "      <td>2010</td>\n",
              "      <td>2012</td>\n",
              "      <td>2012</td>\n",
              "      <td>2011</td>\n",
              "      <td>2012</td>\n",
              "      <td>2014</td>\n",
              "      <td>2016</td>\n",
              "      <td>2018</td>\n",
              "      <td>2022</td>\n",
              "      <td>2024</td>\n",
              "      <td>2022</td>\n",
              "      <td>2029</td>\n",
              "      <td>2032</td>\n",
              "      <td>2033</td>\n",
              "      <td>2034</td>\n",
              "      <td>2037</td>\n",
              "      <td>2038</td>\n",
              "      <td>2045</td>\n",
              "      <td>2042</td>\n",
              "      <td>2055</td>\n",
              "      <td>2067</td>\n",
              "      <td>2083</td>\n",
              "      <td>2120</td>\n",
              "      <td>2192</td>\n",
              "      <td>2314</td>\n",
              "      <td>2521</td>\n",
              "      <td>2799</td>\n",
              "      <td>3123</td>\n",
              "      <td>3457</td>\n",
              "      <td>3785</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x4_10^3.9</th>\n",
              "      <td>2368</td>\n",
              "      <td>2352</td>\n",
              "      <td>2327</td>\n",
              "      <td>2315</td>\n",
              "      <td>2313</td>\n",
              "      <td>2299</td>\n",
              "      <td>2285</td>\n",
              "      <td>2285</td>\n",
              "      <td>2276</td>\n",
              "      <td>2277</td>\n",
              "      <td>2271</td>\n",
              "      <td>2274</td>\n",
              "      <td>2277</td>\n",
              "      <td>2278</td>\n",
              "      <td>2279</td>\n",
              "      <td>2284</td>\n",
              "      <td>2279</td>\n",
              "      <td>2284</td>\n",
              "      <td>2280</td>\n",
              "      <td>2285</td>\n",
              "      <td>2291</td>\n",
              "      <td>2292</td>\n",
              "      <td>2293</td>\n",
              "      <td>2301</td>\n",
              "      <td>2305</td>\n",
              "      <td>2315</td>\n",
              "      <td>2325</td>\n",
              "      <td>2336</td>\n",
              "      <td>2348</td>\n",
              "      <td>2377</td>\n",
              "      <td>2424</td>\n",
              "      <td>2517</td>\n",
              "      <td>2661</td>\n",
              "      <td>2860</td>\n",
              "      <td>3111</td>\n",
              "      <td>3384</td>\n",
              "      <td>3609</td>\n",
              "      <td>3913</td>\n",
              "      <td>4080</td>\n",
              "      <td>4080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x4_10^3.10</th>\n",
              "      <td>2754</td>\n",
              "      <td>2750</td>\n",
              "      <td>2743</td>\n",
              "      <td>2728</td>\n",
              "      <td>2712</td>\n",
              "      <td>2691</td>\n",
              "      <td>2701</td>\n",
              "      <td>2710</td>\n",
              "      <td>2727</td>\n",
              "      <td>2730</td>\n",
              "      <td>2727</td>\n",
              "      <td>2728</td>\n",
              "      <td>2722</td>\n",
              "      <td>2718</td>\n",
              "      <td>2727</td>\n",
              "      <td>2727</td>\n",
              "      <td>2741</td>\n",
              "      <td>2717</td>\n",
              "      <td>2709</td>\n",
              "      <td>2719</td>\n",
              "      <td>2716</td>\n",
              "      <td>2702</td>\n",
              "      <td>2705</td>\n",
              "      <td>2700</td>\n",
              "      <td>2700</td>\n",
              "      <td>2710</td>\n",
              "      <td>2731</td>\n",
              "      <td>2729</td>\n",
              "      <td>2731</td>\n",
              "      <td>2731</td>\n",
              "      <td>2737</td>\n",
              "      <td>2737</td>\n",
              "      <td>2765</td>\n",
              "      <td>2791</td>\n",
              "      <td>2865</td>\n",
              "      <td>2982</td>\n",
              "      <td>3165</td>\n",
              "      <td>3409</td>\n",
              "      <td>3705</td>\n",
              "      <td>4040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x4_10^3.11</th>\n",
              "      <td>2598</td>\n",
              "      <td>2567</td>\n",
              "      <td>2550</td>\n",
              "      <td>2540</td>\n",
              "      <td>2539</td>\n",
              "      <td>2532</td>\n",
              "      <td>2523</td>\n",
              "      <td>2515</td>\n",
              "      <td>2514</td>\n",
              "      <td>2512</td>\n",
              "      <td>2512</td>\n",
              "      <td>2502</td>\n",
              "      <td>2496</td>\n",
              "      <td>2498</td>\n",
              "      <td>2497</td>\n",
              "      <td>2494</td>\n",
              "      <td>2489</td>\n",
              "      <td>2491</td>\n",
              "      <td>2485</td>\n",
              "      <td>2484</td>\n",
              "      <td>2484</td>\n",
              "      <td>2494</td>\n",
              "      <td>2495</td>\n",
              "      <td>2496</td>\n",
              "      <td>2500</td>\n",
              "      <td>2505</td>\n",
              "      <td>2512</td>\n",
              "      <td>2518</td>\n",
              "      <td>2518</td>\n",
              "      <td>2528</td>\n",
              "      <td>2542</td>\n",
              "      <td>2565</td>\n",
              "      <td>2619</td>\n",
              "      <td>2716</td>\n",
              "      <td>2886</td>\n",
              "      <td>3126</td>\n",
              "      <td>3429</td>\n",
              "      <td>3706</td>\n",
              "      <td>4002</td>\n",
              "      <td>4080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x4_10^3.12</th>\n",
              "      <td>2913</td>\n",
              "      <td>2851</td>\n",
              "      <td>2825</td>\n",
              "      <td>2798</td>\n",
              "      <td>2776</td>\n",
              "      <td>2756</td>\n",
              "      <td>2739</td>\n",
              "      <td>2727</td>\n",
              "      <td>2722</td>\n",
              "      <td>2720</td>\n",
              "      <td>2719</td>\n",
              "      <td>2724</td>\n",
              "      <td>2719</td>\n",
              "      <td>2712</td>\n",
              "      <td>2722</td>\n",
              "      <td>2725</td>\n",
              "      <td>2723</td>\n",
              "      <td>2716</td>\n",
              "      <td>2722</td>\n",
              "      <td>2720</td>\n",
              "      <td>2727</td>\n",
              "      <td>2731</td>\n",
              "      <td>2731</td>\n",
              "      <td>2737</td>\n",
              "      <td>2744</td>\n",
              "      <td>2748</td>\n",
              "      <td>2755</td>\n",
              "      <td>2761</td>\n",
              "      <td>2771</td>\n",
              "      <td>2788</td>\n",
              "      <td>2797</td>\n",
              "      <td>2816</td>\n",
              "      <td>2848</td>\n",
              "      <td>2911</td>\n",
              "      <td>3019</td>\n",
              "      <td>3189</td>\n",
              "      <td>3423</td>\n",
              "      <td>3690</td>\n",
              "      <td>3976</td>\n",
              "      <td>4080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x4_10^3.13</th>\n",
              "      <td>2494</td>\n",
              "      <td>2484</td>\n",
              "      <td>2477</td>\n",
              "      <td>2471</td>\n",
              "      <td>2445</td>\n",
              "      <td>2440</td>\n",
              "      <td>2434</td>\n",
              "      <td>2430</td>\n",
              "      <td>2417</td>\n",
              "      <td>2418</td>\n",
              "      <td>2408</td>\n",
              "      <td>2404</td>\n",
              "      <td>2407</td>\n",
              "      <td>2406</td>\n",
              "      <td>2412</td>\n",
              "      <td>2416</td>\n",
              "      <td>2422</td>\n",
              "      <td>2430</td>\n",
              "      <td>2431</td>\n",
              "      <td>2429</td>\n",
              "      <td>2433</td>\n",
              "      <td>2430</td>\n",
              "      <td>2424</td>\n",
              "      <td>2433</td>\n",
              "      <td>2431</td>\n",
              "      <td>2426</td>\n",
              "      <td>2429</td>\n",
              "      <td>2425</td>\n",
              "      <td>2428</td>\n",
              "      <td>2436</td>\n",
              "      <td>2451</td>\n",
              "      <td>2457</td>\n",
              "      <td>2476</td>\n",
              "      <td>2518</td>\n",
              "      <td>2580</td>\n",
              "      <td>2696</td>\n",
              "      <td>2873</td>\n",
              "      <td>3128</td>\n",
              "      <td>3420</td>\n",
              "      <td>3733</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>143 rows × 40 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              0     1     2     3     4   ...    35    36    37    38    39\n",
              "X1_10^5     2091  2077  2068  2056  2053  ...  4080  4080  4080  4080  4080\n",
              "X1_10^5.1   2105  2094  2085  2065  2056  ...  3920  4080  4080  4080  4080\n",
              "X1_10^5.2   1953  1939  1916  1908  1895  ...  3685  3969  4080  4080  4080\n",
              "X1_10^5.3   1811  1810  1806  1797  1798  ...  3623  3874  4080  4080  4080\n",
              "X1_10^5.4   2048  2039  2022  2022  2015  ...  3457  3785  4080  4080  4080\n",
              "...          ...   ...   ...   ...   ...  ...   ...   ...   ...   ...   ...\n",
              "x4_10^3.9   2368  2352  2327  2315  2313  ...  3384  3609  3913  4080  4080\n",
              "x4_10^3.10  2754  2750  2743  2728  2712  ...  2982  3165  3409  3705  4040\n",
              "x4_10^3.11  2598  2567  2550  2540  2539  ...  3126  3429  3706  4002  4080\n",
              "x4_10^3.12  2913  2851  2825  2798  2776  ...  3189  3423  3690  3976  4080\n",
              "x4_10^3.13  2494  2484  2477  2471  2445  ...  2696  2873  3128  3420  3733\n",
              "\n",
              "[143 rows x 40 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fq1XwF6kPAdQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b249d624-1b24-43a9-b8d2-976109067f40"
      },
      "source": [
        "#Get target\n",
        "target_arr = []\n",
        "target_3 = []\n",
        "target_4 = []\n",
        "target_5 = []\n",
        "\n",
        "for index, row in pcr_df.iterrows():\n",
        "  if index[6] == '5':\n",
        "    target_arr.append(27.94744)\n",
        "    target_5.append(27.94744)\n",
        "  elif index[6] == '4':\n",
        "    target_arr.append(31.09441)\n",
        "    target_4.append(31.09441)\n",
        "  elif index[6] == '3':\n",
        "    target_arr.append(33.58142)\n",
        "    target_3.append(33.58142)\n",
        "\n",
        "target_arr = np.array(target_arr)\n",
        "target_3 = np.array(target_3)\n",
        "target_4 = np.array(target_4)\n",
        "target_5 = np.array(target_5)\n",
        "\n",
        "print(target_arr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[27.94744 27.94744 27.94744 27.94744 27.94744 27.94744 27.94744 27.94744\n",
            " 27.94744 27.94744 27.94744 27.94744 27.94744 27.94744 27.94744 27.94744\n",
            " 27.94744 27.94744 27.94744 27.94744 27.94744 27.94744 27.94744 27.94744\n",
            " 31.09441 31.09441 31.09441 31.09441 31.09441 31.09441 31.09441 31.09441\n",
            " 31.09441 31.09441 31.09441 31.09441 31.09441 31.09441 31.09441 31.09441\n",
            " 31.09441 31.09441 31.09441 31.09441 31.09441 31.09441 31.09441 31.09441\n",
            " 31.09441 31.09441 31.09441 31.09441 31.09441 31.09441 31.09441 31.09441\n",
            " 31.09441 31.09441 31.09441 31.09441 31.09441 31.09441 31.09441 31.09441\n",
            " 31.09441 31.09441 31.09441 31.09441 31.09441 31.09441 31.09441 31.09441\n",
            " 31.09441 31.09441 31.09441 31.09441 31.09441 31.09441 31.09441 31.09441\n",
            " 31.09441 31.09441 31.09441 31.09441 31.09441 31.09441 31.09441 31.09441\n",
            " 31.09441 31.09441 31.09441 31.09441 31.09441 31.09441 31.09441 33.58142\n",
            " 33.58142 33.58142 33.58142 33.58142 33.58142 33.58142 33.58142 33.58142\n",
            " 33.58142 33.58142 33.58142 33.58142 33.58142 33.58142 33.58142 33.58142\n",
            " 33.58142 33.58142 33.58142 33.58142 33.58142 33.58142 33.58142 33.58142\n",
            " 33.58142 33.58142 33.58142 33.58142 33.58142 33.58142 33.58142 33.58142\n",
            " 33.58142 33.58142 33.58142 33.58142 33.58142 33.58142 33.58142 33.58142\n",
            " 33.58142 33.58142 33.58142 33.58142 33.58142 33.58142 33.58142]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvBI5i90PHMy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b96eba04-0c7c-4e0b-a69f-ae52b223ab06"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda:0\")\n",
        "  print(\"Cuda Activated\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print(\"Cuda Not Activated\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cuda Not Activated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxIYQFM0PU9v"
      },
      "source": [
        "test_size = (12, 18, 6)\n",
        "\n",
        "#10^3 tensor\n",
        "data_3 = torch.tensor(pcr_3_df.values.astype(np.float32)).to(device)\n",
        "target_3 = torch.tensor(target_3.astype(np.float32)).to(device)\n",
        "\n",
        "dataset_3 = data_utils.TensorDataset(data_3, target_3)\n",
        "splited_dataset_3 = data_utils.random_split(dataset_3, [12, 12, 12, 12])#48\n",
        "\n",
        "\n",
        "#10^4 tensor\n",
        "data_4 = torch.tensor(pcr_4_df.values.astype(np.float32)).to(device)\n",
        "target_4 = torch.tensor(target_4.astype(np.float32)).to(device)\n",
        "\n",
        "dataset_4 = data_utils.TensorDataset(data_4, target_4)\n",
        "splited_dataset_4 = data_utils.random_split(dataset_4, [18, 18, 18, 17])#71\n",
        "\n",
        "\n",
        "#10^5 tensor\n",
        "data_5 = torch.tensor(pcr_5_df.values.astype(np.float32)).to(device)\n",
        "target_5 = torch.tensor(target_5.astype(np.float32)).to(device)\n",
        "\n",
        "dataset_5 = data_utils.TensorDataset(data_5, target_5)\n",
        "splited_dataset_5 = data_utils.random_split(dataset_5, [6, 6, 6, 6])#24"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UHdA1zlPclx"
      },
      "source": [
        "#Definition of model\n",
        "import torch.optim as optim\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.c1 = nn.Conv1d(1, 10, 9)\n",
        "    self.p1 = nn.MaxPool1d(2)\n",
        "\n",
        "    self.fc1 = nn.Linear(160, 300)\n",
        "    self.fc2 = nn.Linear(300, 100)\n",
        "    self.fc3 = nn.Linear(100, 1)\n",
        "\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.c1(x))\n",
        "    x = self.p1(x)\n",
        "\n",
        "    x = x.view(-1)\n",
        "\n",
        "    x = F.relu(self.fc1(x))\n",
        "\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tmGOywvYUZe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "721680b0-8b95-4634-82ce-b915d9ecde17"
      },
      "source": [
        "model = Net()\n",
        "print(model)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "  print(\"model can use cuda\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (c1): Conv1d(1, 10, kernel_size=(9,), stride=(1,))\n",
            "  (p1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=160, out_features=300, bias=True)\n",
            "  (fc2): Linear(in_features=300, out_features=100, bias=True)\n",
            "  (fc3): Linear(in_features=100, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVGUvX7ratBo"
      },
      "source": [
        "from torch.optim import Adam\n",
        "from torch.nn import L1Loss\n",
        "loss_func = L1Loss()\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr = 0.00001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvWWxpWZceHL"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "model_path = Path(\"models\").expanduser()\n",
        "save_path = model_path.joinpath(f'pcr_model_{0}.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1b4uaDjcg8i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2b21648-4378-4809-d5dc-0909fbf2d9e4"
      },
      "source": [
        "BATCH_NUM = 27\n",
        "epochs_num = 3000\n",
        "iter_num = 1\n",
        "\n",
        "result_target = []\n",
        "result_output = []\n",
        "result_loss = []\n",
        "\n",
        "\n",
        "for iter in range(1, iter_num+1):\n",
        "\n",
        "  for test_ind in range(4):\n",
        "    print(f\"Network {test_ind} : start!!\")\n",
        "    model = Net()\n",
        "    loss_func = L1Loss()\n",
        "    optimizer = Adam(model.parameters(), lr = 0.00001)\n",
        "    save_path = model_path.joinpath(f'pcr_model_{iter}_{test_ind}.pt')\n",
        "\n",
        "    train_arr = []\n",
        "    test_arr = []\n",
        "    train_test = []\n",
        "    test_test = []\n",
        "\n",
        "    ####################################set train, test set######################################\n",
        "    for split_ind in range(4):\n",
        "      if split_ind == test_ind:\n",
        "        test_arr.append(splited_dataset_3[split_ind])\n",
        "        test_arr.append(splited_dataset_4[split_ind])\n",
        "        test_arr.append(splited_dataset_5[split_ind])\n",
        "        test_test.append(split_ind)\n",
        "        continue\n",
        "\n",
        "      train_arr.append(splited_dataset_3[split_ind])\n",
        "      train_arr.append(splited_dataset_4[split_ind])\n",
        "      train_arr.append(splited_dataset_5[split_ind])\n",
        "      train_test.append(split_ind)\n",
        "\n",
        "    #Concat Datasets\n",
        "    train_tensor = data_utils.ConcatDataset(train_arr)\n",
        "    test_tensor = data_utils.ConcatDataset(test_arr)\n",
        "\n",
        "    #Get train, test loaders\n",
        "    train_loader = data_utils.DataLoader(train_tensor, batch_size=BATCH_NUM, shuffle = True)\n",
        "    test_loader = data_utils.DataLoader(test_tensor, shuffle = False)\n",
        "    ############################################################################################\n",
        "\n",
        "    \n",
        "    #Init loss variable\n",
        "    valid_loss_min = np.Inf\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "\n",
        "    for epoch in range(1, epochs_num+1):\n",
        "      train_loss = 0.0\n",
        "      valid_loss = 0.0\n",
        "\n",
        "      loader_list = []\n",
        "      batch_len = 0\n",
        "\n",
        "      for i, loader in  enumerate(train_loader):\n",
        "        loader_list.append(loader)\n",
        "        batch_len += 1\n",
        "\n",
        "      for valid_ind in range(0, batch_len):\n",
        "        \n",
        "        #Actually, the train start is here\n",
        "        #model.train()\n",
        "\n",
        "        for train_ind in range(0, batch_len):\n",
        "        \n",
        "          if valid_ind == train_ind:\n",
        "            continue\n",
        "          \n",
        "          #####################\n",
        "          #     Train Start   #\n",
        "          #####################\n",
        "          model.train()\n",
        "          fold_train_loss = 0.0\n",
        "          \n",
        "          #Get data&target as much as batch size\n",
        "          data_loader, target_loader = loader_list[train_ind]\n",
        "          \n",
        "          #num of data in \"one\" batch size\n",
        "          data_num = len(target_loader)\n",
        "\n",
        "          for ind in range(0, data_num):\n",
        "            data = data_loader[ind]\n",
        "            target = target_loader[ind]\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "              data, target = data.cuda(), target.cuda()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data.view([1, 1, -1]))\n",
        "            loss = loss_func(output, target.view([-1]))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            fold_train_loss += loss.item()\n",
        "\n",
        "          train_loss += fold_train_loss / ((batch_len-1)*data_num)\n",
        "          ###########################################################\n",
        "\n",
        "        #####################\n",
        "        #  Validation Start #\n",
        "        #####################\n",
        "        model.eval()\n",
        "        fold_valid_loss = 0.0\n",
        "        \n",
        "        data_loader, target_loader = loader_list[valid_ind]\n",
        "        data_num = len(target_loader)\n",
        "        \n",
        "        for ind in range(0, data_num):\n",
        "          data = data_loader[ind]\n",
        "          target = target_loader[ind]\n",
        "\n",
        "          if torch.cuda.is_available():\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "\n",
        "          output = model(data.view([1, 1, -1]))\n",
        "          loss = loss_func(output, target.view([-1]))\n",
        "\n",
        "          fold_valid_loss += loss.item()\n",
        "\n",
        "        valid_loss += fold_valid_loss / data_num\n",
        "        ###########################################################\n",
        "    \n",
        "      train_loss = train_loss/batch_len\n",
        "      valid_loss = valid_loss/batch_len\n",
        "\n",
        "      train_losses.append(train_loss)\n",
        "      valid_losses.append(valid_loss) \n",
        "\n",
        "      print(f'test_ind: {test_ind}, Epoch: {epoch}, train_loss: {train_loss}, valid_loss: {valid_loss}')\n",
        "\n",
        "      if valid_loss <= valid_loss_min:\n",
        "        print(f'Validation loss decreased ({valid_loss_min} --> {valid_loss}).  Saving model ...')\n",
        "\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        valid_loss_min = valid_loss\n",
        "    \n",
        "    model.load_state_dict(torch.load(save_path))\n",
        "\n",
        "    loss_func = L1Loss()\n",
        "    for data, target in test_loader:\n",
        "      output = model(data.unsqueeze(1))\n",
        "      loss = loss_func(output, target)\n",
        "\n",
        "      result_target.append(target)\n",
        "      result_output.append(output)\n",
        "      result_loss.append(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "test_ind: 2, Epoch: 1106, train_loss: 1.4895130469475266, valid_loss: 1.5776361726288104\n",
            "test_ind: 2, Epoch: 1107, train_loss: 1.5320567955658306, valid_loss: 1.5065862296992898\n",
            "test_ind: 2, Epoch: 1108, train_loss: 1.6042078207021426, valid_loss: 1.4844697900647112\n",
            "test_ind: 2, Epoch: 1109, train_loss: 1.579467338940583, valid_loss: 1.5693743942130325\n",
            "test_ind: 2, Epoch: 1110, train_loss: 1.445499872323568, valid_loss: 1.5770545685053552\n",
            "test_ind: 2, Epoch: 1111, train_loss: 1.482706133343442, valid_loss: 1.3562955122727613\n",
            "test_ind: 2, Epoch: 1112, train_loss: 1.6138685931507337, valid_loss: 1.7833845194248732\n",
            "test_ind: 2, Epoch: 1113, train_loss: 1.7618697183197831, valid_loss: 1.6654728131416516\n",
            "test_ind: 2, Epoch: 1114, train_loss: 1.6069282905781488, valid_loss: 1.5836161860713251\n",
            "test_ind: 2, Epoch: 1115, train_loss: 1.48148839494102, valid_loss: 1.5038259599962807\n",
            "test_ind: 2, Epoch: 1116, train_loss: 1.4717235436127059, valid_loss: 1.6801377436374327\n",
            "test_ind: 2, Epoch: 1117, train_loss: 1.4450116755276323, valid_loss: 1.6036202595104858\n",
            "test_ind: 2, Epoch: 1118, train_loss: 1.5843982601437472, valid_loss: 1.5319144474475133\n",
            "test_ind: 2, Epoch: 1119, train_loss: 1.485584875796935, valid_loss: 1.4254367168133075\n",
            "test_ind: 2, Epoch: 1120, train_loss: 1.5127656892267503, valid_loss: 1.5956298082302778\n",
            "test_ind: 2, Epoch: 1121, train_loss: 1.5267632206847073, valid_loss: 1.323435798329845\n",
            "Validation loss decreased (1.324443980839178 --> 1.323435798329845).  Saving model ...\n",
            "test_ind: 2, Epoch: 1122, train_loss: 1.5565860357492851, valid_loss: 1.6811184597830486\n",
            "test_ind: 2, Epoch: 1123, train_loss: 1.572678117896983, valid_loss: 1.5277358167871111\n",
            "test_ind: 2, Epoch: 1124, train_loss: 1.5214063331046108, valid_loss: 1.569513611644082\n",
            "test_ind: 2, Epoch: 1125, train_loss: 1.6580555606544978, valid_loss: 1.610274833152097\n",
            "test_ind: 2, Epoch: 1126, train_loss: 1.5310763563299223, valid_loss: 1.6239486596523187\n",
            "test_ind: 2, Epoch: 1127, train_loss: 1.5510738172196026, valid_loss: 1.4416184404976347\n",
            "test_ind: 2, Epoch: 1128, train_loss: 1.477792960852526, valid_loss: 1.4686188738570256\n",
            "test_ind: 2, Epoch: 1129, train_loss: 1.5553985842046347, valid_loss: 1.5047696750727813\n",
            "test_ind: 2, Epoch: 1130, train_loss: 1.5298073674425667, valid_loss: 1.365461884740411\n",
            "test_ind: 2, Epoch: 1131, train_loss: 1.495583087517212, valid_loss: 1.3666675797215213\n",
            "test_ind: 2, Epoch: 1132, train_loss: 1.5202655602044866, valid_loss: 1.4720884632860494\n",
            "test_ind: 2, Epoch: 1133, train_loss: 1.508086389059808, valid_loss: 1.380413213686386\n",
            "test_ind: 2, Epoch: 1134, train_loss: 1.4943997531648145, valid_loss: 1.5112953478114897\n",
            "test_ind: 2, Epoch: 1135, train_loss: 1.4986847841728337, valid_loss: 1.612927471470629\n",
            "test_ind: 2, Epoch: 1136, train_loss: 1.4507211429780704, valid_loss: 1.3716762330796983\n",
            "test_ind: 2, Epoch: 1137, train_loss: 1.5123509047491033, valid_loss: 1.3040657077419793\n",
            "Validation loss decreased (1.323435798329845 --> 1.3040657077419793).  Saving model ...\n",
            "test_ind: 2, Epoch: 1138, train_loss: 1.5866284979487872, valid_loss: 1.5499577671713978\n",
            "test_ind: 2, Epoch: 1139, train_loss: 1.5234722006581332, valid_loss: 1.593758213554013\n",
            "test_ind: 2, Epoch: 1140, train_loss: 1.5117406476030777, valid_loss: 1.7509819727677565\n",
            "test_ind: 2, Epoch: 1141, train_loss: 1.5270804661518034, valid_loss: 1.3828499364717053\n",
            "test_ind: 2, Epoch: 1142, train_loss: 1.6420794045030787, valid_loss: 1.9672394729407765\n",
            "test_ind: 2, Epoch: 1143, train_loss: 1.509770249143059, valid_loss: 1.4256387320678798\n",
            "test_ind: 2, Epoch: 1144, train_loss: 1.487279622077036, valid_loss: 1.6552875992918965\n",
            "test_ind: 2, Epoch: 1145, train_loss: 1.4727461630122953, valid_loss: 1.4585925153857282\n",
            "test_ind: 2, Epoch: 1146, train_loss: 1.4854880318908836, valid_loss: 1.4503743308901447\n",
            "test_ind: 2, Epoch: 1147, train_loss: 1.497189738471963, valid_loss: 1.4281787648160233\n",
            "test_ind: 2, Epoch: 1148, train_loss: 1.4952439247486373, valid_loss: 1.6785590281853309\n",
            "test_ind: 2, Epoch: 1149, train_loss: 1.525864658871947, valid_loss: 1.53339924567785\n",
            "test_ind: 2, Epoch: 1150, train_loss: 1.5874940400336428, valid_loss: 2.2303121089935303\n",
            "test_ind: 2, Epoch: 1151, train_loss: 1.554509752269937, valid_loss: 1.608626871027498\n",
            "test_ind: 2, Epoch: 1152, train_loss: 1.5413789282723598, valid_loss: 1.41014328437653\n",
            "test_ind: 2, Epoch: 1153, train_loss: 1.460066941162573, valid_loss: 1.550786783892205\n",
            "test_ind: 2, Epoch: 1154, train_loss: 1.5125893810648978, valid_loss: 1.4511386315707129\n",
            "test_ind: 2, Epoch: 1155, train_loss: 1.4792803370828076, valid_loss: 1.3894208734191722\n",
            "test_ind: 2, Epoch: 1156, train_loss: 1.5664121805772482, valid_loss: 1.4974751384169966\n",
            "test_ind: 2, Epoch: 1157, train_loss: 1.510062734172781, valid_loss: 1.460198375913832\n",
            "test_ind: 2, Epoch: 1158, train_loss: 1.520643229271725, valid_loss: 1.3949835028743471\n",
            "test_ind: 2, Epoch: 1159, train_loss: 1.4619536816451175, valid_loss: 1.602264089122457\n",
            "test_ind: 2, Epoch: 1160, train_loss: 1.6667898462029265, valid_loss: 1.6840182980920515\n",
            "test_ind: 2, Epoch: 1161, train_loss: 1.5751044190960166, valid_loss: 1.7188445285514549\n",
            "test_ind: 2, Epoch: 1162, train_loss: 1.5363764559101851, valid_loss: 1.6462115752391329\n",
            "test_ind: 2, Epoch: 1163, train_loss: 1.4757840674147646, valid_loss: 1.3462175147825497\n",
            "test_ind: 2, Epoch: 1164, train_loss: 1.5650438148411592, valid_loss: 1.9617570662430548\n",
            "test_ind: 2, Epoch: 1165, train_loss: 1.5246406156226553, valid_loss: 1.4254964022894532\n",
            "test_ind: 2, Epoch: 1166, train_loss: 1.5591424573407442, valid_loss: 1.6072619362094804\n",
            "test_ind: 2, Epoch: 1167, train_loss: 1.4802113011804856, valid_loss: 1.4298558941593877\n",
            "test_ind: 2, Epoch: 1168, train_loss: 1.544828225857625, valid_loss: 1.444035114386143\n",
            "test_ind: 2, Epoch: 1169, train_loss: 1.4778272992412134, valid_loss: 1.3486566156403632\n",
            "test_ind: 2, Epoch: 1170, train_loss: 1.537482666720239, valid_loss: 1.5569676517421365\n",
            "test_ind: 2, Epoch: 1171, train_loss: 1.486413250847986, valid_loss: 1.492200350829339\n",
            "test_ind: 2, Epoch: 1172, train_loss: 1.4477588704735014, valid_loss: 1.4779631004713878\n",
            "test_ind: 2, Epoch: 1173, train_loss: 1.5993334343630368, valid_loss: 2.006513338822585\n",
            "test_ind: 2, Epoch: 1174, train_loss: 1.4422075945427615, valid_loss: 1.4594258025840476\n",
            "test_ind: 2, Epoch: 1175, train_loss: 1.454207019362128, valid_loss: 1.4941600822655223\n",
            "test_ind: 2, Epoch: 1176, train_loss: 1.6385322959787598, valid_loss: 1.6946076015461544\n",
            "test_ind: 2, Epoch: 1177, train_loss: 1.443402357364086, valid_loss: 1.754066859215413\n",
            "test_ind: 2, Epoch: 1178, train_loss: 1.50225714899083, valid_loss: 1.5615474634360722\n",
            "test_ind: 2, Epoch: 1179, train_loss: 1.4981290519746957, valid_loss: 1.5663552848022548\n",
            "test_ind: 2, Epoch: 1180, train_loss: 1.5650225706815946, valid_loss: 1.4190141380342662\n",
            "test_ind: 2, Epoch: 1181, train_loss: 1.4762408314493873, valid_loss: 1.3734407642288424\n",
            "test_ind: 2, Epoch: 1182, train_loss: 1.5011706322799496, valid_loss: 1.5643017489006719\n",
            "test_ind: 2, Epoch: 1183, train_loss: 1.6232094945844195, valid_loss: 2.0857886948816455\n",
            "test_ind: 2, Epoch: 1184, train_loss: 1.5634274537067467, valid_loss: 1.4087814904346085\n",
            "test_ind: 2, Epoch: 1185, train_loss: 1.4733815206761376, valid_loss: 1.315459229667642\n",
            "test_ind: 2, Epoch: 1186, train_loss: 1.4425801581014144, valid_loss: 1.4328826003604465\n",
            "test_ind: 2, Epoch: 1187, train_loss: 1.5557291971878675, valid_loss: 1.4207325348487267\n",
            "test_ind: 2, Epoch: 1188, train_loss: 1.4769369973982387, valid_loss: 1.401392621532125\n",
            "test_ind: 2, Epoch: 1189, train_loss: 1.523545675241483, valid_loss: 1.45635235003936\n",
            "test_ind: 2, Epoch: 1190, train_loss: 1.5171349567112649, valid_loss: 1.5020297789505743\n",
            "test_ind: 2, Epoch: 1191, train_loss: 1.61081791105207, valid_loss: 1.6168577215950035\n",
            "test_ind: 2, Epoch: 1192, train_loss: 1.5029170898064363, valid_loss: 1.4438563269427698\n",
            "test_ind: 2, Epoch: 1193, train_loss: 1.583637528496477, valid_loss: 1.4683509599788918\n",
            "test_ind: 2, Epoch: 1194, train_loss: 1.4593678392462808, valid_loss: 1.36502166894766\n",
            "test_ind: 2, Epoch: 1195, train_loss: 1.515880806833251, valid_loss: 1.4536711076046327\n",
            "test_ind: 2, Epoch: 1196, train_loss: 1.492243919843509, valid_loss: 1.655363603195234\n",
            "test_ind: 2, Epoch: 1197, train_loss: 1.4549820217085476, valid_loss: 1.7564440412059468\n",
            "test_ind: 2, Epoch: 1198, train_loss: 1.5135383619542135, valid_loss: 1.3627613331177972\n",
            "test_ind: 2, Epoch: 1199, train_loss: 1.4879671884284966, valid_loss: 1.4016585553813186\n",
            "test_ind: 2, Epoch: 1200, train_loss: 1.4649887028356336, valid_loss: 1.4562720702244685\n",
            "test_ind: 2, Epoch: 1201, train_loss: 1.4493109244793116, valid_loss: 1.3433821432271236\n",
            "test_ind: 2, Epoch: 1202, train_loss: 1.4699021723874954, valid_loss: 1.4242749302475541\n",
            "test_ind: 2, Epoch: 1203, train_loss: 1.376258309863345, valid_loss: 1.463665572326747\n",
            "test_ind: 2, Epoch: 1204, train_loss: 1.5156395371936098, valid_loss: 1.481606086095174\n",
            "test_ind: 2, Epoch: 1205, train_loss: 1.5468562084045845, valid_loss: 1.4454512433109121\n",
            "test_ind: 2, Epoch: 1206, train_loss: 1.4435498598973635, valid_loss: 1.688641717970541\n",
            "test_ind: 2, Epoch: 1207, train_loss: 1.4489605424524146, valid_loss: 1.380576293353002\n",
            "test_ind: 2, Epoch: 1208, train_loss: 1.5024951962437498, valid_loss: 1.5562189929505699\n",
            "test_ind: 2, Epoch: 1209, train_loss: 1.5233321520451237, valid_loss: 1.8412400725220681\n",
            "test_ind: 2, Epoch: 1210, train_loss: 1.48118429713779, valid_loss: 1.638250735410598\n",
            "test_ind: 2, Epoch: 1211, train_loss: 1.6107915025151351, valid_loss: 1.3834049640557706\n",
            "test_ind: 2, Epoch: 1212, train_loss: 1.5572386313254563, valid_loss: 1.4982157921859003\n",
            "test_ind: 2, Epoch: 1213, train_loss: 1.49765299958268, valid_loss: 1.5506129808235714\n",
            "test_ind: 2, Epoch: 1214, train_loss: 1.4264740543148118, valid_loss: 1.335787740528074\n",
            "test_ind: 2, Epoch: 1215, train_loss: 1.5146578662415855, valid_loss: 1.5108188862814185\n",
            "test_ind: 2, Epoch: 1216, train_loss: 1.5482284411405907, valid_loss: 1.839177810908043\n",
            "test_ind: 2, Epoch: 1217, train_loss: 1.517717515641128, valid_loss: 1.543977175003443\n",
            "test_ind: 2, Epoch: 1218, train_loss: 1.6353317375762737, valid_loss: 1.466788841448618\n",
            "test_ind: 2, Epoch: 1219, train_loss: 1.4753161131033758, valid_loss: 1.3314733090903346\n",
            "test_ind: 2, Epoch: 1220, train_loss: 1.5155327125379048, valid_loss: 1.3925077032159876\n",
            "test_ind: 2, Epoch: 1221, train_loss: 1.435479989418617, valid_loss: 1.4958876048737442\n",
            "test_ind: 2, Epoch: 1222, train_loss: 1.6712248212138698, valid_loss: 1.857805084299158\n",
            "test_ind: 2, Epoch: 1223, train_loss: 1.5051670346164976, valid_loss: 1.4872378182207417\n",
            "test_ind: 2, Epoch: 1224, train_loss: 1.5023506407724145, valid_loss: 1.5282224392958856\n",
            "test_ind: 2, Epoch: 1225, train_loss: 1.6538348336052235, valid_loss: 1.4392464256014919\n",
            "test_ind: 2, Epoch: 1226, train_loss: 1.4536218280919033, valid_loss: 1.5619966189066568\n",
            "test_ind: 2, Epoch: 1227, train_loss: 1.468960570926793, valid_loss: 1.791330591565863\n",
            "test_ind: 2, Epoch: 1228, train_loss: 1.4480714544385018, valid_loss: 1.4472856297452226\n",
            "test_ind: 2, Epoch: 1229, train_loss: 1.4580161480935325, valid_loss: 1.5036818403803727\n",
            "test_ind: 2, Epoch: 1230, train_loss: 1.5271048618291292, valid_loss: 1.5298175546858046\n",
            "test_ind: 2, Epoch: 1231, train_loss: 1.4940426222845589, valid_loss: 1.387951052086985\n",
            "test_ind: 2, Epoch: 1232, train_loss: 1.4250475850879638, valid_loss: 1.4459625907093712\n",
            "test_ind: 2, Epoch: 1233, train_loss: 1.455090256045234, valid_loss: 1.6620150794330826\n",
            "test_ind: 2, Epoch: 1234, train_loss: 1.5325881646908923, valid_loss: 1.5783731971371209\n",
            "test_ind: 2, Epoch: 1235, train_loss: 1.4088776464815491, valid_loss: 1.4058898126977122\n",
            "test_ind: 2, Epoch: 1236, train_loss: 1.5132063442712043, valid_loss: 1.3660000294362038\n",
            "test_ind: 2, Epoch: 1237, train_loss: 1.4813806725363448, valid_loss: 1.514988194164048\n",
            "test_ind: 2, Epoch: 1238, train_loss: 1.4662242639891216, valid_loss: 1.4288379623000098\n",
            "test_ind: 2, Epoch: 1239, train_loss: 1.5706716801932281, valid_loss: 1.7631831739702797\n",
            "test_ind: 2, Epoch: 1240, train_loss: 1.5563840617028624, valid_loss: 1.5001045400940116\n",
            "test_ind: 2, Epoch: 1241, train_loss: 1.4411903858637651, valid_loss: 1.5685723935097369\n",
            "test_ind: 2, Epoch: 1242, train_loss: 1.5728013098862552, valid_loss: 1.8248349574216751\n",
            "test_ind: 2, Epoch: 1243, train_loss: 1.4690595956949086, valid_loss: 1.5878489221263137\n",
            "test_ind: 2, Epoch: 1244, train_loss: 1.4856259410763963, valid_loss: 1.3555602477147028\n",
            "test_ind: 2, Epoch: 1245, train_loss: 1.4767735479903696, valid_loss: 1.6244568750049995\n",
            "test_ind: 2, Epoch: 1246, train_loss: 1.5004881508329995, valid_loss: 1.3518349132646523\n",
            "test_ind: 2, Epoch: 1247, train_loss: 1.441768155817972, valid_loss: 1.359692476413868\n",
            "test_ind: 2, Epoch: 1248, train_loss: 1.4996940718303942, valid_loss: 1.4723516454723824\n",
            "test_ind: 2, Epoch: 1249, train_loss: 1.5223931559809933, valid_loss: 1.4323344841981545\n",
            "test_ind: 2, Epoch: 1250, train_loss: 1.368055297664994, valid_loss: 1.3647384731857866\n",
            "test_ind: 2, Epoch: 1251, train_loss: 1.449069753784513, valid_loss: 1.505527179125707\n",
            "test_ind: 2, Epoch: 1252, train_loss: 1.3914428626709854, valid_loss: 1.2842464229659818\n",
            "Validation loss decreased (1.3040657077419793 --> 1.2842464229659818).  Saving model ...\n",
            "test_ind: 2, Epoch: 1253, train_loss: 1.443582168445062, valid_loss: 1.6765828227725124\n",
            "test_ind: 2, Epoch: 1254, train_loss: 1.4833787479291953, valid_loss: 1.69138187764377\n",
            "test_ind: 2, Epoch: 1255, train_loss: 1.514026766828662, valid_loss: 1.4437825143167435\n",
            "test_ind: 2, Epoch: 1256, train_loss: 1.587574613060367, valid_loss: 2.026164800013572\n",
            "test_ind: 2, Epoch: 1257, train_loss: 1.471806433126118, valid_loss: 1.7117724262411438\n",
            "test_ind: 2, Epoch: 1258, train_loss: 1.4982271042757676, valid_loss: 1.4296325727066082\n",
            "test_ind: 2, Epoch: 1259, train_loss: 1.4221539685302762, valid_loss: 1.5541464412993515\n",
            "test_ind: 2, Epoch: 1260, train_loss: 1.438905987644467, valid_loss: 1.6627002788065508\n",
            "test_ind: 2, Epoch: 1261, train_loss: 1.5364816338248402, valid_loss: 1.9602409482341887\n",
            "test_ind: 2, Epoch: 1262, train_loss: 1.455404095500283, valid_loss: 1.3775842393565383\n",
            "test_ind: 2, Epoch: 1263, train_loss: 1.6009582993198548, valid_loss: 1.4212914939619536\n",
            "test_ind: 2, Epoch: 1264, train_loss: 1.4623970442008427, valid_loss: 1.3097841433989696\n",
            "test_ind: 2, Epoch: 1265, train_loss: 1.4229415742307194, valid_loss: 1.3076377546685374\n",
            "test_ind: 2, Epoch: 1266, train_loss: 1.5191440466802004, valid_loss: 1.2620664437611897\n",
            "Validation loss decreased (1.2842464229659818 --> 1.2620664437611897).  Saving model ...\n",
            "test_ind: 2, Epoch: 1267, train_loss: 1.586722640230445, valid_loss: 1.39440136146002\n",
            "test_ind: 2, Epoch: 1268, train_loss: 1.472732324998716, valid_loss: 1.861348810358944\n",
            "test_ind: 2, Epoch: 1269, train_loss: 1.4457219339843488, valid_loss: 1.3658729740697095\n",
            "test_ind: 2, Epoch: 1270, train_loss: 1.4170111473242892, valid_loss: 1.5711205881884973\n",
            "test_ind: 2, Epoch: 1271, train_loss: 1.4630998822472145, valid_loss: 1.3629244195769654\n",
            "test_ind: 2, Epoch: 1272, train_loss: 1.4643550002903227, valid_loss: 1.6153905731320721\n",
            "test_ind: 2, Epoch: 1273, train_loss: 1.482878318426163, valid_loss: 1.3453619154090557\n",
            "test_ind: 2, Epoch: 1274, train_loss: 1.4216893192483264, valid_loss: 1.4627858865634666\n",
            "test_ind: 2, Epoch: 1275, train_loss: 1.544041038578392, valid_loss: 1.5500786046356898\n",
            "test_ind: 2, Epoch: 1276, train_loss: 1.4826010269770031, valid_loss: 1.9517988010689065\n",
            "test_ind: 2, Epoch: 1277, train_loss: 1.5397668516534009, valid_loss: 1.4533421837026916\n",
            "test_ind: 2, Epoch: 1278, train_loss: 1.4189751890876243, valid_loss: 1.3333293576525826\n",
            "test_ind: 2, Epoch: 1279, train_loss: 1.4190318835766567, valid_loss: 1.4592589415036716\n",
            "test_ind: 2, Epoch: 1280, train_loss: 1.5213909330304645, valid_loss: 1.2883305794153457\n",
            "test_ind: 2, Epoch: 1281, train_loss: 1.536971978199335, valid_loss: 1.5001247859748341\n",
            "test_ind: 2, Epoch: 1282, train_loss: 1.4897441637595266, valid_loss: 1.4152274498572717\n",
            "test_ind: 2, Epoch: 1283, train_loss: 1.6060172454131183, valid_loss: 1.4571468891241612\n",
            "test_ind: 2, Epoch: 1284, train_loss: 1.412208577280144, valid_loss: 1.2975205532845608\n",
            "test_ind: 2, Epoch: 1285, train_loss: 1.4876577738683108, valid_loss: 1.5149215762092179\n",
            "test_ind: 2, Epoch: 1286, train_loss: 1.5034961680061798, valid_loss: 1.5621274821778648\n",
            "test_ind: 2, Epoch: 1287, train_loss: 1.4138458114743573, valid_loss: 1.3303358092946544\n",
            "test_ind: 2, Epoch: 1288, train_loss: 1.459168624108107, valid_loss: 1.5961297366693827\n",
            "test_ind: 2, Epoch: 1289, train_loss: 1.4399178376791033, valid_loss: 1.3080888283558378\n",
            "test_ind: 2, Epoch: 1290, train_loss: 1.3998344852487359, valid_loss: 1.3800434509233872\n",
            "test_ind: 2, Epoch: 1291, train_loss: 1.3839134821298569, valid_loss: 1.340201995311639\n",
            "test_ind: 2, Epoch: 1292, train_loss: 1.4571094687281625, valid_loss: 1.4005159625300654\n",
            "test_ind: 2, Epoch: 1293, train_loss: 1.4748638751726433, valid_loss: 2.055007211842768\n",
            "test_ind: 2, Epoch: 1294, train_loss: 1.5410335999042335, valid_loss: 1.605657109507808\n",
            "test_ind: 2, Epoch: 1295, train_loss: 1.4160545361347687, valid_loss: 1.507380118736854\n",
            "test_ind: 2, Epoch: 1296, train_loss: 1.467690451758766, valid_loss: 1.3730778660190075\n",
            "test_ind: 2, Epoch: 1297, train_loss: 1.4690325047329054, valid_loss: 1.3686488876994858\n",
            "test_ind: 2, Epoch: 1298, train_loss: 1.4382660203283444, valid_loss: 1.345802421923037\n",
            "test_ind: 2, Epoch: 1299, train_loss: 1.516280778792509, valid_loss: 1.3992417965859087\n",
            "test_ind: 2, Epoch: 1300, train_loss: 1.4168646430697536, valid_loss: 1.3154713129385924\n",
            "test_ind: 2, Epoch: 1301, train_loss: 1.5370208632912052, valid_loss: 1.4906656531526834\n",
            "test_ind: 2, Epoch: 1302, train_loss: 1.3701285235902185, valid_loss: 1.407356155563963\n",
            "test_ind: 2, Epoch: 1303, train_loss: 1.4689981572874817, valid_loss: 1.6913637218312318\n",
            "test_ind: 2, Epoch: 1304, train_loss: 1.4028513173432091, valid_loss: 1.394283209770833\n",
            "test_ind: 2, Epoch: 1305, train_loss: 1.531938491723476, valid_loss: 1.3026850142030635\n",
            "test_ind: 2, Epoch: 1306, train_loss: 1.432964465783419, valid_loss: 1.4051325049495424\n",
            "test_ind: 2, Epoch: 1307, train_loss: 1.4686006286092177, valid_loss: 1.4293848198023955\n",
            "test_ind: 2, Epoch: 1308, train_loss: 1.4558698485719515, valid_loss: 1.413922985055168\n",
            "test_ind: 2, Epoch: 1309, train_loss: 1.4328774894178196, valid_loss: 1.5361399181887634\n",
            "test_ind: 2, Epoch: 1310, train_loss: 1.6004062360055311, valid_loss: 1.7072939132353522\n",
            "test_ind: 2, Epoch: 1311, train_loss: 1.466164430888177, valid_loss: 1.3614953956712685\n",
            "test_ind: 2, Epoch: 1312, train_loss: 1.4445830097904908, valid_loss: 1.5461768458711456\n",
            "test_ind: 2, Epoch: 1313, train_loss: 1.5722739510839596, valid_loss: 1.561461298554032\n",
            "test_ind: 2, Epoch: 1314, train_loss: 1.3992921393594622, valid_loss: 1.3125365541191862\n",
            "test_ind: 2, Epoch: 1315, train_loss: 1.5353978959470278, valid_loss: 1.4535072990971754\n",
            "test_ind: 2, Epoch: 1316, train_loss: 1.4912548939160586, valid_loss: 1.7824282666556857\n",
            "test_ind: 2, Epoch: 1317, train_loss: 1.507679826060818, valid_loss: 1.5083224657933596\n",
            "test_ind: 2, Epoch: 1318, train_loss: 1.4725146818930834, valid_loss: 1.8122046048145348\n",
            "test_ind: 2, Epoch: 1319, train_loss: 1.4902040430849661, valid_loss: 1.471521079370439\n",
            "test_ind: 2, Epoch: 1320, train_loss: 1.454960580112135, valid_loss: 1.4866460782510262\n",
            "test_ind: 2, Epoch: 1321, train_loss: 1.4130743666138972, valid_loss: 1.2983965187670499\n",
            "test_ind: 2, Epoch: 1322, train_loss: 1.433680460323975, valid_loss: 1.4614309537784327\n",
            "test_ind: 2, Epoch: 1323, train_loss: 1.472892310657845, valid_loss: 1.3480036245112406\n",
            "test_ind: 2, Epoch: 1324, train_loss: 1.4266459959977487, valid_loss: 1.3492625907615379\n",
            "test_ind: 2, Epoch: 1325, train_loss: 1.4212438183971963, valid_loss: 1.291689257336478\n",
            "test_ind: 2, Epoch: 1326, train_loss: 1.4564157770796267, valid_loss: 1.3525576842816127\n",
            "test_ind: 2, Epoch: 1327, train_loss: 1.4777697464000128, valid_loss: 1.4933629925774035\n",
            "test_ind: 2, Epoch: 1328, train_loss: 1.4375743198032505, valid_loss: 1.4277820430929504\n",
            "test_ind: 2, Epoch: 1329, train_loss: 1.4874701902963725, valid_loss: 1.431412397286831\n",
            "test_ind: 2, Epoch: 1330, train_loss: 1.4133168557430607, valid_loss: 1.2778168369901826\n",
            "test_ind: 2, Epoch: 1331, train_loss: 1.381689468340317, valid_loss: 1.304273924596629\n",
            "test_ind: 2, Epoch: 1332, train_loss: 1.5126159360945395, valid_loss: 1.65859816352866\n",
            "test_ind: 2, Epoch: 1333, train_loss: 1.479173133402695, valid_loss: 1.5728779891956906\n",
            "test_ind: 2, Epoch: 1334, train_loss: 1.4957382192186028, valid_loss: 1.650493341973025\n",
            "test_ind: 2, Epoch: 1335, train_loss: 1.511544821269152, valid_loss: 1.6187954845591488\n",
            "test_ind: 2, Epoch: 1336, train_loss: 1.515214604190272, valid_loss: 1.6942016840660334\n",
            "test_ind: 2, Epoch: 1337, train_loss: 1.4179551241404649, valid_loss: 1.379308841167352\n",
            "test_ind: 2, Epoch: 1338, train_loss: 1.389671201153472, valid_loss: 1.4938647285146252\n",
            "test_ind: 2, Epoch: 1339, train_loss: 1.4931954656005022, valid_loss: 1.4358515759818575\n",
            "test_ind: 2, Epoch: 1340, train_loss: 1.420557259154795, valid_loss: 1.4771941815346397\n",
            "test_ind: 2, Epoch: 1341, train_loss: 1.525040176632511, valid_loss: 1.3659852298236639\n",
            "test_ind: 2, Epoch: 1342, train_loss: 1.4819806674946407, valid_loss: 1.5045967808476202\n",
            "test_ind: 2, Epoch: 1343, train_loss: 1.4505058247366067, valid_loss: 1.40571902348445\n",
            "test_ind: 2, Epoch: 1344, train_loss: 1.4822216400733361, valid_loss: 1.6429813784411829\n",
            "test_ind: 2, Epoch: 1345, train_loss: 1.4070600211224098, valid_loss: 1.4831450379472173\n",
            "test_ind: 2, Epoch: 1346, train_loss: 1.5290850026202225, valid_loss: 1.494127549337186\n",
            "test_ind: 2, Epoch: 1347, train_loss: 1.4737705111163977, valid_loss: 1.5581255543265926\n",
            "test_ind: 2, Epoch: 1348, train_loss: 1.5213985739723341, valid_loss: 1.8091822655452283\n",
            "test_ind: 2, Epoch: 1349, train_loss: 1.3779498493795947, valid_loss: 1.341795539584255\n",
            "test_ind: 2, Epoch: 1350, train_loss: 1.4783523952179822, valid_loss: 1.4576091779942526\n",
            "test_ind: 2, Epoch: 1351, train_loss: 1.448521000480833, valid_loss: 1.5038685031086632\n",
            "test_ind: 2, Epoch: 1352, train_loss: 1.4286988660028064, valid_loss: 1.426802228998255\n",
            "test_ind: 2, Epoch: 1353, train_loss: 1.4376326370329826, valid_loss: 1.4045350599153088\n",
            "test_ind: 2, Epoch: 1354, train_loss: 1.5559173524662753, valid_loss: 1.3878654652511293\n",
            "test_ind: 2, Epoch: 1355, train_loss: 1.3819330972597246, valid_loss: 1.5914837988013897\n",
            "test_ind: 2, Epoch: 1356, train_loss: 1.3868270239599072, valid_loss: 1.3144636826637464\n",
            "test_ind: 2, Epoch: 1357, train_loss: 1.369742207604143, valid_loss: 1.40518050112276\n",
            "test_ind: 2, Epoch: 1358, train_loss: 1.46182058000157, valid_loss: 1.7336347171044417\n",
            "test_ind: 2, Epoch: 1359, train_loss: 1.4318025035169843, valid_loss: 1.450836001637994\n",
            "test_ind: 2, Epoch: 1360, train_loss: 1.428264716185509, valid_loss: 1.4949695432288015\n",
            "test_ind: 2, Epoch: 1361, train_loss: 1.402910857005903, valid_loss: 1.4151334198791417\n",
            "test_ind: 2, Epoch: 1362, train_loss: 1.4418242903742922, valid_loss: 1.3880317897198886\n",
            "test_ind: 2, Epoch: 1363, train_loss: 1.4709560173755585, valid_loss: 1.693505135696498\n",
            "test_ind: 2, Epoch: 1364, train_loss: 1.4655335631012805, valid_loss: 1.6424044863111273\n",
            "test_ind: 2, Epoch: 1365, train_loss: 1.4230521696585194, valid_loss: 1.6123197187385667\n",
            "test_ind: 2, Epoch: 1366, train_loss: 1.3632359495642972, valid_loss: 1.447873341731536\n",
            "test_ind: 2, Epoch: 1367, train_loss: 1.4241936360782141, valid_loss: 1.470977329460644\n",
            "test_ind: 2, Epoch: 1368, train_loss: 1.4100258988872214, valid_loss: 1.4843967456763285\n",
            "test_ind: 2, Epoch: 1369, train_loss: 1.3620725497674173, valid_loss: 1.3219967404661694\n",
            "test_ind: 2, Epoch: 1370, train_loss: 1.3956355862920895, valid_loss: 1.4213874285716956\n",
            "test_ind: 2, Epoch: 1371, train_loss: 1.3999187758392304, valid_loss: 1.2674366279884621\n",
            "test_ind: 2, Epoch: 1372, train_loss: 1.440252455324642, valid_loss: 1.289022886515343\n",
            "test_ind: 2, Epoch: 1373, train_loss: 1.4212859376543268, valid_loss: 1.3866555813031318\n",
            "test_ind: 2, Epoch: 1374, train_loss: 1.4619737674481283, valid_loss: 1.2849413825575782\n",
            "test_ind: 2, Epoch: 1375, train_loss: 1.426456499190299, valid_loss: 1.4242178157523826\n",
            "test_ind: 2, Epoch: 1376, train_loss: 1.4092632938540788, valid_loss: 1.5481767477812591\n",
            "test_ind: 2, Epoch: 1377, train_loss: 1.3419863172406143, valid_loss: 1.2472199768761965\n",
            "Validation loss decreased (1.2620664437611897 --> 1.2472199768761965).  Saving model ...\n",
            "test_ind: 2, Epoch: 1378, train_loss: 1.38226430413843, valid_loss: 1.3819852697203983\n",
            "test_ind: 2, Epoch: 1379, train_loss: 1.3977947450204906, valid_loss: 1.6118282096678036\n",
            "test_ind: 2, Epoch: 1380, train_loss: 1.5796722072934604, valid_loss: 1.302055494737761\n",
            "test_ind: 2, Epoch: 1381, train_loss: 1.3804764020816553, valid_loss: 1.4897078903991612\n",
            "test_ind: 2, Epoch: 1382, train_loss: 1.497929851553719, valid_loss: 1.5229977712332352\n",
            "test_ind: 2, Epoch: 1383, train_loss: 1.4086589772477112, valid_loss: 1.3757812874948876\n",
            "test_ind: 2, Epoch: 1384, train_loss: 1.3864314811974623, valid_loss: 1.3654776502538613\n",
            "test_ind: 2, Epoch: 1385, train_loss: 1.408979695293865, valid_loss: 1.3558743720041042\n",
            "test_ind: 2, Epoch: 1386, train_loss: 1.331494479890336, valid_loss: 1.6297191459568818\n",
            "test_ind: 2, Epoch: 1387, train_loss: 1.4320204128906597, valid_loss: 1.2694600715256825\n",
            "test_ind: 2, Epoch: 1388, train_loss: 1.3811656160798396, valid_loss: 1.538951162599091\n",
            "test_ind: 2, Epoch: 1389, train_loss: 1.3808507348737147, valid_loss: 1.2929282174830423\n",
            "test_ind: 2, Epoch: 1390, train_loss: 1.4006456445764612, valid_loss: 1.2958588905823536\n",
            "test_ind: 2, Epoch: 1391, train_loss: 1.4675133402644174, valid_loss: 1.5796738465627034\n",
            "test_ind: 2, Epoch: 1392, train_loss: 1.504992261571422, valid_loss: 1.5909793730135318\n",
            "test_ind: 2, Epoch: 1393, train_loss: 1.4597380378647067, valid_loss: 1.4272652378788702\n",
            "test_ind: 2, Epoch: 1394, train_loss: 1.5413338250470863, valid_loss: 1.4420915485447288\n",
            "test_ind: 2, Epoch: 1395, train_loss: 1.3624408356031685, valid_loss: 1.3104007013163335\n",
            "test_ind: 2, Epoch: 1396, train_loss: 1.4213804148088043, valid_loss: 1.3611131991416299\n",
            "test_ind: 2, Epoch: 1397, train_loss: 1.3665219082338744, valid_loss: 1.8285866102941357\n",
            "test_ind: 2, Epoch: 1398, train_loss: 1.45840263819536, valid_loss: 1.2948571657523131\n",
            "test_ind: 2, Epoch: 1399, train_loss: 1.3683985989091518, valid_loss: 1.3048519938759655\n",
            "test_ind: 2, Epoch: 1400, train_loss: 1.402621991953619, valid_loss: 1.4235475294270747\n",
            "test_ind: 2, Epoch: 1401, train_loss: 1.4820833951093304, valid_loss: 1.3572238286336262\n",
            "test_ind: 2, Epoch: 1402, train_loss: 1.3979669310088851, valid_loss: 1.3709919941730986\n",
            "test_ind: 2, Epoch: 1403, train_loss: 1.4431924412393164, valid_loss: 1.4393882391459583\n",
            "test_ind: 2, Epoch: 1404, train_loss: 1.4797052220854439, valid_loss: 1.4180302090115018\n",
            "test_ind: 2, Epoch: 1405, train_loss: 1.4457810020175073, valid_loss: 1.3237777354031206\n",
            "test_ind: 2, Epoch: 1406, train_loss: 1.470863837462205, valid_loss: 1.4788922468821208\n",
            "test_ind: 2, Epoch: 1407, train_loss: 1.3941106884567827, valid_loss: 1.3848284275783094\n",
            "test_ind: 2, Epoch: 1408, train_loss: 1.3965455967250147, valid_loss: 1.5042628494762629\n",
            "test_ind: 2, Epoch: 1409, train_loss: 1.371591520671718, valid_loss: 1.3256080517402062\n",
            "test_ind: 2, Epoch: 1410, train_loss: 1.4066314051973174, valid_loss: 1.5102014582381291\n",
            "test_ind: 2, Epoch: 1411, train_loss: 1.3931130685924014, valid_loss: 1.286710860722425\n",
            "test_ind: 2, Epoch: 1412, train_loss: 1.4236484416190036, valid_loss: 1.2688976033800348\n",
            "test_ind: 2, Epoch: 1413, train_loss: 1.3950985537634957, valid_loss: 1.5502199705509718\n",
            "test_ind: 2, Epoch: 1414, train_loss: 1.493895866252758, valid_loss: 1.4296196899522742\n",
            "test_ind: 2, Epoch: 1415, train_loss: 1.4225766701802458, valid_loss: 1.4293640291588938\n",
            "test_ind: 2, Epoch: 1416, train_loss: 1.3866749366803726, valid_loss: 1.4381196471700641\n",
            "test_ind: 2, Epoch: 1417, train_loss: 1.616644885125663, valid_loss: 1.612592823485024\n",
            "test_ind: 2, Epoch: 1418, train_loss: 1.467447091145167, valid_loss: 1.4302180275278553\n",
            "test_ind: 2, Epoch: 1419, train_loss: 1.3219251179853622, valid_loss: 1.3111122041685968\n",
            "test_ind: 2, Epoch: 1420, train_loss: 1.4491256682621447, valid_loss: 1.6225749311963378\n",
            "test_ind: 2, Epoch: 1421, train_loss: 1.3851451756041726, valid_loss: 1.2757534498502725\n",
            "test_ind: 2, Epoch: 1422, train_loss: 1.3770878663656267, valid_loss: 1.4693995771924315\n",
            "test_ind: 2, Epoch: 1423, train_loss: 1.3447296372619222, valid_loss: 1.3386736894265199\n",
            "test_ind: 2, Epoch: 1424, train_loss: 1.4051122726538243, valid_loss: 1.3231905403300228\n",
            "test_ind: 2, Epoch: 1425, train_loss: 1.3686165841329925, valid_loss: 1.2739509055417488\n",
            "test_ind: 2, Epoch: 1426, train_loss: 1.3807034168714358, valid_loss: 1.3709973283642718\n",
            "test_ind: 2, Epoch: 1427, train_loss: 1.3854906995978, valid_loss: 1.4179031217200124\n",
            "test_ind: 2, Epoch: 1428, train_loss: 1.3989684930440933, valid_loss: 1.3333920875505845\n",
            "test_ind: 2, Epoch: 1429, train_loss: 1.478742400012691, valid_loss: 1.4569082362020116\n",
            "test_ind: 2, Epoch: 1430, train_loss: 1.4455476377764316, valid_loss: 1.4213152162709468\n",
            "test_ind: 2, Epoch: 1431, train_loss: 1.5211871523915987, valid_loss: 1.2975634142883823\n",
            "test_ind: 2, Epoch: 1432, train_loss: 1.4621197380344866, valid_loss: 1.2740505672248341\n",
            "test_ind: 2, Epoch: 1433, train_loss: 1.4408997794275384, valid_loss: 1.5505383374684216\n",
            "test_ind: 2, Epoch: 1434, train_loss: 1.4154821488252733, valid_loss: 1.2584314237632641\n",
            "test_ind: 2, Epoch: 1435, train_loss: 1.4477318639655623, valid_loss: 1.3642340589452673\n",
            "test_ind: 2, Epoch: 1436, train_loss: 1.4275461232673523, valid_loss: 1.4065675151314152\n",
            "test_ind: 2, Epoch: 1437, train_loss: 1.3542647277980562, valid_loss: 1.2816074410734692\n",
            "test_ind: 2, Epoch: 1438, train_loss: 1.3243108128550385, valid_loss: 1.2714067069213955\n",
            "test_ind: 2, Epoch: 1439, train_loss: 1.321565275291885, valid_loss: 1.2760579592821606\n",
            "test_ind: 2, Epoch: 1440, train_loss: 1.430281394567245, valid_loss: 1.4527923870630075\n",
            "test_ind: 2, Epoch: 1441, train_loss: 1.472307319994326, valid_loss: 1.2962979380561415\n",
            "test_ind: 2, Epoch: 1442, train_loss: 1.3917155659776128, valid_loss: 1.3000361919403076\n",
            "test_ind: 2, Epoch: 1443, train_loss: 1.4679623032793587, valid_loss: 1.4438509533547945\n",
            "test_ind: 2, Epoch: 1444, train_loss: 1.4802269922022804, valid_loss: 1.5141224276985539\n",
            "test_ind: 2, Epoch: 1445, train_loss: 1.4361753090154754, valid_loss: 1.3872615924248328\n",
            "test_ind: 2, Epoch: 1446, train_loss: 1.462314489333831, valid_loss: 1.773255742852844\n",
            "test_ind: 2, Epoch: 1447, train_loss: 1.3541673896206297, valid_loss: 1.2055965549925454\n",
            "Validation loss decreased (1.2472199768761965 --> 1.2055965549925454).  Saving model ...\n",
            "test_ind: 2, Epoch: 1448, train_loss: 1.48375872238862, valid_loss: 1.3853206179420492\n",
            "test_ind: 2, Epoch: 1449, train_loss: 1.4266476450029826, valid_loss: 1.2926610487478751\n",
            "test_ind: 2, Epoch: 1450, train_loss: 1.4205876639765553, valid_loss: 1.5973287022691167\n",
            "test_ind: 2, Epoch: 1451, train_loss: 1.3724018001375258, valid_loss: 1.5110097760148875\n",
            "test_ind: 2, Epoch: 1452, train_loss: 1.471978091106795, valid_loss: 1.4781593373018793\n",
            "test_ind: 2, Epoch: 1453, train_loss: 1.4119887745957769, valid_loss: 1.3339864161619095\n",
            "test_ind: 2, Epoch: 1454, train_loss: 1.367046267218739, valid_loss: 1.9402534982078097\n",
            "test_ind: 2, Epoch: 1455, train_loss: 1.3721472361148932, valid_loss: 1.3359281079381957\n",
            "test_ind: 2, Epoch: 1456, train_loss: 1.4025424733574006, valid_loss: 1.3202803807380872\n",
            "test_ind: 2, Epoch: 1457, train_loss: 1.4206397990781923, valid_loss: 1.3770541200610649\n",
            "test_ind: 2, Epoch: 1458, train_loss: 1.4158025276966582, valid_loss: 1.6227050418527718\n",
            "test_ind: 2, Epoch: 1459, train_loss: 1.3990187275896497, valid_loss: 1.2360908720228407\n",
            "test_ind: 2, Epoch: 1460, train_loss: 1.3418807655091751, valid_loss: 1.3987480303500792\n",
            "test_ind: 2, Epoch: 1461, train_loss: 1.5172725577413302, valid_loss: 1.642233995290903\n",
            "test_ind: 2, Epoch: 1462, train_loss: 1.4337256862680232, valid_loss: 1.3773799866353005\n",
            "test_ind: 2, Epoch: 1463, train_loss: 1.3866455206277815, valid_loss: 1.427286861968516\n",
            "test_ind: 2, Epoch: 1464, train_loss: 1.4308510023644168, valid_loss: 1.4673219881845676\n",
            "test_ind: 2, Epoch: 1465, train_loss: 1.445542345699082, valid_loss: 1.6637743300522154\n",
            "test_ind: 2, Epoch: 1466, train_loss: 1.4033520271522253, valid_loss: 1.4826901502419063\n",
            "test_ind: 2, Epoch: 1467, train_loss: 1.5506555968426798, valid_loss: 1.4238119519334234\n",
            "test_ind: 2, Epoch: 1468, train_loss: 1.467503602235292, valid_loss: 1.4291922631766383\n",
            "test_ind: 2, Epoch: 1469, train_loss: 1.5520087187333211, valid_loss: 1.8497680232056184\n",
            "test_ind: 2, Epoch: 1470, train_loss: 1.386703021619168, valid_loss: 1.3094891965219437\n",
            "test_ind: 2, Epoch: 1471, train_loss: 1.34499954633903, valid_loss: 1.439093898843836\n",
            "test_ind: 2, Epoch: 1472, train_loss: 1.4215815882397513, valid_loss: 1.468956933741556\n",
            "test_ind: 2, Epoch: 1473, train_loss: 1.3904389073932502, valid_loss: 1.3493334024380415\n",
            "test_ind: 2, Epoch: 1474, train_loss: 1.4158094311937874, valid_loss: 1.2481042177249224\n",
            "test_ind: 2, Epoch: 1475, train_loss: 1.397012129581665, valid_loss: 1.5256341909750915\n",
            "test_ind: 2, Epoch: 1476, train_loss: 1.4428702091332968, valid_loss: 1.7526295714908176\n",
            "test_ind: 2, Epoch: 1477, train_loss: 1.3792955830113047, valid_loss: 1.4279682595505674\n",
            "test_ind: 2, Epoch: 1478, train_loss: 1.4131923570931808, valid_loss: 1.3763665514454204\n",
            "test_ind: 2, Epoch: 1479, train_loss: 1.3698975894978696, valid_loss: 1.2643083663407892\n",
            "test_ind: 2, Epoch: 1480, train_loss: 1.4263322704311565, valid_loss: 1.2967009632675737\n",
            "test_ind: 2, Epoch: 1481, train_loss: 1.3567406370429231, valid_loss: 1.3906447595340912\n",
            "test_ind: 2, Epoch: 1482, train_loss: 1.3831956377509427, valid_loss: 1.4625214002071283\n",
            "test_ind: 2, Epoch: 1483, train_loss: 1.407968146848543, valid_loss: 1.318714083429755\n",
            "test_ind: 2, Epoch: 1484, train_loss: 1.366576684732609, valid_loss: 1.3418808737371721\n",
            "test_ind: 2, Epoch: 1485, train_loss: 1.3692802527011967, valid_loss: 1.3063492217974104\n",
            "test_ind: 2, Epoch: 1486, train_loss: 1.3579088817408054, valid_loss: 1.3209328060476189\n",
            "test_ind: 2, Epoch: 1487, train_loss: 1.3732180022559386, valid_loss: 1.3288356510662285\n",
            "test_ind: 2, Epoch: 1488, train_loss: 1.4191233362340974, valid_loss: 1.2752139840030943\n",
            "test_ind: 2, Epoch: 1489, train_loss: 1.3829050100313858, valid_loss: 1.2988621312328894\n",
            "test_ind: 2, Epoch: 1490, train_loss: 1.4737325966754415, valid_loss: 1.7891576480322076\n",
            "test_ind: 2, Epoch: 1491, train_loss: 1.4662219250870112, valid_loss: 1.3384674872428266\n",
            "test_ind: 2, Epoch: 1492, train_loss: 1.3589789831174177, valid_loss: 1.3661198724708667\n",
            "test_ind: 2, Epoch: 1493, train_loss: 1.3193988152492193, valid_loss: 1.2083029040583857\n",
            "test_ind: 2, Epoch: 1494, train_loss: 1.3847482270098592, valid_loss: 1.2079487254476957\n",
            "test_ind: 2, Epoch: 1495, train_loss: 1.305082128711349, valid_loss: 1.377116842487259\n",
            "test_ind: 2, Epoch: 1496, train_loss: 1.3169239853408827, valid_loss: 1.4160142304890515\n",
            "test_ind: 2, Epoch: 1497, train_loss: 1.291711403094126, valid_loss: 1.4096286154200888\n",
            "test_ind: 2, Epoch: 1498, train_loss: 1.3859702352105383, valid_loss: 1.2866843378441966\n",
            "test_ind: 2, Epoch: 1499, train_loss: 1.3616389870530408, valid_loss: 1.6159368641356118\n",
            "test_ind: 2, Epoch: 1500, train_loss: 1.4693313247231448, valid_loss: 1.431637554766446\n",
            "test_ind: 2, Epoch: 1501, train_loss: 1.4751880017214465, valid_loss: 1.5693724610527018\n",
            "test_ind: 2, Epoch: 1502, train_loss: 1.389825759563464, valid_loss: 1.3389698453778216\n",
            "test_ind: 2, Epoch: 1503, train_loss: 1.3098672595572043, valid_loss: 1.3481681808786854\n",
            "test_ind: 2, Epoch: 1504, train_loss: 1.3527454177425347, valid_loss: 1.3182829146371609\n",
            "test_ind: 2, Epoch: 1505, train_loss: 1.3715080232248915, valid_loss: 1.3151016901021668\n",
            "test_ind: 2, Epoch: 1506, train_loss: 1.3937705720138913, valid_loss: 1.5954061580179766\n",
            "test_ind: 2, Epoch: 1507, train_loss: 1.3433241631343948, valid_loss: 1.3259355009790839\n",
            "test_ind: 2, Epoch: 1508, train_loss: 1.4355661547534941, valid_loss: 1.4923724910812162\n",
            "test_ind: 2, Epoch: 1509, train_loss: 1.3441200901640107, valid_loss: 1.2335846369762367\n",
            "test_ind: 2, Epoch: 1510, train_loss: 1.3898690546113655, valid_loss: 1.320489514587272\n",
            "test_ind: 2, Epoch: 1511, train_loss: 1.3109205824244283, valid_loss: 1.5005796709631243\n",
            "test_ind: 2, Epoch: 1512, train_loss: 1.4125925672699582, valid_loss: 1.495710415717883\n",
            "test_ind: 2, Epoch: 1513, train_loss: 1.4167646764010786, valid_loss: 1.535846149819529\n",
            "test_ind: 2, Epoch: 1514, train_loss: 1.4079404422926651, valid_loss: 1.2590552666927675\n",
            "test_ind: 2, Epoch: 1515, train_loss: 1.3702552893675743, valid_loss: 1.2728397391120931\n",
            "test_ind: 2, Epoch: 1516, train_loss: 1.3996856463940393, valid_loss: 1.4969098914382803\n",
            "test_ind: 2, Epoch: 1517, train_loss: 1.3820388745038936, valid_loss: 1.462201236659645\n",
            "test_ind: 2, Epoch: 1518, train_loss: 1.352751164015202, valid_loss: 1.3799082804948855\n",
            "test_ind: 2, Epoch: 1519, train_loss: 1.3066545730529235, valid_loss: 1.2335179653602448\n",
            "test_ind: 2, Epoch: 1520, train_loss: 1.3389426036211611, valid_loss: 1.3845224000110246\n",
            "test_ind: 2, Epoch: 1521, train_loss: 1.3559658178689926, valid_loss: 1.2217563611489755\n",
            "test_ind: 2, Epoch: 1522, train_loss: 1.3915052871305602, valid_loss: 1.4324797972654686\n",
            "test_ind: 2, Epoch: 1523, train_loss: 1.4487225384454105, valid_loss: 1.6414404153144595\n",
            "test_ind: 2, Epoch: 1524, train_loss: 1.4351404994754482, valid_loss: 1.2890589760239648\n",
            "test_ind: 2, Epoch: 1525, train_loss: 1.4653243278166508, valid_loss: 1.2993056332623518\n",
            "test_ind: 2, Epoch: 1526, train_loss: 1.3517677351506914, valid_loss: 1.4283217874347656\n",
            "test_ind: 2, Epoch: 1527, train_loss: 1.3350608228844685, valid_loss: 1.3193206875412553\n",
            "test_ind: 2, Epoch: 1528, train_loss: 1.3735632178677, valid_loss: 1.3500680407227954\n",
            "test_ind: 2, Epoch: 1529, train_loss: 1.3341563938916126, valid_loss: 1.401006182374438\n",
            "test_ind: 2, Epoch: 1530, train_loss: 1.3471983411939283, valid_loss: 1.3097653844078043\n",
            "test_ind: 2, Epoch: 1531, train_loss: 1.3128585745239985, valid_loss: 1.2773578771498808\n",
            "test_ind: 2, Epoch: 1532, train_loss: 1.399624321422233, valid_loss: 1.820314999659177\n",
            "test_ind: 2, Epoch: 1533, train_loss: 1.3885978658427993, valid_loss: 1.6667582160047658\n",
            "test_ind: 2, Epoch: 1534, train_loss: 1.3212255795343877, valid_loss: 1.351362516397764\n",
            "test_ind: 2, Epoch: 1535, train_loss: 1.417851730176413, valid_loss: 1.4058430364668537\n",
            "test_ind: 2, Epoch: 1536, train_loss: 1.4151698675816782, valid_loss: 1.262272499905013\n",
            "test_ind: 2, Epoch: 1537, train_loss: 1.4439081294357718, valid_loss: 1.4401687278367177\n",
            "test_ind: 2, Epoch: 1538, train_loss: 1.4475752838656433, valid_loss: 1.3501158507800852\n",
            "test_ind: 2, Epoch: 1539, train_loss: 1.3122403827714333, valid_loss: 1.217284407031502\n",
            "test_ind: 2, Epoch: 1540, train_loss: 1.2899824214457105, valid_loss: 1.3192602446955493\n",
            "test_ind: 2, Epoch: 1541, train_loss: 1.401361667645736, valid_loss: 1.721940469198417\n",
            "test_ind: 2, Epoch: 1542, train_loss: 1.5068310701835754, valid_loss: 2.005737989376753\n",
            "test_ind: 2, Epoch: 1543, train_loss: 1.390240289999662, valid_loss: 1.3333079149240783\n",
            "test_ind: 2, Epoch: 1544, train_loss: 1.3640199049472357, valid_loss: 1.283689312106184\n",
            "test_ind: 2, Epoch: 1545, train_loss: 1.3371704700212759, valid_loss: 1.3235193772872968\n",
            "test_ind: 2, Epoch: 1546, train_loss: 1.488583860913573, valid_loss: 1.3298408767776273\n",
            "test_ind: 2, Epoch: 1547, train_loss: 1.3618246143699713, valid_loss: 1.4511260816514324\n",
            "test_ind: 2, Epoch: 1548, train_loss: 1.3824455740331811, valid_loss: 1.263631566637262\n",
            "test_ind: 2, Epoch: 1549, train_loss: 1.3082956439069875, valid_loss: 1.310440962810462\n",
            "test_ind: 2, Epoch: 1550, train_loss: 1.3809401816905167, valid_loss: 1.3792481524312599\n",
            "test_ind: 2, Epoch: 1551, train_loss: 1.3752642284657768, valid_loss: 1.369591094829418\n",
            "test_ind: 2, Epoch: 1552, train_loss: 1.415502368667979, valid_loss: 1.595567339166277\n",
            "test_ind: 2, Epoch: 1553, train_loss: 1.3994235125249153, valid_loss: 1.6417855082074464\n",
            "test_ind: 2, Epoch: 1554, train_loss: 1.401126948290514, valid_loss: 1.2695622831328301\n",
            "test_ind: 2, Epoch: 1555, train_loss: 1.4727234973979924, valid_loss: 1.5115970926746682\n",
            "test_ind: 2, Epoch: 1556, train_loss: 1.39377269129015, valid_loss: 1.3443783389197455\n",
            "test_ind: 2, Epoch: 1557, train_loss: 1.405614917660937, valid_loss: 1.2533726556348666\n",
            "test_ind: 2, Epoch: 1558, train_loss: 1.3669644223998414, valid_loss: 1.4055285059828364\n",
            "test_ind: 2, Epoch: 1559, train_loss: 1.345486059487715, valid_loss: 1.4874271490635018\n",
            "test_ind: 2, Epoch: 1560, train_loss: 1.2958573896547556, valid_loss: 1.4449525490785255\n",
            "test_ind: 2, Epoch: 1561, train_loss: 1.4442552551131869, valid_loss: 1.3706637733003013\n",
            "test_ind: 2, Epoch: 1562, train_loss: 1.414043483797528, valid_loss: 1.3594633935183882\n",
            "test_ind: 2, Epoch: 1563, train_loss: 1.4143371656749324, valid_loss: 1.7296751795331298\n",
            "test_ind: 2, Epoch: 1564, train_loss: 1.4026458132074542, valid_loss: 1.8631484664743103\n",
            "test_ind: 2, Epoch: 1565, train_loss: 1.3962741371346787, valid_loss: 1.2401760799592716\n",
            "test_ind: 2, Epoch: 1566, train_loss: 1.3175937633568746, valid_loss: 1.297285132258706\n",
            "test_ind: 2, Epoch: 1567, train_loss: 1.360103554195828, valid_loss: 1.2890283674256415\n",
            "test_ind: 2, Epoch: 1568, train_loss: 1.344080540302925, valid_loss: 1.2692054044826757\n",
            "test_ind: 2, Epoch: 1569, train_loss: 1.3581926872474401, valid_loss: 1.3817065203631367\n",
            "test_ind: 2, Epoch: 1570, train_loss: 1.28851148432363, valid_loss: 1.333982933620442\n",
            "test_ind: 2, Epoch: 1571, train_loss: 1.3391113211060295, valid_loss: 1.2705938299836597\n",
            "test_ind: 2, Epoch: 1572, train_loss: 1.3060969169776098, valid_loss: 1.3327143593051836\n",
            "test_ind: 2, Epoch: 1573, train_loss: 1.373725470880724, valid_loss: 1.3214667441158892\n",
            "test_ind: 2, Epoch: 1574, train_loss: 1.4324169781586156, valid_loss: 1.3194763354766064\n",
            "test_ind: 2, Epoch: 1575, train_loss: 1.364567046151881, valid_loss: 1.457709599084664\n",
            "test_ind: 2, Epoch: 1576, train_loss: 1.377126494703809, valid_loss: 1.38115473690196\n",
            "test_ind: 2, Epoch: 1577, train_loss: 1.431988340270938, valid_loss: 1.5042593377268212\n",
            "test_ind: 2, Epoch: 1578, train_loss: 1.3859756237421281, valid_loss: 1.3440767754177083\n",
            "test_ind: 2, Epoch: 1579, train_loss: 1.3242545059943132, valid_loss: 1.3189339841532912\n",
            "test_ind: 2, Epoch: 1580, train_loss: 1.3535287660977326, valid_loss: 1.2629542146992478\n",
            "test_ind: 2, Epoch: 1581, train_loss: 1.322750357820777, valid_loss: 1.4508871020075262\n",
            "test_ind: 2, Epoch: 1582, train_loss: 1.3917999324182726, valid_loss: 1.291065492521324\n",
            "test_ind: 2, Epoch: 1583, train_loss: 1.312245413561945, valid_loss: 1.4320664772620568\n",
            "test_ind: 2, Epoch: 1584, train_loss: 1.3561987523679382, valid_loss: 1.3644825264259621\n",
            "test_ind: 2, Epoch: 1585, train_loss: 1.3804397746029062, valid_loss: 1.3204662847383069\n",
            "test_ind: 2, Epoch: 1586, train_loss: 1.3425106895841652, valid_loss: 1.4534738824578093\n",
            "test_ind: 2, Epoch: 1587, train_loss: 1.3245883393265014, valid_loss: 1.423062985439246\n",
            "test_ind: 2, Epoch: 1588, train_loss: 1.3945473827641008, valid_loss: 1.3952523025012762\n",
            "test_ind: 2, Epoch: 1589, train_loss: 1.4729674804357833, valid_loss: 1.4892597918496853\n",
            "test_ind: 2, Epoch: 1590, train_loss: 1.3551803262371847, valid_loss: 1.2447655914176223\n",
            "test_ind: 2, Epoch: 1591, train_loss: 1.3166522146516602, valid_loss: 1.2905402231080578\n",
            "test_ind: 2, Epoch: 1592, train_loss: 1.3115483788456785, valid_loss: 1.276002759607429\n",
            "test_ind: 2, Epoch: 1593, train_loss: 1.3733430275550254, valid_loss: 1.3483497675327833\n",
            "test_ind: 2, Epoch: 1594, train_loss: 1.3124842134296384, valid_loss: 1.2209355104343165\n",
            "test_ind: 2, Epoch: 1595, train_loss: 1.4021846661653727, valid_loss: 1.431562469895409\n",
            "test_ind: 2, Epoch: 1596, train_loss: 1.4163918237061242, valid_loss: 1.3932670412579833\n",
            "test_ind: 2, Epoch: 1597, train_loss: 1.3725228189629592, valid_loss: 1.4644087644723744\n",
            "test_ind: 2, Epoch: 1598, train_loss: 1.332495643429154, valid_loss: 1.2856494585673015\n",
            "test_ind: 2, Epoch: 1599, train_loss: 1.3186193601584728, valid_loss: 1.3026369616516635\n",
            "test_ind: 2, Epoch: 1600, train_loss: 1.3764308671779215, valid_loss: 1.5626659033305283\n",
            "test_ind: 2, Epoch: 1601, train_loss: 1.3118325416405543, valid_loss: 1.274693180013586\n",
            "test_ind: 2, Epoch: 1602, train_loss: 1.3150351699600873, valid_loss: 1.5333581704359789\n",
            "test_ind: 2, Epoch: 1603, train_loss: 1.4213845295104546, valid_loss: 1.5391986865942973\n",
            "test_ind: 2, Epoch: 1604, train_loss: 1.3218838280988443, valid_loss: 1.5556252586875545\n",
            "test_ind: 2, Epoch: 1605, train_loss: 1.3116444834956416, valid_loss: 1.4933662271907187\n",
            "test_ind: 2, Epoch: 1606, train_loss: 1.3366765303489487, valid_loss: 1.3258953529205757\n",
            "test_ind: 2, Epoch: 1607, train_loss: 1.3535020460090748, valid_loss: 1.179764062930376\n",
            "Validation loss decreased (1.2055965549925454 --> 1.179764062930376).  Saving model ...\n",
            "test_ind: 2, Epoch: 1608, train_loss: 1.420620040676193, valid_loss: 1.581133801033694\n",
            "test_ind: 2, Epoch: 1609, train_loss: 1.3549433925099748, valid_loss: 1.170727548436222\n",
            "Validation loss decreased (1.179764062930376 --> 1.170727548436222).  Saving model ...\n",
            "test_ind: 2, Epoch: 1610, train_loss: 1.3102804991820824, valid_loss: 1.309568685004514\n",
            "test_ind: 2, Epoch: 1611, train_loss: 1.30863586087965, valid_loss: 1.2563790955774463\n",
            "test_ind: 2, Epoch: 1612, train_loss: 1.3183351122302773, valid_loss: 1.4328888753200868\n",
            "test_ind: 2, Epoch: 1613, train_loss: 1.4231476401105339, valid_loss: 1.442285864441483\n",
            "test_ind: 2, Epoch: 1614, train_loss: 1.3259186930353033, valid_loss: 1.4099590037962653\n",
            "test_ind: 2, Epoch: 1615, train_loss: 1.342777406614617, valid_loss: 1.3664792969695523\n",
            "test_ind: 2, Epoch: 1616, train_loss: 1.3507966757499932, valid_loss: 1.3888669054732365\n",
            "test_ind: 2, Epoch: 1617, train_loss: 1.3327718593908964, valid_loss: 1.187524616888106\n",
            "test_ind: 2, Epoch: 1618, train_loss: 1.3753701512969796, valid_loss: 1.3595023454084696\n",
            "test_ind: 2, Epoch: 1619, train_loss: 1.283598082470871, valid_loss: 1.3731680269594544\n",
            "test_ind: 2, Epoch: 1620, train_loss: 1.2788008491085012, valid_loss: 1.3533592529785938\n",
            "test_ind: 2, Epoch: 1621, train_loss: 1.3141042871466162, valid_loss: 1.2739249573134288\n",
            "test_ind: 2, Epoch: 1622, train_loss: 1.3650490807445415, valid_loss: 1.5055344682133773\n",
            "test_ind: 2, Epoch: 1623, train_loss: 1.3024172472704736, valid_loss: 1.3789562873351269\n",
            "test_ind: 2, Epoch: 1624, train_loss: 1.376560079179711, valid_loss: 1.2822358166729964\n",
            "test_ind: 2, Epoch: 1625, train_loss: 1.3831622722821357, valid_loss: 1.4687845163535527\n",
            "test_ind: 2, Epoch: 1626, train_loss: 1.3195264201331796, valid_loss: 1.3452593580610053\n",
            "test_ind: 2, Epoch: 1627, train_loss: 1.349508202653325, valid_loss: 1.4213061441383472\n",
            "test_ind: 2, Epoch: 1628, train_loss: 1.2774548564541375, valid_loss: 1.2054189229622867\n",
            "test_ind: 2, Epoch: 1629, train_loss: 1.3419565126540427, valid_loss: 1.3181948546330813\n",
            "test_ind: 2, Epoch: 1630, train_loss: 1.3446019377350693, valid_loss: 1.2699980470869277\n",
            "test_ind: 2, Epoch: 1631, train_loss: 1.2894464616875136, valid_loss: 1.191063259401892\n",
            "test_ind: 2, Epoch: 1632, train_loss: 1.280192642356822, valid_loss: 1.3715822438568812\n",
            "test_ind: 2, Epoch: 1633, train_loss: 1.3791903584544591, valid_loss: 1.41608188838361\n",
            "test_ind: 2, Epoch: 1634, train_loss: 1.3524097719310242, valid_loss: 1.2651265373936407\n",
            "test_ind: 2, Epoch: 1635, train_loss: 1.308172043458915, valid_loss: 1.5333783083152226\n",
            "test_ind: 2, Epoch: 1636, train_loss: 1.2855539518883428, valid_loss: 1.305936539614642\n",
            "test_ind: 2, Epoch: 1637, train_loss: 1.3738652548106196, valid_loss: 1.3382708965203702\n",
            "test_ind: 2, Epoch: 1638, train_loss: 1.4012127026759889, valid_loss: 1.390461423797825\n",
            "test_ind: 2, Epoch: 1639, train_loss: 1.3464361964694, valid_loss: 1.3775142081442722\n",
            "test_ind: 2, Epoch: 1640, train_loss: 1.320944262139591, valid_loss: 1.5763495348797223\n",
            "test_ind: 2, Epoch: 1641, train_loss: 1.4036839212107863, valid_loss: 1.398493052887441\n",
            "test_ind: 2, Epoch: 1642, train_loss: 1.422673371216284, valid_loss: 1.441886580567754\n",
            "test_ind: 2, Epoch: 1643, train_loss: 1.3418883208648886, valid_loss: 1.284850633382118\n",
            "test_ind: 2, Epoch: 1644, train_loss: 1.3027418737511123, valid_loss: 1.2767006630911106\n",
            "test_ind: 2, Epoch: 1645, train_loss: 1.362325434897587, valid_loss: 1.193067146502329\n",
            "test_ind: 2, Epoch: 1646, train_loss: 1.3039778091742216, valid_loss: 1.5823402703657448\n",
            "test_ind: 2, Epoch: 1647, train_loss: 1.2657645529831236, valid_loss: 1.348964558028088\n",
            "test_ind: 2, Epoch: 1648, train_loss: 1.3594633552536777, valid_loss: 1.3236843636232902\n",
            "test_ind: 2, Epoch: 1649, train_loss: 1.3373291098494136, valid_loss: 1.2485676891783364\n",
            "test_ind: 2, Epoch: 1650, train_loss: 1.293510455125191, valid_loss: 1.236160437945287\n",
            "test_ind: 2, Epoch: 1651, train_loss: 1.379131695030988, valid_loss: 1.5723146685847529\n",
            "test_ind: 2, Epoch: 1652, train_loss: 1.3359743437083245, valid_loss: 1.3268876598771142\n",
            "test_ind: 2, Epoch: 1653, train_loss: 1.3987149802821086, valid_loss: 1.2947272079283016\n",
            "test_ind: 2, Epoch: 1654, train_loss: 1.3857659632437362, valid_loss: 1.6717086415684803\n",
            "test_ind: 2, Epoch: 1655, train_loss: 1.3980496246703782, valid_loss: 1.6658660482477259\n",
            "test_ind: 2, Epoch: 1656, train_loss: 1.41184668459444, valid_loss: 1.3158415246892858\n",
            "test_ind: 2, Epoch: 1657, train_loss: 1.3708991601369773, valid_loss: 1.431795995799225\n",
            "test_ind: 2, Epoch: 1658, train_loss: 1.2813962026652224, valid_loss: 1.306428675637965\n",
            "test_ind: 2, Epoch: 1659, train_loss: 1.381228261523777, valid_loss: 1.2205883607565506\n",
            "test_ind: 2, Epoch: 1660, train_loss: 1.3835437275179205, valid_loss: 1.250295938589634\n",
            "test_ind: 2, Epoch: 1661, train_loss: 1.3837667648156031, valid_loss: 1.379617465527309\n",
            "test_ind: 2, Epoch: 1662, train_loss: 1.4554169587826344, valid_loss: 1.4796682816964608\n",
            "test_ind: 2, Epoch: 1663, train_loss: 1.3400675870074847, valid_loss: 1.2392473519697487\n",
            "test_ind: 2, Epoch: 1664, train_loss: 1.3571190451398307, valid_loss: 1.2236748230762973\n",
            "test_ind: 2, Epoch: 1665, train_loss: 1.315191991195606, valid_loss: 1.507093980441406\n",
            "test_ind: 2, Epoch: 1666, train_loss: 1.3301482270812264, valid_loss: 1.254877587668916\n",
            "test_ind: 2, Epoch: 1667, train_loss: 1.254418253558993, valid_loss: 1.1625860613635464\n",
            "Validation loss decreased (1.170727548436222 --> 1.1625860613635464).  Saving model ...\n",
            "test_ind: 2, Epoch: 1668, train_loss: 1.2575673864551191, valid_loss: 1.2476049807676222\n",
            "test_ind: 2, Epoch: 1669, train_loss: 1.321660645214128, valid_loss: 1.227749319837304\n",
            "test_ind: 2, Epoch: 1670, train_loss: 1.3710566354952645, valid_loss: 1.2982920866746168\n",
            "test_ind: 2, Epoch: 1671, train_loss: 1.323342690327455, valid_loss: 1.221241643286159\n",
            "test_ind: 2, Epoch: 1672, train_loss: 1.317541716105578, valid_loss: 1.154720127752364\n",
            "Validation loss decreased (1.1625860613635464 --> 1.154720127752364).  Saving model ...\n",
            "test_ind: 2, Epoch: 1673, train_loss: 1.316791606877717, valid_loss: 1.336271491145816\n",
            "test_ind: 2, Epoch: 1674, train_loss: 1.306628637277616, valid_loss: 1.2589178947981265\n",
            "test_ind: 2, Epoch: 1675, train_loss: 1.3583382595638265, valid_loss: 1.2513583528350223\n",
            "test_ind: 2, Epoch: 1676, train_loss: 1.337263155753343, valid_loss: 1.2939719637574634\n",
            "test_ind: 2, Epoch: 1677, train_loss: 1.31331324373555, valid_loss: 1.2991875304795397\n",
            "test_ind: 2, Epoch: 1678, train_loss: 1.2778800618388149, valid_loss: 1.3976821770355574\n",
            "test_ind: 2, Epoch: 1679, train_loss: 1.28928741136281, valid_loss: 1.3352214692324995\n",
            "test_ind: 2, Epoch: 1680, train_loss: 1.3436891996396347, valid_loss: 1.4924590485727687\n",
            "test_ind: 2, Epoch: 1681, train_loss: 1.308143116922913, valid_loss: 1.405844256409213\n",
            "test_ind: 2, Epoch: 1682, train_loss: 1.294451666693402, valid_loss: 1.2550794460155346\n",
            "test_ind: 2, Epoch: 1683, train_loss: 1.4063862733125463, valid_loss: 1.8303924727643657\n",
            "test_ind: 2, Epoch: 1684, train_loss: 1.3204279955748932, valid_loss: 1.5368499946050833\n",
            "test_ind: 2, Epoch: 1685, train_loss: 1.3701826810610374, valid_loss: 1.193129677378554\n",
            "test_ind: 2, Epoch: 1686, train_loss: 1.3351719241309823, valid_loss: 1.3801524591581775\n",
            "test_ind: 2, Epoch: 1687, train_loss: 1.443984819160454, valid_loss: 1.500699896418471\n",
            "test_ind: 2, Epoch: 1688, train_loss: 1.3187979510706715, valid_loss: 1.2189321293790116\n",
            "test_ind: 2, Epoch: 1689, train_loss: 1.3594716430729272, valid_loss: 1.2833518954763385\n",
            "test_ind: 2, Epoch: 1690, train_loss: 1.2847302143390362, valid_loss: 1.166379684056991\n",
            "test_ind: 2, Epoch: 1691, train_loss: 1.2829774046442333, valid_loss: 1.3223263171323685\n",
            "test_ind: 2, Epoch: 1692, train_loss: 1.3393401270918015, valid_loss: 1.3792071933420296\n",
            "test_ind: 2, Epoch: 1693, train_loss: 1.2981646382457737, valid_loss: 1.2068759478055513\n",
            "test_ind: 2, Epoch: 1694, train_loss: 1.281362359227165, valid_loss: 1.2733429639767377\n",
            "test_ind: 2, Epoch: 1695, train_loss: 1.3537533887770778, valid_loss: 1.5150772778057306\n",
            "test_ind: 2, Epoch: 1696, train_loss: 1.2563705673018526, valid_loss: 1.2045799456430637\n",
            "test_ind: 2, Epoch: 1697, train_loss: 1.2920579706501758, valid_loss: 1.1984012548060838\n",
            "test_ind: 2, Epoch: 1698, train_loss: 1.2709228981141238, valid_loss: 1.1708443477282837\n",
            "test_ind: 2, Epoch: 1699, train_loss: 1.2448977543304676, valid_loss: 1.2494364260268687\n",
            "test_ind: 2, Epoch: 1700, train_loss: 1.3270443042798599, valid_loss: 1.3959052569506174\n",
            "test_ind: 2, Epoch: 1701, train_loss: 1.2825210710309962, valid_loss: 1.2307643564338357\n",
            "test_ind: 2, Epoch: 1702, train_loss: 1.2804283984819143, valid_loss: 1.261767416598111\n",
            "test_ind: 2, Epoch: 1703, train_loss: 1.3814057475141652, valid_loss: 1.4334467224925331\n",
            "test_ind: 2, Epoch: 1704, train_loss: 1.3305280156511412, valid_loss: 1.3589649845731904\n",
            "test_ind: 2, Epoch: 1705, train_loss: 1.3772094066326435, valid_loss: 1.330149031092978\n",
            "test_ind: 2, Epoch: 1706, train_loss: 1.332794481759284, valid_loss: 1.593511736970342\n",
            "test_ind: 2, Epoch: 1707, train_loss: 1.3367371042909106, valid_loss: 1.5563005361801538\n",
            "test_ind: 2, Epoch: 1708, train_loss: 1.378287049327028, valid_loss: 1.2630603625903443\n",
            "test_ind: 2, Epoch: 1709, train_loss: 1.4596971020965719, valid_loss: 1.4693476691884533\n",
            "test_ind: 2, Epoch: 1710, train_loss: 1.363503108563473, valid_loss: 1.4859476415520039\n",
            "test_ind: 2, Epoch: 1711, train_loss: 1.2864633233685325, valid_loss: 1.24697094085889\n",
            "test_ind: 2, Epoch: 1712, train_loss: 1.3741242564075464, valid_loss: 1.2804434774947642\n",
            "test_ind: 2, Epoch: 1713, train_loss: 1.2896850842695968, valid_loss: 1.1884590893389493\n",
            "test_ind: 2, Epoch: 1714, train_loss: 1.3224247237782418, valid_loss: 1.3938661620148227\n",
            "test_ind: 2, Epoch: 1715, train_loss: 1.2975097651268794, valid_loss: 1.292245311275167\n",
            "test_ind: 2, Epoch: 1716, train_loss: 1.3118400428822687, valid_loss: 1.4389853694839694\n",
            "test_ind: 2, Epoch: 1717, train_loss: 1.3046683069647549, valid_loss: 1.2399624759315424\n",
            "test_ind: 2, Epoch: 1718, train_loss: 1.2847269951221723, valid_loss: 1.2506746084262164\n",
            "test_ind: 2, Epoch: 1719, train_loss: 1.3429585522056646, valid_loss: 1.5238490885818785\n",
            "test_ind: 2, Epoch: 1720, train_loss: 1.243760989036089, valid_loss: 1.303985444229213\n",
            "test_ind: 2, Epoch: 1721, train_loss: 1.3391431643639082, valid_loss: 1.375722550938272\n",
            "test_ind: 2, Epoch: 1722, train_loss: 1.2554120926888694, valid_loss: 1.2028717084487959\n",
            "test_ind: 2, Epoch: 1723, train_loss: 1.334045106755589, valid_loss: 1.3231731561514049\n",
            "test_ind: 2, Epoch: 1724, train_loss: 1.3067561289523737, valid_loss: 1.174175623135689\n",
            "test_ind: 2, Epoch: 1725, train_loss: 1.364773507584647, valid_loss: 1.7998144416048314\n",
            "test_ind: 2, Epoch: 1726, train_loss: 1.3712358193972625, valid_loss: 1.4021005576152747\n",
            "test_ind: 2, Epoch: 1727, train_loss: 1.3104835718106003, valid_loss: 1.2636626839977383\n",
            "test_ind: 2, Epoch: 1728, train_loss: 1.2868097743191045, valid_loss: 1.2767438501374335\n",
            "test_ind: 2, Epoch: 1729, train_loss: 1.332285005256095, valid_loss: 1.6198038300897322\n",
            "test_ind: 2, Epoch: 1730, train_loss: 1.2872502631724503, valid_loss: 1.3770469362579518\n",
            "test_ind: 2, Epoch: 1731, train_loss: 1.2955022981250617, valid_loss: 1.3019564790263813\n",
            "test_ind: 2, Epoch: 1732, train_loss: 1.3914854132551753, valid_loss: 1.2330774262420132\n",
            "test_ind: 2, Epoch: 1733, train_loss: 1.2906762881156726, valid_loss: 1.2218330707984772\n",
            "test_ind: 2, Epoch: 1734, train_loss: 1.3912253198687057, valid_loss: 1.4358594920220877\n",
            "test_ind: 2, Epoch: 1735, train_loss: 1.3707151752591473, valid_loss: 1.2243619651196689\n",
            "test_ind: 2, Epoch: 1736, train_loss: 1.3807182282577326, valid_loss: 1.232099649912951\n",
            "test_ind: 2, Epoch: 1737, train_loss: 1.2847784693084892, valid_loss: 1.2556399422833042\n",
            "test_ind: 2, Epoch: 1738, train_loss: 1.2623381236566324, valid_loss: 1.293414200133408\n",
            "test_ind: 2, Epoch: 1739, train_loss: 1.2546611418638023, valid_loss: 1.2519562536495026\n",
            "test_ind: 2, Epoch: 1740, train_loss: 1.2846403937054496, valid_loss: 1.2674260655699292\n",
            "test_ind: 2, Epoch: 1741, train_loss: 1.3118752187926317, valid_loss: 1.2883645999125943\n",
            "test_ind: 2, Epoch: 1742, train_loss: 1.3750814198315537, valid_loss: 1.4910751924215897\n",
            "test_ind: 2, Epoch: 1743, train_loss: 1.334929852743774, valid_loss: 1.3450252901115307\n",
            "test_ind: 2, Epoch: 1744, train_loss: 1.2953436297682053, valid_loss: 1.252270006386303\n",
            "test_ind: 2, Epoch: 1745, train_loss: 1.2892120104116818, valid_loss: 1.4044619041290718\n",
            "test_ind: 2, Epoch: 1746, train_loss: 1.288896220135666, valid_loss: 1.2277806946355054\n",
            "test_ind: 2, Epoch: 1747, train_loss: 1.3479236707388507, valid_loss: 1.3415066421541393\n",
            "test_ind: 2, Epoch: 1748, train_loss: 1.4190378093991183, valid_loss: 1.2049640603894183\n",
            "test_ind: 2, Epoch: 1749, train_loss: 1.3310662513671323, valid_loss: 1.431107235090685\n",
            "test_ind: 2, Epoch: 1750, train_loss: 1.2947800496817767, valid_loss: 1.2979370392965115\n",
            "test_ind: 2, Epoch: 1751, train_loss: 1.315838997407064, valid_loss: 1.2632027820304588\n",
            "test_ind: 2, Epoch: 1752, train_loss: 1.2992151308829516, valid_loss: 1.3449016731349155\n",
            "test_ind: 2, Epoch: 1753, train_loss: 1.2993426965965278, valid_loss: 1.3193453526564813\n",
            "test_ind: 2, Epoch: 1754, train_loss: 1.3128358120026187, valid_loss: 1.1722595358845855\n",
            "test_ind: 2, Epoch: 1755, train_loss: 1.2775797130715132, valid_loss: 1.2305637125955347\n",
            "test_ind: 2, Epoch: 1756, train_loss: 1.2968502404682996, valid_loss: 1.3595498953110132\n",
            "test_ind: 2, Epoch: 1757, train_loss: 1.432389474209444, valid_loss: 1.2607931744339118\n",
            "test_ind: 2, Epoch: 1758, train_loss: 1.3155402842863109, valid_loss: 1.2137867915324678\n",
            "test_ind: 2, Epoch: 1759, train_loss: 1.4106899630083771, valid_loss: 1.4323470191738203\n",
            "test_ind: 2, Epoch: 1760, train_loss: 1.363463641571523, valid_loss: 1.4670124013199763\n",
            "test_ind: 2, Epoch: 1761, train_loss: 1.3381204734161032, valid_loss: 1.6315225305041017\n",
            "test_ind: 2, Epoch: 1762, train_loss: 1.3272936242258446, valid_loss: 1.2171213022324434\n",
            "test_ind: 2, Epoch: 1763, train_loss: 1.297082039252532, valid_loss: 1.1409662367611528\n",
            "Validation loss decreased (1.154720127752364 --> 1.1409662367611528).  Saving model ...\n",
            "test_ind: 2, Epoch: 1764, train_loss: 1.2952000852550423, valid_loss: 1.206144580134639\n",
            "test_ind: 2, Epoch: 1765, train_loss: 1.3127967980738946, valid_loss: 1.2375053727728689\n",
            "test_ind: 2, Epoch: 1766, train_loss: 1.2704679363699947, valid_loss: 1.2974212461726957\n",
            "test_ind: 2, Epoch: 1767, train_loss: 1.2680174964785238, valid_loss: 1.3066241598536825\n",
            "test_ind: 2, Epoch: 1768, train_loss: 1.313855835062373, valid_loss: 1.256946161601618\n",
            "test_ind: 2, Epoch: 1769, train_loss: 1.2840270142496366, valid_loss: 1.4267665273443586\n",
            "test_ind: 2, Epoch: 1770, train_loss: 1.2564862333698037, valid_loss: 1.4868598629606415\n",
            "test_ind: 2, Epoch: 1771, train_loss: 1.3565466118673992, valid_loss: 1.1970508146150158\n",
            "test_ind: 2, Epoch: 1772, train_loss: 1.258079550318342, valid_loss: 1.225230479172492\n",
            "test_ind: 2, Epoch: 1773, train_loss: 1.2241162610755913, valid_loss: 1.20425584037759\n",
            "test_ind: 2, Epoch: 1774, train_loss: 1.3452686901219326, valid_loss: 1.339564277915194\n",
            "test_ind: 2, Epoch: 1775, train_loss: 1.2450080678673552, valid_loss: 1.3108643792633317\n",
            "test_ind: 2, Epoch: 1776, train_loss: 1.239330085480881, valid_loss: 1.3388782111328212\n",
            "test_ind: 2, Epoch: 1777, train_loss: 1.2793514033441644, valid_loss: 1.251827002930166\n",
            "test_ind: 2, Epoch: 1778, train_loss: 1.3183335583207727, valid_loss: 1.230281458960639\n",
            "test_ind: 2, Epoch: 1779, train_loss: 1.254390980330175, valid_loss: 1.177091472169273\n",
            "test_ind: 2, Epoch: 1780, train_loss: 1.3542787827204663, valid_loss: 1.1817886510126272\n",
            "test_ind: 2, Epoch: 1781, train_loss: 1.3107422124965917, valid_loss: 1.3091518091000722\n",
            "test_ind: 2, Epoch: 1782, train_loss: 1.3048852490790097, valid_loss: 1.178312143369278\n",
            "test_ind: 2, Epoch: 1783, train_loss: 1.2504205447882557, valid_loss: 1.154383501775584\n",
            "test_ind: 2, Epoch: 1784, train_loss: 1.348983002524091, valid_loss: 1.297370957513141\n",
            "test_ind: 2, Epoch: 1785, train_loss: 1.32125871156582, valid_loss: 1.1456120075323641\n",
            "test_ind: 2, Epoch: 1786, train_loss: 1.2838886839258932, valid_loss: 1.396092044661867\n",
            "test_ind: 2, Epoch: 1787, train_loss: 1.3371938853069136, valid_loss: 1.5559340936166268\n",
            "test_ind: 2, Epoch: 1788, train_loss: 1.3662963694656678, valid_loss: 1.2404840698948614\n",
            "test_ind: 2, Epoch: 1789, train_loss: 1.2974165842177634, valid_loss: 1.2218671802781587\n",
            "test_ind: 2, Epoch: 1790, train_loss: 1.342646168168114, valid_loss: 1.2836414857467693\n",
            "test_ind: 2, Epoch: 1791, train_loss: 1.3026314150800278, valid_loss: 1.5151139473983026\n",
            "test_ind: 2, Epoch: 1792, train_loss: 1.3765521429882428, valid_loss: 1.3253927434611525\n",
            "test_ind: 2, Epoch: 1793, train_loss: 1.2151057643202068, valid_loss: 1.4288200639251971\n",
            "test_ind: 2, Epoch: 1794, train_loss: 1.2540788068617397, valid_loss: 1.2774048693838962\n",
            "test_ind: 2, Epoch: 1795, train_loss: 1.2640540543218397, valid_loss: 1.190893650734187\n",
            "test_ind: 2, Epoch: 1796, train_loss: 1.2817681650830128, valid_loss: 1.2479652828640408\n",
            "test_ind: 2, Epoch: 1797, train_loss: 1.5365770368041702, valid_loss: 1.4107952994159145\n",
            "test_ind: 2, Epoch: 1798, train_loss: 1.3555607610052243, valid_loss: 1.3315542613678848\n",
            "test_ind: 2, Epoch: 1799, train_loss: 1.2332123163645312, valid_loss: 1.216701650891209\n",
            "test_ind: 2, Epoch: 1800, train_loss: 1.307716523140584, valid_loss: 1.6407246956458459\n",
            "test_ind: 2, Epoch: 1801, train_loss: 1.3353299073457492, valid_loss: 1.477154122458564\n",
            "test_ind: 2, Epoch: 1802, train_loss: 1.2943109791276575, valid_loss: 1.1893188491506115\n",
            "test_ind: 2, Epoch: 1803, train_loss: 1.2714261125635218, valid_loss: 1.4353814192986556\n",
            "test_ind: 2, Epoch: 1804, train_loss: 1.3091956688354724, valid_loss: 1.2465053698276183\n",
            "test_ind: 2, Epoch: 1805, train_loss: 1.2624112171778086, valid_loss: 1.2633599250065295\n",
            "test_ind: 2, Epoch: 1806, train_loss: 1.2213620023283636, valid_loss: 1.1641316223687936\n",
            "test_ind: 2, Epoch: 1807, train_loss: 1.3268565751208878, valid_loss: 1.5201393185857355\n",
            "test_ind: 2, Epoch: 1808, train_loss: 1.2942905872069643, valid_loss: 1.3335584582086981\n",
            "test_ind: 2, Epoch: 1809, train_loss: 1.3450379527871767, valid_loss: 1.2821393896032263\n",
            "test_ind: 2, Epoch: 1810, train_loss: 1.3248692339981383, valid_loss: 1.2337667636382275\n",
            "test_ind: 2, Epoch: 1811, train_loss: 1.3405187881683465, valid_loss: 1.2185028142738885\n",
            "test_ind: 2, Epoch: 1812, train_loss: 1.3039377419470835, valid_loss: 1.2258730315075301\n",
            "test_ind: 2, Epoch: 1813, train_loss: 1.2550513866620185, valid_loss: 1.4043608520105693\n",
            "test_ind: 2, Epoch: 1814, train_loss: 1.360687386502794, valid_loss: 1.4467957773779192\n",
            "test_ind: 2, Epoch: 1815, train_loss: 1.292115790665093, valid_loss: 1.2784063870410973\n",
            "test_ind: 2, Epoch: 1816, train_loss: 1.2481582692319286, valid_loss: 1.1045268478556576\n",
            "Validation loss decreased (1.1409662367611528 --> 1.1045268478556576).  Saving model ...\n",
            "test_ind: 2, Epoch: 1817, train_loss: 1.2595517359115007, valid_loss: 1.21525064492837\n",
            "test_ind: 2, Epoch: 1818, train_loss: 1.2650329758751877, valid_loss: 1.3398910553706678\n",
            "test_ind: 2, Epoch: 1819, train_loss: 1.2942839748839028, valid_loss: 1.1585382695211643\n",
            "test_ind: 2, Epoch: 1820, train_loss: 1.2042726897559386, valid_loss: 1.2268729243862664\n",
            "test_ind: 2, Epoch: 1821, train_loss: 1.2797711402715555, valid_loss: 1.4898189893814913\n",
            "test_ind: 2, Epoch: 1822, train_loss: 1.2466582131182027, valid_loss: 1.2208307305632153\n",
            "test_ind: 2, Epoch: 1823, train_loss: 1.4809078384555188, valid_loss: 1.5315418331711381\n",
            "test_ind: 2, Epoch: 1824, train_loss: 1.3521359908727952, valid_loss: 1.2632362360288614\n",
            "test_ind: 2, Epoch: 1825, train_loss: 1.2825873230936862, valid_loss: 1.2498471872759\n",
            "test_ind: 2, Epoch: 1826, train_loss: 1.2827510362789953, valid_loss: 1.2463630590683374\n",
            "test_ind: 2, Epoch: 1827, train_loss: 1.268736866238224, valid_loss: 1.3048149982408919\n",
            "test_ind: 2, Epoch: 1828, train_loss: 1.2850419723297002, valid_loss: 1.1874762973894082\n",
            "test_ind: 2, Epoch: 1829, train_loss: 1.269481431837888, valid_loss: 1.4204662477868237\n",
            "test_ind: 2, Epoch: 1830, train_loss: 1.2179835078157024, valid_loss: 1.3367365853399291\n",
            "test_ind: 2, Epoch: 1831, train_loss: 1.324549469399882, valid_loss: 1.1598468852518629\n",
            "test_ind: 2, Epoch: 1832, train_loss: 1.253648421250404, valid_loss: 1.2469106956764504\n",
            "test_ind: 2, Epoch: 1833, train_loss: 1.3330440822376712, valid_loss: 1.2115181681097744\n",
            "test_ind: 2, Epoch: 1834, train_loss: 1.3657977162150121, valid_loss: 1.396381622705704\n",
            "test_ind: 2, Epoch: 1835, train_loss: 1.3085007253195826, valid_loss: 1.2436807522406945\n",
            "test_ind: 2, Epoch: 1836, train_loss: 1.2884775276763711, valid_loss: 1.2003960195090355\n",
            "test_ind: 2, Epoch: 1837, train_loss: 1.2744136673998856, valid_loss: 1.3721659278597926\n",
            "test_ind: 2, Epoch: 1838, train_loss: 1.2903864700683276, valid_loss: 1.337910843710614\n",
            "test_ind: 2, Epoch: 1839, train_loss: 1.2913257926277963, valid_loss: 1.1812779278497072\n",
            "test_ind: 2, Epoch: 1840, train_loss: 1.272489732713328, valid_loss: 1.3149547862191486\n",
            "test_ind: 2, Epoch: 1841, train_loss: 1.3260210869092661, valid_loss: 1.1919885211520724\n",
            "test_ind: 2, Epoch: 1842, train_loss: 1.236853159390963, valid_loss: 1.2213629338136764\n",
            "test_ind: 2, Epoch: 1843, train_loss: 1.3042211337872947, valid_loss: 1.401681739040929\n",
            "test_ind: 2, Epoch: 1844, train_loss: 1.2670838044919182, valid_loss: 1.3858170088200148\n",
            "test_ind: 2, Epoch: 1845, train_loss: 1.235762547223996, valid_loss: 1.1971782143639025\n",
            "test_ind: 2, Epoch: 1846, train_loss: 1.2639131104504622, valid_loss: 1.5791833618087985\n",
            "test_ind: 2, Epoch: 1847, train_loss: 1.366027815955543, valid_loss: 1.376574693581997\n",
            "test_ind: 2, Epoch: 1848, train_loss: 1.3471779538016033, valid_loss: 1.3226892982113396\n",
            "test_ind: 2, Epoch: 1849, train_loss: 1.6111927689084076, valid_loss: 1.5561375020236372\n",
            "test_ind: 2, Epoch: 1850, train_loss: 1.332724350017247, valid_loss: 1.359231912852013\n",
            "test_ind: 2, Epoch: 1851, train_loss: 1.3054236512125272, valid_loss: 1.312084107657104\n",
            "test_ind: 2, Epoch: 1852, train_loss: 1.334259833365764, valid_loss: 1.195534063540293\n",
            "test_ind: 2, Epoch: 1853, train_loss: 1.2203635379233366, valid_loss: 1.2402281054744013\n",
            "test_ind: 2, Epoch: 1854, train_loss: 1.2521468551523438, valid_loss: 1.5066106923964628\n",
            "test_ind: 2, Epoch: 1855, train_loss: 1.3444591870448304, valid_loss: 1.3070076619118367\n",
            "test_ind: 2, Epoch: 1856, train_loss: 1.246363188353246, valid_loss: 1.1648949102798418\n",
            "test_ind: 2, Epoch: 1857, train_loss: 1.2549306832374219, valid_loss: 1.3627822331213884\n",
            "test_ind: 2, Epoch: 1858, train_loss: 1.3284625212351482, valid_loss: 1.4667185256284188\n",
            "test_ind: 2, Epoch: 1859, train_loss: 1.2517406641587911, valid_loss: 1.2995440491244323\n",
            "test_ind: 2, Epoch: 1860, train_loss: 1.309838658837285, valid_loss: 1.3296110521354565\n",
            "test_ind: 2, Epoch: 1861, train_loss: 1.2749348162246226, valid_loss: 1.2623014191956263\n",
            "test_ind: 2, Epoch: 1862, train_loss: 1.2449973179743838, valid_loss: 1.2933701762446652\n",
            "test_ind: 2, Epoch: 1863, train_loss: 1.189886986360251, valid_loss: 1.3051893948829412\n",
            "test_ind: 2, Epoch: 1864, train_loss: 1.3427507551307354, valid_loss: 1.2207023744229917\n",
            "test_ind: 2, Epoch: 1865, train_loss: 1.2610837804173018, valid_loss: 1.3795383608239329\n",
            "test_ind: 2, Epoch: 1866, train_loss: 1.2649817838062023, valid_loss: 1.4834865661088559\n",
            "test_ind: 2, Epoch: 1867, train_loss: 1.3242257395361228, valid_loss: 1.2682098968755826\n",
            "test_ind: 2, Epoch: 1868, train_loss: 1.2504909400813147, valid_loss: 1.3214051675932361\n",
            "test_ind: 2, Epoch: 1869, train_loss: 1.2656067545257743, valid_loss: 1.290850357452349\n",
            "test_ind: 2, Epoch: 1870, train_loss: 1.2754900131696538, valid_loss: 1.2988583905744417\n",
            "test_ind: 2, Epoch: 1871, train_loss: 1.3330036586279659, valid_loss: 1.1778378534181164\n",
            "test_ind: 2, Epoch: 1872, train_loss: 1.3041031937993153, valid_loss: 1.2485551059755504\n",
            "test_ind: 2, Epoch: 1873, train_loss: 1.273761991988107, valid_loss: 1.493216837912883\n",
            "test_ind: 2, Epoch: 1874, train_loss: 1.2434657937780746, valid_loss: 1.136380315845848\n",
            "test_ind: 2, Epoch: 1875, train_loss: 1.2855243832297474, valid_loss: 1.2339582090024594\n",
            "test_ind: 2, Epoch: 1876, train_loss: 1.3841422652473703, valid_loss: 1.9313041831013822\n",
            "test_ind: 2, Epoch: 1877, train_loss: 1.198476511755107, valid_loss: 1.2376955085330539\n",
            "test_ind: 2, Epoch: 1878, train_loss: 1.1989879805138308, valid_loss: 1.1426728593657838\n",
            "test_ind: 2, Epoch: 1879, train_loss: 1.2736708449955114, valid_loss: 1.151162557112865\n",
            "test_ind: 2, Epoch: 1880, train_loss: 1.280855103888516, valid_loss: 1.3071516137517074\n",
            "test_ind: 2, Epoch: 1881, train_loss: 1.3829697086374078, valid_loss: 1.354416281409413\n",
            "test_ind: 2, Epoch: 1882, train_loss: 1.2710735795617893, valid_loss: 1.299030428938037\n",
            "test_ind: 2, Epoch: 1883, train_loss: 1.2701130916363605, valid_loss: 1.1254910655171104\n",
            "test_ind: 2, Epoch: 1884, train_loss: 1.231716462123541, valid_loss: 1.1407277067841968\n",
            "test_ind: 2, Epoch: 1885, train_loss: 1.370892274526902, valid_loss: 1.3127094069097796\n",
            "test_ind: 2, Epoch: 1886, train_loss: 1.261210309134589, valid_loss: 1.2429017952704362\n",
            "test_ind: 2, Epoch: 1887, train_loss: 1.253384624338105, valid_loss: 1.3250179005484295\n",
            "test_ind: 2, Epoch: 1888, train_loss: 1.332537148186737, valid_loss: 1.1239639585174386\n",
            "test_ind: 2, Epoch: 1889, train_loss: 1.1992337887103741, valid_loss: 1.1623901118580093\n",
            "test_ind: 2, Epoch: 1890, train_loss: 1.2949744348625625, valid_loss: 1.291637746696798\n",
            "test_ind: 2, Epoch: 1891, train_loss: 1.3006950058715183, valid_loss: 1.2156332001047596\n",
            "test_ind: 2, Epoch: 1892, train_loss: 1.2524869589158045, valid_loss: 1.3340235068927124\n",
            "test_ind: 2, Epoch: 1893, train_loss: 1.2876130945888566, valid_loss: 1.1956379698891926\n",
            "test_ind: 2, Epoch: 1894, train_loss: 1.2912207557718074, valid_loss: 1.1683642008365729\n",
            "test_ind: 2, Epoch: 1895, train_loss: 1.2587339649399687, valid_loss: 1.2805865653220065\n",
            "test_ind: 2, Epoch: 1896, train_loss: 1.2577350470188788, valid_loss: 1.1974428148351164\n",
            "test_ind: 2, Epoch: 1897, train_loss: 1.22009079476707, valid_loss: 1.1498812664607991\n",
            "test_ind: 2, Epoch: 1898, train_loss: 1.245924250692384, valid_loss: 1.1403223503688802\n",
            "test_ind: 2, Epoch: 1899, train_loss: 1.2154920859667424, valid_loss: 1.2929687717361666\n",
            "test_ind: 2, Epoch: 1900, train_loss: 1.249487691228999, valid_loss: 1.2324632246609766\n",
            "test_ind: 2, Epoch: 1901, train_loss: 1.2378105281764626, valid_loss: 1.2629586665379016\n",
            "test_ind: 2, Epoch: 1902, train_loss: 1.1942017395385425, valid_loss: 1.3098567306485949\n",
            "test_ind: 2, Epoch: 1903, train_loss: 1.332621032356197, valid_loss: 1.2790871074057033\n",
            "test_ind: 2, Epoch: 1904, train_loss: 1.2400131239171042, valid_loss: 1.2797665989976323\n",
            "test_ind: 2, Epoch: 1905, train_loss: 1.3357392872160996, valid_loss: 1.3354238281902084\n",
            "test_ind: 2, Epoch: 1906, train_loss: 1.2517519742108931, valid_loss: 1.1606434369698548\n",
            "test_ind: 2, Epoch: 1907, train_loss: 1.2526173990110159, valid_loss: 1.1913301931147562\n",
            "test_ind: 2, Epoch: 1908, train_loss: 1.3504964888718507, valid_loss: 1.2207180387274152\n",
            "test_ind: 2, Epoch: 1909, train_loss: 1.2473040664071486, valid_loss: 1.2234811443209308\n",
            "test_ind: 2, Epoch: 1910, train_loss: 1.2535815379332045, valid_loss: 1.2962852128890165\n",
            "test_ind: 2, Epoch: 1911, train_loss: 1.2198022197567613, valid_loss: 1.0879409863398626\n",
            "Validation loss decreased (1.1045268478556576 --> 1.0879409863398626).  Saving model ...\n",
            "test_ind: 2, Epoch: 1912, train_loss: 1.2460445200729007, valid_loss: 1.4731835775565558\n",
            "test_ind: 2, Epoch: 1913, train_loss: 1.2532615226897756, valid_loss: 1.1605092371970498\n",
            "test_ind: 2, Epoch: 1914, train_loss: 1.4004390830667612, valid_loss: 1.1919272319543737\n",
            "test_ind: 2, Epoch: 1915, train_loss: 1.2365552000170759, valid_loss: 1.5723424345000176\n",
            "test_ind: 2, Epoch: 1916, train_loss: 1.2470606188488822, valid_loss: 1.1108395849537644\n",
            "test_ind: 2, Epoch: 1917, train_loss: 1.2361596783115427, valid_loss: 1.1820636162391076\n",
            "test_ind: 2, Epoch: 1918, train_loss: 1.2086698787051615, valid_loss: 1.2149311674286498\n",
            "test_ind: 2, Epoch: 1919, train_loss: 1.2495851344645645, valid_loss: 1.179715552560964\n",
            "test_ind: 2, Epoch: 1920, train_loss: 1.2426863051774044, valid_loss: 1.2441302996415358\n",
            "test_ind: 2, Epoch: 1921, train_loss: 1.2759793651522848, valid_loss: 1.3835180892563954\n",
            "test_ind: 2, Epoch: 1922, train_loss: 1.2891330598992385, valid_loss: 1.1388843602943963\n",
            "test_ind: 2, Epoch: 1923, train_loss: 1.212663203789185, valid_loss: 1.2701653354188316\n",
            "test_ind: 2, Epoch: 1924, train_loss: 1.2887174394848453, valid_loss: 1.6482100982611674\n",
            "test_ind: 2, Epoch: 1925, train_loss: 1.2182214371046336, valid_loss: 1.217603536752554\n",
            "test_ind: 2, Epoch: 1926, train_loss: 1.2536207391099488, valid_loss: 1.2621055022943393\n",
            "test_ind: 2, Epoch: 1927, train_loss: 1.1964674464657776, valid_loss: 1.2214569733013794\n",
            "test_ind: 2, Epoch: 1928, train_loss: 1.2754144079891252, valid_loss: 1.322829729471451\n",
            "test_ind: 2, Epoch: 1929, train_loss: 1.2330994148652892, valid_loss: 1.2399276980647336\n",
            "test_ind: 2, Epoch: 1930, train_loss: 1.2003662083563302, valid_loss: 1.3771107943988592\n",
            "test_ind: 2, Epoch: 1931, train_loss: 1.253968301322046, valid_loss: 1.2144914311900779\n",
            "test_ind: 2, Epoch: 1932, train_loss: 1.2519457136463914, valid_loss: 1.2443326382215885\n",
            "test_ind: 2, Epoch: 1933, train_loss: 1.2589676484309937, valid_loss: 1.1872675153944228\n",
            "test_ind: 2, Epoch: 1934, train_loss: 1.2318083390890702, valid_loss: 1.397648022045777\n",
            "test_ind: 2, Epoch: 1935, train_loss: 1.1920639001406153, valid_loss: 1.101707052980733\n",
            "test_ind: 2, Epoch: 1936, train_loss: 1.2851238608473499, valid_loss: 1.5450242713645652\n",
            "test_ind: 2, Epoch: 1937, train_loss: 1.2493706118573717, valid_loss: 1.1278999476690919\n",
            "test_ind: 2, Epoch: 1938, train_loss: 1.284741530051598, valid_loss: 1.10620842699991\n",
            "test_ind: 2, Epoch: 1939, train_loss: 1.193187604942213, valid_loss: 1.0990358709949375\n",
            "test_ind: 2, Epoch: 1940, train_loss: 1.2171779328714862, valid_loss: 1.123470732968757\n",
            "test_ind: 2, Epoch: 1941, train_loss: 1.2731655391193182, valid_loss: 1.1970982551574707\n",
            "test_ind: 2, Epoch: 1942, train_loss: 1.1980126322957299, valid_loss: 1.3336772579073566\n",
            "test_ind: 2, Epoch: 1943, train_loss: 1.2069442029918587, valid_loss: 1.1999106407165527\n",
            "test_ind: 2, Epoch: 1944, train_loss: 1.2456363277670774, valid_loss: 1.2477060885850522\n",
            "test_ind: 2, Epoch: 1945, train_loss: 1.2062138107314295, valid_loss: 1.2638357106776659\n",
            "test_ind: 2, Epoch: 1946, train_loss: 1.229618112132986, valid_loss: 1.1075331220599662\n",
            "test_ind: 2, Epoch: 1947, train_loss: 1.2565068813697111, valid_loss: 1.3708260650308721\n",
            "test_ind: 2, Epoch: 1948, train_loss: 1.3838206141309748, valid_loss: 1.2448852585251855\n",
            "test_ind: 2, Epoch: 1949, train_loss: 1.2970653685183038, valid_loss: 1.5813431366216764\n",
            "test_ind: 2, Epoch: 1950, train_loss: 1.2243293339257453, valid_loss: 1.1849616429744623\n",
            "test_ind: 2, Epoch: 1951, train_loss: 1.247264215636004, valid_loss: 1.538737820084618\n",
            "test_ind: 2, Epoch: 1952, train_loss: 1.2744549590977505, valid_loss: 1.1291867328165601\n",
            "test_ind: 2, Epoch: 1953, train_loss: 1.203233808533758, valid_loss: 1.2529029465808488\n",
            "test_ind: 2, Epoch: 1954, train_loss: 1.3385602569761212, valid_loss: 1.564259498547285\n",
            "test_ind: 2, Epoch: 1955, train_loss: 1.2278443039425873, valid_loss: 1.2992383360523103\n",
            "test_ind: 2, Epoch: 1956, train_loss: 1.2384903181878477, valid_loss: 1.1902383317974559\n",
            "test_ind: 2, Epoch: 1957, train_loss: 1.1706323016855904, valid_loss: 1.6022461024444667\n",
            "test_ind: 2, Epoch: 1958, train_loss: 1.1723273862347867, valid_loss: 1.3275538285573323\n",
            "test_ind: 2, Epoch: 1959, train_loss: 1.1742687399683966, valid_loss: 1.376075190356654\n",
            "test_ind: 2, Epoch: 1960, train_loss: 1.2988255402075033, valid_loss: 1.3280351474414185\n",
            "test_ind: 2, Epoch: 1961, train_loss: 1.1748444026465203, valid_loss: 1.120584690333092\n",
            "test_ind: 2, Epoch: 1962, train_loss: 1.2805169298891104, valid_loss: 1.520999754935588\n",
            "test_ind: 2, Epoch: 1963, train_loss: 1.2460421395098042, valid_loss: 1.2455616425245237\n",
            "test_ind: 2, Epoch: 1964, train_loss: 1.2127025068542103, valid_loss: 1.3264206834668109\n",
            "test_ind: 2, Epoch: 1965, train_loss: 1.2347499432160758, valid_loss: 1.3008335349906206\n",
            "test_ind: 2, Epoch: 1966, train_loss: 1.2574070783761833, valid_loss: 1.2560411097317339\n",
            "test_ind: 2, Epoch: 1967, train_loss: 1.2740096140224821, valid_loss: 1.1473584276998146\n",
            "test_ind: 2, Epoch: 1968, train_loss: 1.1988437309337592, valid_loss: 1.2012408893672148\n",
            "test_ind: 2, Epoch: 1969, train_loss: 1.2617936636987237, valid_loss: 1.310598830551843\n",
            "test_ind: 2, Epoch: 1970, train_loss: 1.2031520425084652, valid_loss: 1.2571088751496753\n",
            "test_ind: 2, Epoch: 1971, train_loss: 1.2538576997809487, valid_loss: 1.173906125234403\n",
            "test_ind: 2, Epoch: 1972, train_loss: 1.2462925618869967, valid_loss: 1.2139709844888105\n",
            "test_ind: 2, Epoch: 1973, train_loss: 1.2041748065894147, valid_loss: 1.1777325865210289\n",
            "test_ind: 2, Epoch: 1974, train_loss: 1.1906603949475265, valid_loss: 1.1629260768238296\n",
            "test_ind: 2, Epoch: 1975, train_loss: 1.2012831603246763, valid_loss: 1.276373834691496\n",
            "test_ind: 2, Epoch: 1976, train_loss: 1.2382171056662303, valid_loss: 1.2783933901718878\n",
            "test_ind: 2, Epoch: 1977, train_loss: 1.2106630691209523, valid_loss: 1.1836010662578789\n",
            "test_ind: 2, Epoch: 1978, train_loss: 1.3164481503105345, valid_loss: 1.3070427692174231\n",
            "test_ind: 2, Epoch: 1979, train_loss: 1.2030343948266446, valid_loss: 1.2049906634197616\n",
            "test_ind: 2, Epoch: 1980, train_loss: 1.2279114211499975, valid_loss: 1.287234492451377\n",
            "test_ind: 2, Epoch: 1981, train_loss: 1.2022391552033023, valid_loss: 1.217402291094136\n",
            "test_ind: 2, Epoch: 1982, train_loss: 1.3165626256441003, valid_loss: 1.4246949103483109\n",
            "test_ind: 2, Epoch: 1983, train_loss: 1.2666641934531595, valid_loss: 1.1903872775216389\n",
            "test_ind: 2, Epoch: 1984, train_loss: 1.2671017975096235, valid_loss: 1.289484257711644\n",
            "test_ind: 2, Epoch: 1985, train_loss: 1.3139984668829503, valid_loss: 1.286984139358216\n",
            "test_ind: 2, Epoch: 1986, train_loss: 1.2014859263826752, valid_loss: 1.2574735265172103\n",
            "test_ind: 2, Epoch: 1987, train_loss: 1.2294244578308076, valid_loss: 1.4714538045758196\n",
            "test_ind: 2, Epoch: 1988, train_loss: 1.2264955428704012, valid_loss: 1.2637702012673404\n",
            "test_ind: 2, Epoch: 1989, train_loss: 1.2633354428373738, valid_loss: 1.4398155076551302\n",
            "test_ind: 2, Epoch: 1990, train_loss: 1.1988435572708436, valid_loss: 1.257624327287375\n",
            "test_ind: 2, Epoch: 1991, train_loss: 1.2560494722690565, valid_loss: 1.4608842266930473\n",
            "test_ind: 2, Epoch: 1992, train_loss: 1.2682213923643573, valid_loss: 1.0905542468752956\n",
            "test_ind: 2, Epoch: 1993, train_loss: 1.3414294674412364, valid_loss: 1.1743045444162483\n",
            "test_ind: 2, Epoch: 1994, train_loss: 1.2498617541303212, valid_loss: 1.3718460163159927\n",
            "test_ind: 2, Epoch: 1995, train_loss: 1.2537022596976923, valid_loss: 1.2534067576427406\n",
            "test_ind: 2, Epoch: 1996, train_loss: 1.1460791859078836, valid_loss: 1.1093271050358091\n",
            "test_ind: 2, Epoch: 1997, train_loss: 1.2609272800166609, valid_loss: 1.1818043968276761\n",
            "test_ind: 2, Epoch: 1998, train_loss: 1.2416173655536213, valid_loss: 1.2301924235460764\n",
            "test_ind: 2, Epoch: 1999, train_loss: 1.202446659292817, valid_loss: 1.178646803581477\n",
            "test_ind: 2, Epoch: 2000, train_loss: 1.213111230790445, valid_loss: 1.251883639908924\n",
            "test_ind: 2, Epoch: 2001, train_loss: 1.2709369675273117, valid_loss: 1.2275412992874104\n",
            "test_ind: 2, Epoch: 2002, train_loss: 1.2277370282614217, valid_loss: 1.476609503781354\n",
            "test_ind: 2, Epoch: 2003, train_loss: 1.2317847754993783, valid_loss: 1.1995920129651017\n",
            "test_ind: 2, Epoch: 2004, train_loss: 1.2629594012656218, valid_loss: 1.4210240630342752\n",
            "test_ind: 2, Epoch: 2005, train_loss: 1.2239559978274992, valid_loss: 1.317217693029985\n",
            "test_ind: 2, Epoch: 2006, train_loss: 1.468468152333302, valid_loss: 1.639926097331903\n",
            "test_ind: 2, Epoch: 2007, train_loss: 1.362651425775526, valid_loss: 1.0769018868775109\n",
            "Validation loss decreased (1.0879409863398626 --> 1.0769018868775109).  Saving model ...\n",
            "test_ind: 2, Epoch: 2008, train_loss: 1.234675761301633, valid_loss: 1.200889874048043\n",
            "test_ind: 2, Epoch: 2009, train_loss: 1.2109192253178, valid_loss: 1.1993488803548351\n",
            "test_ind: 2, Epoch: 2010, train_loss: 1.2216795693095932, valid_loss: 1.0710305250608005\n",
            "Validation loss decreased (1.0769018868775109 --> 1.0710305250608005).  Saving model ...\n",
            "test_ind: 2, Epoch: 2011, train_loss: 1.190734043193792, valid_loss: 1.0549305707980425\n",
            "Validation loss decreased (1.0710305250608005 --> 1.0549305707980425).  Saving model ...\n",
            "test_ind: 2, Epoch: 2012, train_loss: 1.1827393098887329, valid_loss: 1.1011705364596809\n",
            "test_ind: 2, Epoch: 2013, train_loss: 1.241780392917586, valid_loss: 1.2723300103788022\n",
            "test_ind: 2, Epoch: 2014, train_loss: 1.257858790336964, valid_loss: 1.384565873023791\n",
            "test_ind: 2, Epoch: 2015, train_loss: 1.199668575442641, valid_loss: 1.081564261363103\n",
            "test_ind: 2, Epoch: 2016, train_loss: 1.266644295577423, valid_loss: 1.3240649985451982\n",
            "test_ind: 2, Epoch: 2017, train_loss: 1.2319447498375875, valid_loss: 1.3928110422911466\n",
            "test_ind: 2, Epoch: 2018, train_loss: 1.257185971295392, valid_loss: 1.1681988347969163\n",
            "test_ind: 2, Epoch: 2019, train_loss: 1.2165980391126525, valid_loss: 1.2308051742379822\n",
            "test_ind: 2, Epoch: 2020, train_loss: 1.1721094777667853, valid_loss: 1.1702704524722194\n",
            "test_ind: 2, Epoch: 2021, train_loss: 1.2913485698663725, valid_loss: 1.3848174365497385\n",
            "test_ind: 2, Epoch: 2022, train_loss: 1.3071103184311477, valid_loss: 1.2343536693486052\n",
            "test_ind: 2, Epoch: 2023, train_loss: 1.2110039873114111, valid_loss: 1.2659699645137514\n",
            "test_ind: 2, Epoch: 2024, train_loss: 1.2151757022480905, valid_loss: 1.0597042419292309\n",
            "test_ind: 2, Epoch: 2025, train_loss: 1.2274367274948221, valid_loss: 1.2617490678770928\n",
            "test_ind: 2, Epoch: 2026, train_loss: 1.2041434700559235, valid_loss: 1.1620629356797265\n",
            "test_ind: 2, Epoch: 2027, train_loss: 1.2774937139277445, valid_loss: 1.3858209281225828\n",
            "test_ind: 2, Epoch: 2028, train_loss: 1.2515664924809964, valid_loss: 1.2300777917573935\n",
            "test_ind: 2, Epoch: 2029, train_loss: 1.2587032062262438, valid_loss: 1.303158544746899\n",
            "test_ind: 2, Epoch: 2030, train_loss: 1.2878623253259904, valid_loss: 1.2866620997078397\n",
            "test_ind: 2, Epoch: 2031, train_loss: 1.270787978330795, valid_loss: 1.1604008606696061\n",
            "test_ind: 2, Epoch: 2032, train_loss: 1.2415438282523736, valid_loss: 1.1916024141501838\n",
            "test_ind: 2, Epoch: 2033, train_loss: 1.2308487747243104, valid_loss: 1.2831970741945793\n",
            "test_ind: 2, Epoch: 2034, train_loss: 1.2690070651308876, valid_loss: 1.340847812147222\n",
            "test_ind: 2, Epoch: 2035, train_loss: 1.2229267627085716, valid_loss: 1.385127344702044\n",
            "test_ind: 2, Epoch: 2036, train_loss: 1.297433687184724, valid_loss: 1.5175105452197908\n",
            "test_ind: 2, Epoch: 2037, train_loss: 1.227625614783929, valid_loss: 1.1391311323540843\n",
            "test_ind: 2, Epoch: 2038, train_loss: 1.2514318518715593, valid_loss: 1.2695192855987114\n",
            "test_ind: 2, Epoch: 2039, train_loss: 1.2337844752178573, valid_loss: 1.4503217015171324\n",
            "test_ind: 2, Epoch: 2040, train_loss: 1.2218016622639336, valid_loss: 1.3194538419402904\n",
            "test_ind: 2, Epoch: 2041, train_loss: 1.1816574893219631, valid_loss: 1.2480004282079191\n",
            "test_ind: 2, Epoch: 2042, train_loss: 1.2144660865932222, valid_loss: 1.1976261641564871\n",
            "test_ind: 2, Epoch: 2043, train_loss: 1.213311895912076, valid_loss: 1.1391126878580815\n",
            "test_ind: 2, Epoch: 2044, train_loss: 1.2236209117222827, valid_loss: 1.1603946794471849\n",
            "test_ind: 2, Epoch: 2045, train_loss: 1.1908512273971399, valid_loss: 1.1662839776770002\n",
            "test_ind: 2, Epoch: 2046, train_loss: 1.2153958730887824, valid_loss: 1.1130133653298402\n",
            "test_ind: 2, Epoch: 2047, train_loss: 1.202700968821164, valid_loss: 1.2576570381805767\n",
            "test_ind: 2, Epoch: 2048, train_loss: 1.2007885454726694, valid_loss: 1.1155665246849387\n",
            "test_ind: 2, Epoch: 2049, train_loss: 1.2062309057737461, valid_loss: 1.1737806355511702\n",
            "test_ind: 2, Epoch: 2050, train_loss: 1.3130428958595763, valid_loss: 1.4452919389447596\n",
            "test_ind: 2, Epoch: 2051, train_loss: 1.2229711495460156, valid_loss: 1.1661203797386583\n",
            "test_ind: 2, Epoch: 2052, train_loss: 1.1552592469529661, valid_loss: 1.2594239555532776\n",
            "test_ind: 2, Epoch: 2053, train_loss: 1.301571801856712, valid_loss: 1.3681782188578548\n",
            "test_ind: 2, Epoch: 2054, train_loss: 1.2203486785363382, valid_loss: 1.1708520552371642\n",
            "test_ind: 2, Epoch: 2055, train_loss: 1.268473888507709, valid_loss: 1.6156078770629363\n",
            "test_ind: 2, Epoch: 2056, train_loss: 1.2116957836114897, valid_loss: 1.2128468476808987\n",
            "test_ind: 2, Epoch: 2057, train_loss: 1.2414706056727076, valid_loss: 1.357473759909301\n",
            "test_ind: 2, Epoch: 2058, train_loss: 1.1769336189186699, valid_loss: 1.1467136609927882\n",
            "test_ind: 2, Epoch: 2059, train_loss: 1.1921860082649889, valid_loss: 1.1835424159666752\n",
            "test_ind: 2, Epoch: 2060, train_loss: 1.2711719771509267, valid_loss: 1.06959368768241\n",
            "test_ind: 2, Epoch: 2061, train_loss: 1.2463449150295565, valid_loss: 1.1319668985839584\n",
            "test_ind: 2, Epoch: 2062, train_loss: 1.2007019734903392, valid_loss: 1.401825887864811\n",
            "test_ind: 2, Epoch: 2063, train_loss: 1.2178193100950092, valid_loss: 1.2917662230651943\n",
            "test_ind: 2, Epoch: 2064, train_loss: 1.2159153552476498, valid_loss: 1.2063925578723267\n",
            "test_ind: 2, Epoch: 2065, train_loss: 1.1734279651134667, valid_loss: 1.1546015277547375\n",
            "test_ind: 2, Epoch: 2066, train_loss: 1.2223838562073306, valid_loss: 1.1572447529545535\n",
            "test_ind: 2, Epoch: 2067, train_loss: 1.174955730311438, valid_loss: 1.2484027497109524\n",
            "test_ind: 2, Epoch: 2068, train_loss: 1.2112447423020205, valid_loss: 1.1595039034840728\n",
            "test_ind: 2, Epoch: 2069, train_loss: 1.27865548034226, valid_loss: 1.1625793727374822\n",
            "test_ind: 2, Epoch: 2070, train_loss: 1.2318578470579693, valid_loss: 1.1995511523678772\n",
            "test_ind: 2, Epoch: 2071, train_loss: 1.2107645043393938, valid_loss: 1.1396183295127673\n",
            "test_ind: 2, Epoch: 2072, train_loss: 1.248408227224975, valid_loss: 1.1138746018423316\n",
            "test_ind: 2, Epoch: 2073, train_loss: 1.2004867107213844, valid_loss: 1.251846392270167\n",
            "test_ind: 2, Epoch: 2074, train_loss: 1.2948466311832438, valid_loss: 1.5277152876568656\n",
            "test_ind: 2, Epoch: 2075, train_loss: 1.2347461529719976, valid_loss: 1.0963542936873913\n",
            "test_ind: 2, Epoch: 2076, train_loss: 1.2183893913330628, valid_loss: 1.2677240242645613\n",
            "test_ind: 2, Epoch: 2077, train_loss: 1.250625676239318, valid_loss: 1.1043173940772684\n",
            "test_ind: 2, Epoch: 2078, train_loss: 1.2163461182984643, valid_loss: 1.2647967881966182\n",
            "test_ind: 2, Epoch: 2079, train_loss: 1.2078976262102552, valid_loss: 1.2685661370258385\n",
            "test_ind: 2, Epoch: 2080, train_loss: 1.1899891737406298, valid_loss: 1.163915066297917\n",
            "test_ind: 2, Epoch: 2081, train_loss: 1.2224570203710485, valid_loss: 1.1577951758675424\n",
            "test_ind: 2, Epoch: 2082, train_loss: 1.2116032162962478, valid_loss: 1.2283882463080251\n",
            "test_ind: 2, Epoch: 2083, train_loss: 1.198512190087908, valid_loss: 1.2632966061942599\n",
            "test_ind: 2, Epoch: 2084, train_loss: 1.263570683407761, valid_loss: 1.202483253261642\n",
            "test_ind: 2, Epoch: 2085, train_loss: 1.2170374203271674, valid_loss: 1.259999959217517\n",
            "test_ind: 2, Epoch: 2086, train_loss: 1.2272545134353277, valid_loss: 1.160581649198831\n",
            "test_ind: 2, Epoch: 2087, train_loss: 1.1996992533249962, valid_loss: 1.195878415365844\n",
            "test_ind: 2, Epoch: 2088, train_loss: 1.2129110311850522, valid_loss: 1.096374679494787\n",
            "test_ind: 2, Epoch: 2089, train_loss: 1.1507752618671938, valid_loss: 1.1417562207605085\n",
            "test_ind: 2, Epoch: 2090, train_loss: 1.2038307092581493, valid_loss: 1.1612585141108587\n",
            "test_ind: 2, Epoch: 2091, train_loss: 1.2044105903375522, valid_loss: 1.220493633183319\n",
            "test_ind: 2, Epoch: 2092, train_loss: 1.202715139896215, valid_loss: 1.1788534798853079\n",
            "test_ind: 2, Epoch: 2093, train_loss: 1.2090690160861837, valid_loss: 1.7099439927995035\n",
            "test_ind: 2, Epoch: 2094, train_loss: 1.197029464038802, valid_loss: 1.0976764143701971\n",
            "test_ind: 2, Epoch: 2095, train_loss: 1.1892163506713462, valid_loss: 1.2080930637837812\n",
            "test_ind: 2, Epoch: 2096, train_loss: 1.2143604556153416, valid_loss: 1.2398333508744201\n",
            "test_ind: 2, Epoch: 2097, train_loss: 1.1555204611105343, valid_loss: 1.0800398204401347\n",
            "test_ind: 2, Epoch: 2098, train_loss: 1.2132852949194985, valid_loss: 1.2282963444364718\n",
            "test_ind: 2, Epoch: 2099, train_loss: 1.1561276405511984, valid_loss: 1.1946625050656137\n",
            "test_ind: 2, Epoch: 2100, train_loss: 1.2053452110924496, valid_loss: 1.209983626661817\n",
            "test_ind: 2, Epoch: 2101, train_loss: 1.1777106803140522, valid_loss: 1.1397889285345701\n",
            "test_ind: 2, Epoch: 2102, train_loss: 1.221283242686635, valid_loss: 1.2434536765443633\n",
            "test_ind: 2, Epoch: 2103, train_loss: 1.1933367915755775, valid_loss: 1.3756962970451072\n",
            "test_ind: 2, Epoch: 2104, train_loss: 1.1671550937301187, valid_loss: 1.2410056584241382\n",
            "test_ind: 2, Epoch: 2105, train_loss: 1.2917641873373265, valid_loss: 1.3551971090485226\n",
            "test_ind: 2, Epoch: 2106, train_loss: 1.2632681186835424, valid_loss: 1.2321138436298424\n",
            "test_ind: 2, Epoch: 2107, train_loss: 1.1599111575120307, valid_loss: 1.1193551399089672\n",
            "test_ind: 2, Epoch: 2108, train_loss: 1.3202373472034419, valid_loss: 1.405143429411103\n",
            "test_ind: 2, Epoch: 2109, train_loss: 1.2232367550885237, valid_loss: 1.1751200867514326\n",
            "test_ind: 2, Epoch: 2110, train_loss: 1.1851367454583148, valid_loss: 1.1940304771108166\n",
            "test_ind: 2, Epoch: 2111, train_loss: 1.2281721353757302, valid_loss: 1.2109182519450827\n",
            "test_ind: 2, Epoch: 2112, train_loss: 1.218906208773737, valid_loss: 1.347341515060164\n",
            "test_ind: 2, Epoch: 2113, train_loss: 1.2647407498681196, valid_loss: 1.267207438449914\n",
            "test_ind: 2, Epoch: 2114, train_loss: 1.223464709741098, valid_loss: 1.4182490827011587\n",
            "test_ind: 2, Epoch: 2115, train_loss: 1.2780940451173703, valid_loss: 1.3127638818192007\n",
            "test_ind: 2, Epoch: 2116, train_loss: 1.2019778620257109, valid_loss: 1.1903588337096733\n",
            "test_ind: 2, Epoch: 2117, train_loss: 1.238475236457977, valid_loss: 1.1245538332523444\n",
            "test_ind: 2, Epoch: 2118, train_loss: 1.1527029102684085, valid_loss: 1.2275216375660694\n",
            "test_ind: 2, Epoch: 2119, train_loss: 1.143884742814704, valid_loss: 1.163180833528524\n",
            "test_ind: 2, Epoch: 2120, train_loss: 1.1814651987378302, valid_loss: 1.0568044892063848\n",
            "test_ind: 2, Epoch: 2121, train_loss: 1.2023255202392116, valid_loss: 1.323193304219477\n",
            "test_ind: 2, Epoch: 2122, train_loss: 1.2849713146177113, valid_loss: 1.166294832854529\n",
            "test_ind: 2, Epoch: 2123, train_loss: 1.159723399371503, valid_loss: 1.2111256170136977\n",
            "test_ind: 2, Epoch: 2124, train_loss: 1.229920955804678, valid_loss: 1.2778385228920526\n",
            "test_ind: 2, Epoch: 2125, train_loss: 1.2211300443719932, valid_loss: 1.1831756473606467\n",
            "test_ind: 2, Epoch: 2126, train_loss: 1.2424116259173679, valid_loss: 1.1493194802873834\n",
            "test_ind: 2, Epoch: 2127, train_loss: 1.17659182992303, valid_loss: 1.2747493197775295\n",
            "test_ind: 2, Epoch: 2128, train_loss: 1.130329534199163, valid_loss: 1.15276245310096\n",
            "test_ind: 2, Epoch: 2129, train_loss: 1.193111669643652, valid_loss: 1.227077605717542\n",
            "test_ind: 2, Epoch: 2130, train_loss: 1.1673563042936843, valid_loss: 1.2624581172595337\n",
            "test_ind: 2, Epoch: 2131, train_loss: 1.1605952673601403, valid_loss: 1.3811556926140418\n",
            "test_ind: 2, Epoch: 2132, train_loss: 1.1403860878287784, valid_loss: 1.1588388222914476\n",
            "test_ind: 2, Epoch: 2133, train_loss: 1.2053143350939917, valid_loss: 1.0555756126034295\n",
            "test_ind: 2, Epoch: 2134, train_loss: 1.20884078044837, valid_loss: 1.1437065662481847\n",
            "test_ind: 2, Epoch: 2135, train_loss: 1.1938073958879636, valid_loss: 1.3515836024216437\n",
            "test_ind: 2, Epoch: 2136, train_loss: 1.274333012183281, valid_loss: 1.3198447295403548\n",
            "test_ind: 2, Epoch: 2137, train_loss: 1.26042059230895, valid_loss: 1.1693280599056146\n",
            "test_ind: 2, Epoch: 2138, train_loss: 1.268477907660793, valid_loss: 1.3365144913013165\n",
            "test_ind: 2, Epoch: 2139, train_loss: 1.2178073069761735, valid_loss: 1.3904531029214886\n",
            "test_ind: 2, Epoch: 2140, train_loss: 1.2353129183125293, valid_loss: 1.1464621606375758\n",
            "test_ind: 2, Epoch: 2141, train_loss: 1.2065787799904035, valid_loss: 1.1088704145871675\n",
            "test_ind: 2, Epoch: 2142, train_loss: 1.247606398373248, valid_loss: 1.164026197205242\n",
            "test_ind: 2, Epoch: 2143, train_loss: 1.2296884048585084, valid_loss: 1.1553076638115778\n",
            "test_ind: 2, Epoch: 2144, train_loss: 1.1351770609758742, valid_loss: 1.1530718844161076\n",
            "test_ind: 2, Epoch: 2145, train_loss: 1.1578250635496685, valid_loss: 1.1431224346160889\n",
            "test_ind: 2, Epoch: 2146, train_loss: 1.1318522233229418, valid_loss: 1.1471698861516098\n",
            "test_ind: 2, Epoch: 2147, train_loss: 1.172094264487822, valid_loss: 1.0756920538736545\n",
            "test_ind: 2, Epoch: 2148, train_loss: 1.0994411780510418, valid_loss: 1.0894251542213635\n",
            "test_ind: 2, Epoch: 2149, train_loss: 1.209452243498814, valid_loss: 1.2318328622399572\n",
            "test_ind: 2, Epoch: 2150, train_loss: 1.176088646266535, valid_loss: 1.1372075196344968\n",
            "test_ind: 2, Epoch: 2151, train_loss: 1.1656094761202704, valid_loss: 1.0721133570385795\n",
            "test_ind: 2, Epoch: 2152, train_loss: 1.1640790038638644, valid_loss: 1.2094801334913639\n",
            "test_ind: 2, Epoch: 2153, train_loss: 1.19589799693507, valid_loss: 1.0692526485845233\n",
            "test_ind: 2, Epoch: 2154, train_loss: 1.224378037430056, valid_loss: 1.0749507412271961\n",
            "test_ind: 2, Epoch: 2155, train_loss: 1.242661262396281, valid_loss: 1.320523571764302\n",
            "test_ind: 2, Epoch: 2156, train_loss: 1.1369547538268259, valid_loss: 1.1302140417941275\n",
            "test_ind: 2, Epoch: 2157, train_loss: 1.2005021952495953, valid_loss: 1.3296736054271034\n",
            "test_ind: 2, Epoch: 2158, train_loss: 1.1928768431925254, valid_loss: 1.195913517237389\n",
            "test_ind: 2, Epoch: 2159, train_loss: 1.1229233617230134, valid_loss: 1.1078283216199303\n",
            "test_ind: 2, Epoch: 2160, train_loss: 1.1633825478730377, valid_loss: 1.1680026597786493\n",
            "test_ind: 2, Epoch: 2161, train_loss: 1.187233995734683, valid_loss: 1.1488603302556226\n",
            "test_ind: 2, Epoch: 2162, train_loss: 1.1956536208349302, valid_loss: 1.1986196869798533\n",
            "test_ind: 2, Epoch: 2163, train_loss: 1.2023808258777557, valid_loss: 1.078319874923793\n",
            "test_ind: 2, Epoch: 2164, train_loss: 1.1957199455779275, valid_loss: 1.2078490861800322\n",
            "test_ind: 2, Epoch: 2165, train_loss: 1.1709844063037027, valid_loss: 1.2173495768142222\n",
            "test_ind: 2, Epoch: 2166, train_loss: 1.193029160739576, valid_loss: 1.2333069723895471\n",
            "test_ind: 2, Epoch: 2167, train_loss: 1.2201457539854565, valid_loss: 1.1569852740676314\n",
            "test_ind: 2, Epoch: 2168, train_loss: 1.2311210469302967, valid_loss: 1.1430453554517523\n",
            "test_ind: 2, Epoch: 2169, train_loss: 1.2239530831433882, valid_loss: 1.236397580203847\n",
            "test_ind: 2, Epoch: 2170, train_loss: 1.1855886030061293, valid_loss: 1.1077142169332912\n",
            "test_ind: 2, Epoch: 2171, train_loss: 1.2103381677683713, valid_loss: 1.0539475995251255\n",
            "Validation loss decreased (1.0549305707980425 --> 1.0539475995251255).  Saving model ...\n",
            "test_ind: 2, Epoch: 2172, train_loss: 1.2083901247294428, valid_loss: 1.1551657409070224\n",
            "test_ind: 2, Epoch: 2173, train_loss: 1.1752158279998577, valid_loss: 1.2222268812337154\n",
            "test_ind: 2, Epoch: 2174, train_loss: 1.1521200948064936, valid_loss: 1.1888901671113452\n",
            "test_ind: 2, Epoch: 2175, train_loss: 1.145456406012786, valid_loss: 1.1995171053796752\n",
            "test_ind: 2, Epoch: 2176, train_loss: 1.1172841562051947, valid_loss: 1.1619845971762284\n",
            "test_ind: 2, Epoch: 2177, train_loss: 1.207928232770813, valid_loss: 1.2602259392751927\n",
            "test_ind: 2, Epoch: 2178, train_loss: 1.1195215744170708, valid_loss: 1.1376675567735632\n",
            "test_ind: 2, Epoch: 2179, train_loss: 1.1894012767704805, valid_loss: 1.1216465068338943\n",
            "test_ind: 2, Epoch: 2180, train_loss: 1.2627055271398646, valid_loss: 1.263981101180074\n",
            "test_ind: 2, Epoch: 2181, train_loss: 1.1663980099097502, valid_loss: 1.2260784715668769\n",
            "test_ind: 2, Epoch: 2182, train_loss: 1.1147143206818266, valid_loss: 1.1828527049800948\n",
            "test_ind: 2, Epoch: 2183, train_loss: 1.172328311153966, valid_loss: 1.177988923173345\n",
            "test_ind: 2, Epoch: 2184, train_loss: 1.1515727885428315, valid_loss: 1.0706898663458322\n",
            "test_ind: 2, Epoch: 2185, train_loss: 1.1792090874225438, valid_loss: 1.23292709619571\n",
            "test_ind: 2, Epoch: 2186, train_loss: 1.235776243273236, valid_loss: 1.382392685637515\n",
            "test_ind: 2, Epoch: 2187, train_loss: 1.1755475291499384, valid_loss: 1.10086781890304\n",
            "test_ind: 2, Epoch: 2188, train_loss: 1.1714352852711762, valid_loss: 1.180461414179571\n",
            "test_ind: 2, Epoch: 2189, train_loss: 1.180518688978972, valid_loss: 1.3909563610696385\n",
            "test_ind: 2, Epoch: 2190, train_loss: 1.2368331254377662, valid_loss: 1.2094012081113636\n",
            "test_ind: 2, Epoch: 2191, train_loss: 1.240081684541838, valid_loss: 1.190286362612689\n",
            "test_ind: 2, Epoch: 2192, train_loss: 1.1770053186081526, valid_loss: 1.063336998648793\n",
            "test_ind: 2, Epoch: 2193, train_loss: 1.1073649971120605, valid_loss: 1.1320690782661111\n",
            "test_ind: 2, Epoch: 2194, train_loss: 1.1846374321980127, valid_loss: 1.1421779241317358\n",
            "test_ind: 2, Epoch: 2195, train_loss: 1.2216430156432438, valid_loss: 1.2724246279126898\n",
            "test_ind: 2, Epoch: 2196, train_loss: 1.1625543127485605, valid_loss: 1.1812323375984475\n",
            "test_ind: 2, Epoch: 2197, train_loss: 1.1909183092832791, valid_loss: 1.1961002988353413\n",
            "test_ind: 2, Epoch: 2198, train_loss: 1.2153266827944675, valid_loss: 1.1740307339236267\n",
            "test_ind: 2, Epoch: 2199, train_loss: 1.1703913295597321, valid_loss: 1.1159011271604447\n",
            "test_ind: 2, Epoch: 2200, train_loss: 1.1976521775933076, valid_loss: 1.2147343382876143\n",
            "test_ind: 2, Epoch: 2201, train_loss: 1.2772172230940597, valid_loss: 1.2770658075979293\n",
            "test_ind: 2, Epoch: 2202, train_loss: 1.2132147155482773, valid_loss: 1.324927366017616\n",
            "test_ind: 2, Epoch: 2203, train_loss: 1.1585491736050684, valid_loss: 1.1476468632363865\n",
            "test_ind: 2, Epoch: 2204, train_loss: 1.122407954868994, valid_loss: 1.1676245645919756\n",
            "test_ind: 2, Epoch: 2205, train_loss: 1.2102378492454968, valid_loss: 1.2179643258749588\n",
            "test_ind: 2, Epoch: 2206, train_loss: 1.1852948599957558, valid_loss: 1.2637007161762641\n",
            "test_ind: 2, Epoch: 2207, train_loss: 1.1643404473832755, valid_loss: 1.319702814786862\n",
            "test_ind: 2, Epoch: 2208, train_loss: 1.163452011680784, valid_loss: 1.7308575149275298\n",
            "test_ind: 2, Epoch: 2209, train_loss: 1.1410804458266763, valid_loss: 1.1204760074615479\n",
            "test_ind: 2, Epoch: 2210, train_loss: 1.1614931696160904, valid_loss: 1.1126092804802787\n",
            "test_ind: 2, Epoch: 2211, train_loss: 1.1644441667105738, valid_loss: 1.249816116104778\n",
            "test_ind: 2, Epoch: 2212, train_loss: 1.1836890499589563, valid_loss: 1.2993401421440973\n",
            "test_ind: 2, Epoch: 2213, train_loss: 1.2048775130866938, valid_loss: 1.2117085545151323\n",
            "test_ind: 2, Epoch: 2214, train_loss: 1.08344068961945, valid_loss: 1.2690425197623054\n",
            "test_ind: 2, Epoch: 2215, train_loss: 1.1720226175538269, valid_loss: 1.2101097643545211\n",
            "test_ind: 2, Epoch: 2216, train_loss: 1.1608614278541556, valid_loss: 1.1755749201163268\n",
            "test_ind: 2, Epoch: 2217, train_loss: 1.1847964793528587, valid_loss: 1.1443143909812994\n",
            "test_ind: 2, Epoch: 2218, train_loss: 1.1825104436756653, valid_loss: 1.1376862668583536\n",
            "test_ind: 2, Epoch: 2219, train_loss: 1.221643486819942, valid_loss: 1.2909002100300584\n",
            "test_ind: 2, Epoch: 2220, train_loss: 1.2531364573146317, valid_loss: 1.1726205770106737\n",
            "test_ind: 2, Epoch: 2221, train_loss: 1.2575756520853651, valid_loss: 1.2237647299752956\n",
            "test_ind: 2, Epoch: 2222, train_loss: 1.175740781785869, valid_loss: 1.321882455097644\n",
            "test_ind: 2, Epoch: 2223, train_loss: 1.2051053873607351, valid_loss: 1.5183853804216088\n",
            "test_ind: 2, Epoch: 2224, train_loss: 1.2760245528316225, valid_loss: 1.3379072293936356\n",
            "test_ind: 2, Epoch: 2225, train_loss: 1.2257522472515632, valid_loss: 1.246072016550265\n",
            "test_ind: 2, Epoch: 2226, train_loss: 1.2644666923530197, valid_loss: 1.1114700968109306\n",
            "test_ind: 2, Epoch: 2227, train_loss: 1.2198372810994118, valid_loss: 1.1443394296868914\n",
            "test_ind: 2, Epoch: 2228, train_loss: 1.187758422645069, valid_loss: 1.1083987947882412\n",
            "test_ind: 2, Epoch: 2229, train_loss: 1.1451612562648703, valid_loss: 1.0538966091949376\n",
            "Validation loss decreased (1.0539475995251255 --> 1.0538966091949376).  Saving model ...\n",
            "test_ind: 2, Epoch: 2230, train_loss: 1.1077937452655007, valid_loss: 1.1340765348526827\n",
            "test_ind: 2, Epoch: 2231, train_loss: 1.153037071001609, valid_loss: 1.1527788931148344\n",
            "test_ind: 2, Epoch: 2232, train_loss: 1.1502269604946473, valid_loss: 1.076772310115673\n",
            "test_ind: 2, Epoch: 2233, train_loss: 1.1406193986351107, valid_loss: 1.1375073096011779\n",
            "test_ind: 2, Epoch: 2234, train_loss: 1.1221226742464594, valid_loss: 1.0264357617098383\n",
            "Validation loss decreased (1.0538966091949376 --> 1.0264357617098383).  Saving model ...\n",
            "test_ind: 2, Epoch: 2235, train_loss: 1.111987079310621, valid_loss: 1.0646882722860047\n",
            "test_ind: 2, Epoch: 2236, train_loss: 1.1243315836642882, valid_loss: 1.1836311382445854\n",
            "test_ind: 2, Epoch: 2237, train_loss: 1.1390311602513674, valid_loss: 1.0196601230534394\n",
            "Validation loss decreased (1.0264357617098383 --> 1.0196601230534394).  Saving model ...\n",
            "test_ind: 2, Epoch: 2238, train_loss: 1.2820985731575905, valid_loss: 1.2115526145000404\n",
            "test_ind: 2, Epoch: 2239, train_loss: 1.158635979704028, valid_loss: 1.0707231546059632\n",
            "test_ind: 2, Epoch: 2240, train_loss: 1.183525455190472, valid_loss: 1.0912643273671467\n",
            "test_ind: 2, Epoch: 2241, train_loss: 1.1433122508999063, valid_loss: 1.4847623364538207\n",
            "test_ind: 2, Epoch: 2242, train_loss: 1.2345473544436867, valid_loss: 1.0817137436989024\n",
            "test_ind: 2, Epoch: 2243, train_loss: 1.1849435086716729, valid_loss: 1.0219672126987382\n",
            "test_ind: 2, Epoch: 2244, train_loss: 1.2460762308760132, valid_loss: 1.3152872408896767\n",
            "test_ind: 2, Epoch: 2245, train_loss: 1.1308596446643187, valid_loss: 1.1349818258204012\n",
            "test_ind: 2, Epoch: 2246, train_loss: 1.1591826875438491, valid_loss: 1.3115058074309953\n",
            "test_ind: 2, Epoch: 2247, train_loss: 1.1886543561929992, valid_loss: 1.1493729037097378\n",
            "test_ind: 2, Epoch: 2248, train_loss: 1.2199805604766238, valid_loss: 1.1497453950409198\n",
            "test_ind: 2, Epoch: 2249, train_loss: 1.1650228815087793, valid_loss: 1.1570067792876153\n",
            "test_ind: 2, Epoch: 2250, train_loss: 1.1616416908057667, valid_loss: 1.1412892518220124\n",
            "test_ind: 2, Epoch: 2251, train_loss: 1.1398260274164358, valid_loss: 1.1473754865151864\n",
            "test_ind: 2, Epoch: 2252, train_loss: 1.1474539737755756, valid_loss: 1.0447028607045143\n",
            "test_ind: 2, Epoch: 2253, train_loss: 1.182891876043191, valid_loss: 1.1565534036044043\n",
            "test_ind: 2, Epoch: 2254, train_loss: 1.1532917126857543, valid_loss: 1.0856496099053623\n",
            "test_ind: 2, Epoch: 2255, train_loss: 1.2865068944204678, valid_loss: 1.3065945368546705\n",
            "test_ind: 2, Epoch: 2256, train_loss: 1.2177477224826359, valid_loss: 1.2921454335889244\n",
            "test_ind: 2, Epoch: 2257, train_loss: 1.1778101291638379, valid_loss: 1.21001458847285\n",
            "test_ind: 2, Epoch: 2258, train_loss: 1.113344226241225, valid_loss: 1.09598555388274\n",
            "test_ind: 2, Epoch: 2259, train_loss: 1.2001211620577152, valid_loss: 1.3982602544659561\n",
            "test_ind: 2, Epoch: 2260, train_loss: 1.302597635718379, valid_loss: 1.4287937536538498\n",
            "test_ind: 2, Epoch: 2261, train_loss: 1.1873557839298519, valid_loss: 1.1604793988741362\n",
            "test_ind: 2, Epoch: 2262, train_loss: 1.1672895640729157, valid_loss: 1.0905780805821432\n",
            "test_ind: 2, Epoch: 2263, train_loss: 1.2453007358431478, valid_loss: 1.419387481151483\n",
            "test_ind: 2, Epoch: 2264, train_loss: 1.1324320011555524, valid_loss: 1.210540327930722\n",
            "test_ind: 2, Epoch: 2265, train_loss: 1.1692537742915432, valid_loss: 1.173418646184807\n",
            "test_ind: 2, Epoch: 2266, train_loss: 1.163942291073197, valid_loss: 1.141723776814605\n",
            "test_ind: 2, Epoch: 2267, train_loss: 1.1843695400560457, valid_loss: 1.159802259543003\n",
            "test_ind: 2, Epoch: 2268, train_loss: 1.2112197497857826, valid_loss: 1.08189656727674\n",
            "test_ind: 2, Epoch: 2269, train_loss: 1.266384616310214, valid_loss: 1.219868026907288\n",
            "test_ind: 2, Epoch: 2270, train_loss: 1.1771233518805144, valid_loss: 1.5194339127282472\n",
            "test_ind: 2, Epoch: 2271, train_loss: 1.2135025291134944, valid_loss: 1.0461501137823122\n",
            "test_ind: 2, Epoch: 2272, train_loss: 1.2267853294455882, valid_loss: 1.2062066905518882\n",
            "test_ind: 2, Epoch: 2273, train_loss: 1.2098640571405856, valid_loss: 1.2182769252364112\n",
            "test_ind: 2, Epoch: 2274, train_loss: 1.1456224639870842, valid_loss: 1.22809444397603\n",
            "test_ind: 2, Epoch: 2275, train_loss: 1.1086687806211872, valid_loss: 1.1181010928249087\n",
            "test_ind: 2, Epoch: 2276, train_loss: 1.1487171733707668, valid_loss: 1.152100417009446\n",
            "test_ind: 2, Epoch: 2277, train_loss: 1.1604294224455598, valid_loss: 1.1919280973255126\n",
            "test_ind: 2, Epoch: 2278, train_loss: 1.1104767933869972, valid_loss: 1.1038579064556677\n",
            "test_ind: 2, Epoch: 2279, train_loss: 1.2144645843071136, valid_loss: 1.214599400164395\n",
            "test_ind: 2, Epoch: 2280, train_loss: 1.1963452948690707, valid_loss: 1.2404728737312165\n",
            "test_ind: 2, Epoch: 2281, train_loss: 1.1675318709352642, valid_loss: 1.200475257346433\n",
            "test_ind: 2, Epoch: 2282, train_loss: 1.1541887337212773, valid_loss: 1.1429736390073075\n",
            "test_ind: 2, Epoch: 2283, train_loss: 1.1762952553241002, valid_loss: 1.151026844638705\n",
            "test_ind: 2, Epoch: 2284, train_loss: 1.2115108287572183, valid_loss: 1.1634731788580914\n",
            "test_ind: 2, Epoch: 2285, train_loss: 1.1786986769434393, valid_loss: 1.166565240278543\n",
            "test_ind: 2, Epoch: 2286, train_loss: 1.1293508139317758, valid_loss: 1.2061015656191398\n",
            "test_ind: 2, Epoch: 2287, train_loss: 1.1820746434946234, valid_loss: 1.0994327312860734\n",
            "test_ind: 2, Epoch: 2288, train_loss: 1.2065058705473897, valid_loss: 1.1367028989003933\n",
            "test_ind: 2, Epoch: 2289, train_loss: 1.1156340003126821, valid_loss: 1.107590295650341\n",
            "test_ind: 2, Epoch: 2290, train_loss: 1.1793090427702988, valid_loss: 1.092362040468091\n",
            "test_ind: 2, Epoch: 2291, train_loss: 1.1328558742943426, valid_loss: 1.204414100049228\n",
            "test_ind: 2, Epoch: 2292, train_loss: 1.2138698891243929, valid_loss: 1.1726782362685244\n",
            "test_ind: 2, Epoch: 2293, train_loss: 1.2051915361670689, valid_loss: 1.2685404455559885\n",
            "test_ind: 2, Epoch: 2294, train_loss: 1.14296155958547, valid_loss: 1.1897026786097773\n",
            "test_ind: 2, Epoch: 2295, train_loss: 1.1288079570161653, valid_loss: 1.0937422755097392\n",
            "test_ind: 2, Epoch: 2296, train_loss: 1.1520493655462891, valid_loss: 1.2839361201663981\n",
            "test_ind: 2, Epoch: 2297, train_loss: 1.1694969795821173, valid_loss: 1.1232643215744584\n",
            "test_ind: 2, Epoch: 2298, train_loss: 1.1097702296257925, valid_loss: 1.1993044692906218\n",
            "test_ind: 2, Epoch: 2299, train_loss: 1.22119589408918, valid_loss: 1.1788000326890211\n",
            "test_ind: 2, Epoch: 2300, train_loss: 1.1371076025514522, valid_loss: 1.0934363545855226\n",
            "test_ind: 2, Epoch: 2301, train_loss: 1.1406566184244038, valid_loss: 1.2253850097330208\n",
            "test_ind: 2, Epoch: 2302, train_loss: 1.1668688399612848, valid_loss: 1.0622702195094182\n",
            "test_ind: 2, Epoch: 2303, train_loss: 1.1947309044804442, valid_loss: 1.1042264397667345\n",
            "test_ind: 2, Epoch: 2304, train_loss: 1.1273631474457804, valid_loss: 1.1530486810580958\n",
            "test_ind: 2, Epoch: 2305, train_loss: 1.0976867764084426, valid_loss: 1.0924487630186597\n",
            "test_ind: 2, Epoch: 2306, train_loss: 1.1166481312863166, valid_loss: 1.0705963233936886\n",
            "test_ind: 2, Epoch: 2307, train_loss: 1.1311935280349747, valid_loss: 1.0962119958339593\n",
            "test_ind: 2, Epoch: 2308, train_loss: 1.2982643945717518, valid_loss: 1.6110706655388203\n",
            "test_ind: 2, Epoch: 2309, train_loss: 1.114653626964529, valid_loss: 1.1786622484864673\n",
            "test_ind: 2, Epoch: 2310, train_loss: 1.281440618484222, valid_loss: 1.3505649254192993\n",
            "test_ind: 2, Epoch: 2311, train_loss: 1.097726506272159, valid_loss: 1.031437487344117\n",
            "test_ind: 2, Epoch: 2312, train_loss: 1.2182173149311757, valid_loss: 1.0472557999809242\n",
            "test_ind: 2, Epoch: 2313, train_loss: 1.2170478550457204, valid_loss: 1.141528613207347\n",
            "test_ind: 2, Epoch: 2314, train_loss: 1.124305982988218, valid_loss: 1.0781050319345589\n",
            "test_ind: 2, Epoch: 2315, train_loss: 1.182474666850859, valid_loss: 1.2254116562356976\n",
            "test_ind: 2, Epoch: 2316, train_loss: 1.1344478596762486, valid_loss: 1.0834440145737085\n",
            "test_ind: 2, Epoch: 2317, train_loss: 1.1322685931822511, valid_loss: 1.0684845128290335\n",
            "test_ind: 2, Epoch: 2318, train_loss: 1.1412118755967302, valid_loss: 1.167153456272223\n",
            "test_ind: 2, Epoch: 2319, train_loss: 1.144549941518481, valid_loss: 1.0815892749362521\n",
            "test_ind: 2, Epoch: 2320, train_loss: 1.2003155809748431, valid_loss: 1.2053128722046855\n",
            "test_ind: 2, Epoch: 2321, train_loss: 1.2027608499228106, valid_loss: 1.1463913088850146\n",
            "test_ind: 2, Epoch: 2322, train_loss: 1.1124860533961543, valid_loss: 1.1674286993140848\n",
            "test_ind: 2, Epoch: 2323, train_loss: 1.1113695899079443, valid_loss: 1.1474137163569784\n",
            "test_ind: 2, Epoch: 2324, train_loss: 1.1621869980213422, valid_loss: 1.1572751829087564\n",
            "test_ind: 2, Epoch: 2325, train_loss: 1.1118138010798468, valid_loss: 1.018288848067281\n",
            "Validation loss decreased (1.0196601230534394 --> 1.018288848067281).  Saving model ...\n",
            "test_ind: 2, Epoch: 2326, train_loss: 1.1213883692495956, valid_loss: 1.1299264954026267\n",
            "test_ind: 2, Epoch: 2327, train_loss: 1.1770601141260335, valid_loss: 1.453404024455622\n",
            "test_ind: 2, Epoch: 2328, train_loss: 1.1063770684081944, valid_loss: 1.169042968342447\n",
            "test_ind: 2, Epoch: 2329, train_loss: 1.1121536440093067, valid_loss: 1.1227647789523132\n",
            "test_ind: 2, Epoch: 2330, train_loss: 1.1298657573073223, valid_loss: 1.0977880261902115\n",
            "test_ind: 2, Epoch: 2331, train_loss: 1.1789672999640135, valid_loss: 1.2000049777180382\n",
            "test_ind: 2, Epoch: 2332, train_loss: 1.133738911276416, valid_loss: 1.2840974439582933\n",
            "test_ind: 2, Epoch: 2333, train_loss: 1.1613984060423328, valid_loss: 1.0820498575172535\n",
            "test_ind: 2, Epoch: 2334, train_loss: 1.0990231424315362, valid_loss: 1.2232065907231084\n",
            "test_ind: 2, Epoch: 2335, train_loss: 1.132871770224793, valid_loss: 1.2396846911166808\n",
            "test_ind: 2, Epoch: 2336, train_loss: 1.1438379751877454, valid_loss: 1.0393398500915267\n",
            "test_ind: 2, Epoch: 2337, train_loss: 1.1482905336255023, valid_loss: 1.2835580548669538\n",
            "test_ind: 2, Epoch: 2338, train_loss: 1.160684436814398, valid_loss: 1.1341478892541\n",
            "test_ind: 2, Epoch: 2339, train_loss: 1.1938818712406574, valid_loss: 1.2203944211671836\n",
            "test_ind: 2, Epoch: 2340, train_loss: 1.1313063749673813, valid_loss: 1.1373924332806187\n",
            "test_ind: 2, Epoch: 2341, train_loss: 1.2308132775262322, valid_loss: 1.33410202132331\n",
            "test_ind: 2, Epoch: 2342, train_loss: 1.1537383877427263, valid_loss: 1.0456018216929206\n",
            "test_ind: 2, Epoch: 2343, train_loss: 1.1003519272872186, valid_loss: 0.9818519133108633\n",
            "Validation loss decreased (1.018288848067281 --> 0.9818519133108633).  Saving model ...\n",
            "test_ind: 2, Epoch: 2344, train_loss: 1.1169531146119236, valid_loss: 1.3788854335448\n",
            "test_ind: 2, Epoch: 2345, train_loss: 1.1306685285124458, valid_loss: 1.2405234096396682\n",
            "test_ind: 2, Epoch: 2346, train_loss: 1.1052069353808254, valid_loss: 1.074097680230426\n",
            "test_ind: 2, Epoch: 2347, train_loss: 1.144883754246595, valid_loss: 1.1038878425913319\n",
            "test_ind: 2, Epoch: 2348, train_loss: 1.1238384645322563, valid_loss: 1.3313759949132589\n",
            "test_ind: 2, Epoch: 2349, train_loss: 1.1063271723581516, valid_loss: 1.0416731460821256\n",
            "test_ind: 2, Epoch: 2350, train_loss: 1.1682089375860898, valid_loss: 1.2702547423859947\n",
            "test_ind: 2, Epoch: 2351, train_loss: 1.11884065050232, valid_loss: 1.011890390999297\n",
            "test_ind: 2, Epoch: 2352, train_loss: 1.1074498392577863, valid_loss: 1.1949619377440537\n",
            "test_ind: 2, Epoch: 2353, train_loss: 1.1394159979516851, valid_loss: 1.1254147797228604\n",
            "test_ind: 2, Epoch: 2354, train_loss: 1.156646323000264, valid_loss: 1.122937085621717\n",
            "test_ind: 2, Epoch: 2355, train_loss: 1.167175204891991, valid_loss: 1.101245184569617\n",
            "test_ind: 2, Epoch: 2356, train_loss: 1.1897998850116929, valid_loss: 1.1604928039757274\n",
            "test_ind: 2, Epoch: 2357, train_loss: 1.1054459858936914, valid_loss: 1.0217114140165497\n",
            "test_ind: 2, Epoch: 2358, train_loss: 1.2132492669966828, valid_loss: 1.222999234484811\n",
            "test_ind: 2, Epoch: 2359, train_loss: 1.208764763186347, valid_loss: 1.4665961238393757\n",
            "test_ind: 2, Epoch: 2360, train_loss: 1.1588970860864365, valid_loss: 1.0878270535047916\n",
            "test_ind: 2, Epoch: 2361, train_loss: 1.1272035079803902, valid_loss: 1.1125627623664007\n",
            "test_ind: 2, Epoch: 2362, train_loss: 1.138482335399472, valid_loss: 1.1028201430611462\n",
            "test_ind: 2, Epoch: 2363, train_loss: 1.1190427164746146, valid_loss: 1.169092890883443\n",
            "test_ind: 2, Epoch: 2364, train_loss: 1.12351874114215, valid_loss: 1.097174306880375\n",
            "test_ind: 2, Epoch: 2365, train_loss: 1.1799558255067917, valid_loss: 1.3033820801650697\n",
            "test_ind: 2, Epoch: 2366, train_loss: 1.1240072143949558, valid_loss: 1.199454567710898\n",
            "test_ind: 2, Epoch: 2367, train_loss: 1.1848949681886127, valid_loss: 1.110323422315114\n",
            "test_ind: 2, Epoch: 2368, train_loss: 1.1835236395412925, valid_loss: 1.215367906113975\n",
            "test_ind: 2, Epoch: 2369, train_loss: 1.155053018051901, valid_loss: 1.1018084610289658\n",
            "test_ind: 2, Epoch: 2370, train_loss: 1.0982734409832209, valid_loss: 1.0838951908285461\n",
            "test_ind: 2, Epoch: 2371, train_loss: 1.1187866879777466, valid_loss: 1.3162008880550027\n",
            "test_ind: 2, Epoch: 2372, train_loss: 1.1787656415448455, valid_loss: 1.0899932411661175\n",
            "test_ind: 2, Epoch: 2373, train_loss: 1.1503254183110803, valid_loss: 1.0303663426315004\n",
            "test_ind: 2, Epoch: 2374, train_loss: 1.1478395036369533, valid_loss: 1.036205558695345\n",
            "test_ind: 2, Epoch: 2375, train_loss: 1.1344778012459549, valid_loss: 1.155621777912151\n",
            "test_ind: 2, Epoch: 2376, train_loss: 1.1474558306329043, valid_loss: 1.1785647590615471\n",
            "test_ind: 2, Epoch: 2377, train_loss: 1.2547399943370767, valid_loss: 1.2910307662779108\n",
            "test_ind: 2, Epoch: 2378, train_loss: 1.1848099526063895, valid_loss: 1.0299856832564047\n",
            "test_ind: 2, Epoch: 2379, train_loss: 1.1313994589694205, valid_loss: 1.1265388737376938\n",
            "test_ind: 2, Epoch: 2380, train_loss: 1.1990520111855616, valid_loss: 1.1982823810686072\n",
            "test_ind: 2, Epoch: 2381, train_loss: 1.1966033584145512, valid_loss: 1.1564872801473678\n",
            "test_ind: 2, Epoch: 2382, train_loss: 1.1392761802854474, valid_loss: 1.0760863852976394\n",
            "test_ind: 2, Epoch: 2383, train_loss: 1.0914326905072134, valid_loss: 1.0455642772196365\n",
            "test_ind: 2, Epoch: 2384, train_loss: 1.1410832008858127, valid_loss: 1.0750476200016816\n",
            "test_ind: 2, Epoch: 2385, train_loss: 1.2091409557791744, valid_loss: 1.2517077664704064\n",
            "test_ind: 2, Epoch: 2386, train_loss: 1.216895112964163, valid_loss: 1.1588451081191713\n",
            "test_ind: 2, Epoch: 2387, train_loss: 1.0918930410093506, valid_loss: 1.0641094829961446\n",
            "test_ind: 2, Epoch: 2388, train_loss: 1.0818541502794081, valid_loss: 1.0806465834973544\n",
            "test_ind: 2, Epoch: 2389, train_loss: 1.261419535136064, valid_loss: 1.1536474255075482\n",
            "test_ind: 2, Epoch: 2390, train_loss: 1.1874094373933044, valid_loss: 1.1236822380978837\n",
            "test_ind: 2, Epoch: 2391, train_loss: 1.1100188863922729, valid_loss: 1.0335547299126953\n",
            "test_ind: 2, Epoch: 2392, train_loss: 1.2342984513339834, valid_loss: 1.2699801385232865\n",
            "test_ind: 2, Epoch: 2393, train_loss: 1.1150976282465719, valid_loss: 1.359253553243784\n",
            "test_ind: 2, Epoch: 2394, train_loss: 1.0849514861165743, valid_loss: 1.0174550758807408\n",
            "test_ind: 2, Epoch: 2395, train_loss: 1.040328752620947, valid_loss: 0.9958485334347456\n",
            "test_ind: 2, Epoch: 2396, train_loss: 1.0829565264673766, valid_loss: 1.0263703603010912\n",
            "test_ind: 2, Epoch: 2397, train_loss: 1.11982843678901, valid_loss: 1.0518313690468117\n",
            "test_ind: 2, Epoch: 2398, train_loss: 1.114066605327929, valid_loss: 1.0786001906435714\n",
            "test_ind: 2, Epoch: 2399, train_loss: 1.153650532420884, valid_loss: 0.9829649375035212\n",
            "test_ind: 2, Epoch: 2400, train_loss: 1.0583756739823793, valid_loss: 1.1375910468250938\n",
            "test_ind: 2, Epoch: 2401, train_loss: 1.1715462864407562, valid_loss: 1.3837262599216906\n",
            "test_ind: 2, Epoch: 2402, train_loss: 1.1213388999982439, valid_loss: 1.4416018761800564\n",
            "test_ind: 2, Epoch: 2403, train_loss: 1.1224817498343849, valid_loss: 1.101237051167719\n",
            "test_ind: 2, Epoch: 2404, train_loss: 1.1332986465772898, valid_loss: 1.3569217359917796\n",
            "test_ind: 2, Epoch: 2405, train_loss: 1.2047291492578085, valid_loss: 1.2175180626730633\n",
            "test_ind: 2, Epoch: 2406, train_loss: 1.1006574205070705, valid_loss: 1.2190857945684015\n",
            "test_ind: 2, Epoch: 2407, train_loss: 1.209344754757931, valid_loss: 1.1066750631033526\n",
            "test_ind: 2, Epoch: 2408, train_loss: 1.1105040025394073, valid_loss: 1.1247284561820179\n",
            "test_ind: 2, Epoch: 2409, train_loss: 1.1669980130191089, valid_loss: 1.3329315871594638\n",
            "test_ind: 2, Epoch: 2410, train_loss: 1.1166936968579704, valid_loss: 1.166556733965534\n",
            "test_ind: 2, Epoch: 2411, train_loss: 1.2303228416787029, valid_loss: 1.5404643204137471\n",
            "test_ind: 2, Epoch: 2412, train_loss: 1.1787742914071226, valid_loss: 1.2638243521720254\n",
            "test_ind: 2, Epoch: 2413, train_loss: 1.1674095907329043, valid_loss: 1.3472847979292912\n",
            "test_ind: 2, Epoch: 2414, train_loss: 1.0840722685865527, valid_loss: 1.1363385253482394\n",
            "test_ind: 2, Epoch: 2415, train_loss: 1.128286260258891, valid_loss: 1.0406124748056091\n",
            "test_ind: 2, Epoch: 2416, train_loss: 1.1152987799413523, valid_loss: 1.169863535807683\n",
            "test_ind: 2, Epoch: 2417, train_loss: 1.149359308416687, valid_loss: 1.1130969999862192\n",
            "test_ind: 2, Epoch: 2418, train_loss: 1.1108635481719393, valid_loss: 1.0514647084423618\n",
            "test_ind: 2, Epoch: 2419, train_loss: 1.161672741825651, valid_loss: 1.2024979958167443\n",
            "test_ind: 2, Epoch: 2420, train_loss: 1.1265916559431288, valid_loss: 1.2353010184404858\n",
            "test_ind: 2, Epoch: 2421, train_loss: 1.15530745641232, valid_loss: 1.1886921798401748\n",
            "test_ind: 2, Epoch: 2422, train_loss: 1.1600065170190272, valid_loss: 1.3974136402803947\n",
            "test_ind: 2, Epoch: 2423, train_loss: 1.233748387067746, valid_loss: 1.03864886346366\n",
            "test_ind: 2, Epoch: 2424, train_loss: 1.1692852405627794, valid_loss: 1.1103124930987671\n",
            "test_ind: 2, Epoch: 2425, train_loss: 1.203458517704934, valid_loss: 1.0632932987647858\n",
            "test_ind: 2, Epoch: 2426, train_loss: 1.107650929366761, valid_loss: 1.1226386847319425\n",
            "test_ind: 2, Epoch: 2427, train_loss: 1.1180175971894295, valid_loss: 1.0110015577060885\n",
            "test_ind: 2, Epoch: 2428, train_loss: 1.123111983196914, valid_loss: 1.059840156821444\n",
            "test_ind: 2, Epoch: 2429, train_loss: 1.0700612385162034, valid_loss: 1.1594725891395852\n",
            "test_ind: 2, Epoch: 2430, train_loss: 1.1856307073196455, valid_loss: 1.0449788122095613\n",
            "test_ind: 2, Epoch: 2431, train_loss: 1.1487582662732738, valid_loss: 1.0432511252215786\n",
            "test_ind: 2, Epoch: 2432, train_loss: 1.2210529054331984, valid_loss: 1.1944757954687133\n",
            "test_ind: 2, Epoch: 2433, train_loss: 1.1266155582547528, valid_loss: 1.1178201786812894\n",
            "test_ind: 2, Epoch: 2434, train_loss: 1.1190881228741303, valid_loss: 1.0609485946829162\n",
            "test_ind: 2, Epoch: 2435, train_loss: 1.0832654598884548, valid_loss: 1.1152952118137285\n",
            "test_ind: 2, Epoch: 2436, train_loss: 1.2403032935922302, valid_loss: 1.2710804789833872\n",
            "test_ind: 2, Epoch: 2437, train_loss: 1.2166863782906239, valid_loss: 1.1544332327666105\n",
            "test_ind: 2, Epoch: 2438, train_loss: 1.1583690518780423, valid_loss: 1.4461169752300294\n",
            "test_ind: 2, Epoch: 2439, train_loss: 1.1607872389207423, valid_loss: 1.2011741674863377\n",
            "test_ind: 2, Epoch: 2440, train_loss: 1.0883547512554377, valid_loss: 1.1690885205554147\n",
            "test_ind: 2, Epoch: 2441, train_loss: 1.1317838055682203, valid_loss: 1.1017990241363176\n",
            "test_ind: 2, Epoch: 2442, train_loss: 1.1447563843849378, valid_loss: 1.1317647522331302\n",
            "test_ind: 2, Epoch: 2443, train_loss: 1.133194689510668, valid_loss: 1.077436589787149\n",
            "test_ind: 2, Epoch: 2444, train_loss: 1.1700467705613413, valid_loss: 1.12494644548139\n",
            "test_ind: 2, Epoch: 2445, train_loss: 1.1935659342455163, valid_loss: 1.4654007195747136\n",
            "test_ind: 2, Epoch: 2446, train_loss: 1.115754992647162, valid_loss: 1.2661605085063186\n",
            "test_ind: 2, Epoch: 2447, train_loss: 1.159022972907549, valid_loss: 0.9946373521092949\n",
            "test_ind: 2, Epoch: 2448, train_loss: 1.083885065171114, valid_loss: 1.0249618145814987\n",
            "test_ind: 2, Epoch: 2449, train_loss: 1.1261886816758377, valid_loss: 1.2135479429848173\n",
            "test_ind: 2, Epoch: 2450, train_loss: 1.0712296376314372, valid_loss: 0.9370170875831887\n",
            "Validation loss decreased (0.9818519133108633 --> 0.9370170875831887).  Saving model ...\n",
            "test_ind: 2, Epoch: 2451, train_loss: 1.1327904110733713, valid_loss: 1.0038042605093063\n",
            "test_ind: 2, Epoch: 2452, train_loss: 1.1613711550025179, valid_loss: 1.265266006149118\n",
            "test_ind: 2, Epoch: 2453, train_loss: 1.0895718846678848, valid_loss: 1.0195706773687292\n",
            "test_ind: 2, Epoch: 2454, train_loss: 1.1095913319166568, valid_loss: 1.0446029735086988\n",
            "test_ind: 2, Epoch: 2455, train_loss: 1.0392847962302474, valid_loss: 1.0113041760914685\n",
            "test_ind: 2, Epoch: 2456, train_loss: 1.1247230418387302, valid_loss: 1.165686074145499\n",
            "test_ind: 2, Epoch: 2457, train_loss: 1.067077541396602, valid_loss: 1.0495812254413919\n",
            "test_ind: 2, Epoch: 2458, train_loss: 1.1135221650684208, valid_loss: 1.1720645217134742\n",
            "test_ind: 2, Epoch: 2459, train_loss: 1.1267305828793661, valid_loss: 1.089732460826211\n",
            "test_ind: 2, Epoch: 2460, train_loss: 1.108573277791341, valid_loss: 1.0017623222111978\n",
            "test_ind: 2, Epoch: 2461, train_loss: 1.0853056001980195, valid_loss: 1.0719269571820556\n",
            "test_ind: 2, Epoch: 2462, train_loss: 1.1194766425905291, valid_loss: 1.1736855860109683\n",
            "test_ind: 2, Epoch: 2463, train_loss: 1.1112509308151144, valid_loss: 1.2349771808015655\n",
            "test_ind: 2, Epoch: 2464, train_loss: 1.1625087541958772, valid_loss: 1.0396607601404868\n",
            "test_ind: 2, Epoch: 2465, train_loss: 1.0996624563041464, valid_loss: 1.0689806123065133\n",
            "test_ind: 2, Epoch: 2466, train_loss: 1.1078050276945572, valid_loss: 1.2858790896217367\n",
            "test_ind: 2, Epoch: 2467, train_loss: 1.131354422763995, valid_loss: 1.2419707279259664\n",
            "test_ind: 2, Epoch: 2468, train_loss: 1.0899507528469885, valid_loss: 1.1233448703744133\n",
            "test_ind: 2, Epoch: 2469, train_loss: 1.1360986074264687, valid_loss: 1.0450405680555903\n",
            "test_ind: 2, Epoch: 2470, train_loss: 1.1565082729825042, valid_loss: 1.3041470553460623\n",
            "test_ind: 2, Epoch: 2471, train_loss: 1.1182651180147785, valid_loss: 1.0725124507208497\n",
            "test_ind: 2, Epoch: 2472, train_loss: 1.16170841042246, valid_loss: 1.3643733246034366\n",
            "test_ind: 2, Epoch: 2473, train_loss: 1.2040073991614302, valid_loss: 1.147399024066762\n",
            "test_ind: 2, Epoch: 2474, train_loss: 1.0636871094264422, valid_loss: 1.253411970926486\n",
            "test_ind: 2, Epoch: 2475, train_loss: 1.184746416885289, valid_loss: 1.3336378271423515\n",
            "test_ind: 2, Epoch: 2476, train_loss: 1.0739985737252664, valid_loss: 1.054552858711308\n",
            "test_ind: 2, Epoch: 2477, train_loss: 1.15856550199467, valid_loss: 1.1233523523705637\n",
            "test_ind: 2, Epoch: 2478, train_loss: 1.1249423332703419, valid_loss: 1.0636442454791815\n",
            "test_ind: 2, Epoch: 2479, train_loss: 1.0757289668660104, valid_loss: 1.0006676453810472\n",
            "test_ind: 2, Epoch: 2480, train_loss: 1.1214381957891886, valid_loss: 1.0573608793764033\n",
            "test_ind: 2, Epoch: 2481, train_loss: 1.1142250640213434, valid_loss: 1.1078914862412672\n",
            "test_ind: 2, Epoch: 2482, train_loss: 1.0844212472721835, valid_loss: 0.989760546942382\n",
            "test_ind: 2, Epoch: 2483, train_loss: 1.0654240573799736, valid_loss: 1.1251797377214134\n",
            "test_ind: 2, Epoch: 2484, train_loss: 1.1731491985484066, valid_loss: 1.0663891035607058\n",
            "test_ind: 2, Epoch: 2485, train_loss: 1.1068389234606246, valid_loss: 1.1399284210639802\n",
            "test_ind: 2, Epoch: 2486, train_loss: 1.1028714238861461, valid_loss: 1.043651166464868\n",
            "test_ind: 2, Epoch: 2487, train_loss: 1.0725474597608489, valid_loss: 1.0615415505194596\n",
            "test_ind: 2, Epoch: 2488, train_loss: 1.0870283419816467, valid_loss: 1.1420816709513\n",
            "test_ind: 2, Epoch: 2489, train_loss: 1.0482450611570961, valid_loss: 1.0637733114411008\n",
            "test_ind: 2, Epoch: 2490, train_loss: 1.0868772412070973, valid_loss: 1.0680660610525017\n",
            "test_ind: 2, Epoch: 2491, train_loss: 1.1461388863276438, valid_loss: 1.0972775631820375\n",
            "test_ind: 2, Epoch: 2492, train_loss: 1.0106046306215235, valid_loss: 1.2407638673429136\n",
            "test_ind: 2, Epoch: 2493, train_loss: 1.1133968771239855, valid_loss: 1.129667972907042\n",
            "test_ind: 2, Epoch: 2494, train_loss: 1.1210626706778155, valid_loss: 0.9932551703222118\n",
            "test_ind: 2, Epoch: 2495, train_loss: 1.0980408773576207, valid_loss: 1.461661605074195\n",
            "test_ind: 2, Epoch: 2496, train_loss: 1.1475441374330442, valid_loss: 1.1531411141072243\n",
            "test_ind: 2, Epoch: 2497, train_loss: 1.1215742646459161, valid_loss: 1.1868646511664758\n",
            "test_ind: 2, Epoch: 2498, train_loss: 1.1005545531922256, valid_loss: 1.1114474049320926\n",
            "test_ind: 2, Epoch: 2499, train_loss: 1.1007643717306632, valid_loss: 1.0906124868963518\n",
            "test_ind: 2, Epoch: 2500, train_loss: 1.0780613657868938, valid_loss: 1.0176657339786193\n",
            "test_ind: 2, Epoch: 2501, train_loss: 1.0638858610861437, valid_loss: 1.0245902932267583\n",
            "test_ind: 2, Epoch: 2502, train_loss: 1.063101939892384, valid_loss: 1.057023862827877\n",
            "test_ind: 2, Epoch: 2503, train_loss: 1.164682653668033, valid_loss: 1.1102140275841086\n",
            "test_ind: 2, Epoch: 2504, train_loss: 1.1479967446521928, valid_loss: 1.1564020689396437\n",
            "test_ind: 2, Epoch: 2505, train_loss: 1.092966350055488, valid_loss: 1.2205482548118658\n",
            "test_ind: 2, Epoch: 2506, train_loss: 1.1020485947274754, valid_loss: 0.9649798333475053\n",
            "test_ind: 2, Epoch: 2507, train_loss: 1.08269783257306, valid_loss: 1.1079102812329589\n",
            "test_ind: 2, Epoch: 2508, train_loss: 1.0650799326520812, valid_loss: 1.2118857049534464\n",
            "test_ind: 2, Epoch: 2509, train_loss: 1.1844845750959057, valid_loss: 1.0421902562817955\n",
            "test_ind: 2, Epoch: 2510, train_loss: 1.0730235098886807, valid_loss: 1.0670505120204046\n",
            "test_ind: 2, Epoch: 2511, train_loss: 1.0997355330703604, valid_loss: 1.0797612660291187\n",
            "test_ind: 2, Epoch: 2512, train_loss: 1.031584190167593, valid_loss: 1.225210924094219\n",
            "test_ind: 2, Epoch: 2513, train_loss: 1.147948083488124, valid_loss: 1.2437074612348507\n",
            "test_ind: 2, Epoch: 2514, train_loss: 1.1210524714344021, valid_loss: 1.0252267811712716\n",
            "test_ind: 2, Epoch: 2515, train_loss: 1.1082098017617394, valid_loss: 1.1226772270311318\n",
            "test_ind: 2, Epoch: 2516, train_loss: 1.0762061527085554, valid_loss: 1.0206600003093056\n",
            "test_ind: 2, Epoch: 2517, train_loss: 1.1087808570517659, valid_loss: 1.0420129095387254\n",
            "test_ind: 2, Epoch: 2518, train_loss: 1.0587166698343506, valid_loss: 1.0243525083927687\n",
            "test_ind: 2, Epoch: 2519, train_loss: 1.0490275880663256, valid_loss: 1.0516353896540456\n",
            "test_ind: 2, Epoch: 2520, train_loss: 1.2493348775992252, valid_loss: 1.5086457776887463\n",
            "test_ind: 2, Epoch: 2521, train_loss: 1.0815788483687618, valid_loss: 1.0028242602986828\n",
            "test_ind: 2, Epoch: 2522, train_loss: 1.056706283167217, valid_loss: 0.997557377203917\n",
            "test_ind: 2, Epoch: 2523, train_loss: 1.1936670828182587, valid_loss: 1.2426457391505228\n",
            "test_ind: 2, Epoch: 2524, train_loss: 1.1658892518321557, valid_loss: 1.2103052607968323\n",
            "test_ind: 2, Epoch: 2525, train_loss: 1.1058036513477987, valid_loss: 1.0064551721610915\n",
            "test_ind: 2, Epoch: 2526, train_loss: 1.1396342340471173, valid_loss: 1.2601666464085592\n",
            "test_ind: 2, Epoch: 2527, train_loss: 1.0764806965703864, valid_loss: 1.1676240014894055\n",
            "test_ind: 2, Epoch: 2528, train_loss: 1.1094216057378004, valid_loss: 1.3753728622045271\n",
            "test_ind: 2, Epoch: 2529, train_loss: 1.1112152855394004, valid_loss: 1.2773179776987797\n",
            "test_ind: 2, Epoch: 2530, train_loss: 1.076588597845601, valid_loss: 1.1823075299928671\n",
            "test_ind: 2, Epoch: 2531, train_loss: 1.1175160238206219, valid_loss: 1.0725903456706947\n",
            "test_ind: 2, Epoch: 2532, train_loss: 1.1159754394918877, valid_loss: 1.006180863095145\n",
            "test_ind: 2, Epoch: 2533, train_loss: 1.076114456877749, valid_loss: 1.0583645978204883\n",
            "test_ind: 2, Epoch: 2534, train_loss: 1.1329039669218002, valid_loss: 1.0052644663047248\n",
            "test_ind: 2, Epoch: 2535, train_loss: 1.1588289948270531, valid_loss: 1.048641165437182\n",
            "test_ind: 2, Epoch: 2536, train_loss: 1.0711850608289526, valid_loss: 1.042068865903762\n",
            "test_ind: 2, Epoch: 2537, train_loss: 1.1002861300991018, valid_loss: 1.046414971691251\n",
            "test_ind: 2, Epoch: 2538, train_loss: 1.1636052788266202, valid_loss: 1.046100689814641\n",
            "test_ind: 2, Epoch: 2539, train_loss: 1.0804939489645382, valid_loss: 1.1975592323857496\n",
            "test_ind: 2, Epoch: 2540, train_loss: 1.0576424709638865, valid_loss: 1.0778693195082183\n",
            "test_ind: 2, Epoch: 2541, train_loss: 1.0631722369198557, valid_loss: 1.1142187003056887\n",
            "test_ind: 2, Epoch: 2542, train_loss: 1.055326249185111, valid_loss: 1.3111939233252805\n",
            "test_ind: 2, Epoch: 2543, train_loss: 1.099500812809465, valid_loss: 1.0197070059273656\n",
            "test_ind: 2, Epoch: 2544, train_loss: 1.128460306727309, valid_loss: 1.12201957349424\n",
            "test_ind: 2, Epoch: 2545, train_loss: 1.2014739923214528, valid_loss: 1.1899520494999032\n",
            "test_ind: 2, Epoch: 2546, train_loss: 1.0449254062214242, valid_loss: 0.9604494999616573\n",
            "test_ind: 2, Epoch: 2547, train_loss: 1.0615690601290917, valid_loss: 1.0422941796120755\n",
            "test_ind: 2, Epoch: 2548, train_loss: 1.1058450109711853, valid_loss: 1.1710231120769794\n",
            "test_ind: 2, Epoch: 2549, train_loss: 1.1556728271111238, valid_loss: 1.2241754185440195\n",
            "test_ind: 2, Epoch: 2550, train_loss: 1.0518114435480304, valid_loss: 1.1123064943188616\n",
            "test_ind: 2, Epoch: 2551, train_loss: 1.0731881142568271, valid_loss: 1.3707566560163795\n",
            "test_ind: 2, Epoch: 2552, train_loss: 1.1185800259382748, valid_loss: 1.0848686124524498\n",
            "test_ind: 2, Epoch: 2553, train_loss: 1.1431636355201744, valid_loss: 1.2297953628746532\n",
            "test_ind: 2, Epoch: 2554, train_loss: 1.040221222898333, valid_loss: 1.132325107215816\n",
            "test_ind: 2, Epoch: 2555, train_loss: 1.1044605102068112, valid_loss: 0.9942905868899788\n",
            "test_ind: 2, Epoch: 2556, train_loss: 1.1306770495426508, valid_loss: 1.0524805081196322\n",
            "test_ind: 2, Epoch: 2557, train_loss: 1.1740078570609531, valid_loss: 0.9931687615875504\n",
            "test_ind: 2, Epoch: 2558, train_loss: 1.0729900432108475, valid_loss: 0.9379165335598154\n",
            "test_ind: 2, Epoch: 2559, train_loss: 1.0696230989802145, valid_loss: 1.022131926653392\n",
            "test_ind: 2, Epoch: 2560, train_loss: 1.0594291997204932, valid_loss: 1.0357251975611064\n",
            "test_ind: 2, Epoch: 2561, train_loss: 1.1299081718140516, valid_loss: 1.519198522948132\n",
            "test_ind: 2, Epoch: 2562, train_loss: 1.048478196715584, valid_loss: 1.0711591821110826\n",
            "test_ind: 2, Epoch: 2563, train_loss: 1.1265817555720083, valid_loss: 1.0572510010156877\n",
            "test_ind: 2, Epoch: 2564, train_loss: 1.05523625330368, valid_loss: 1.2566664300413213\n",
            "test_ind: 2, Epoch: 2565, train_loss: 1.0977491268292, valid_loss: 1.1435200434464674\n",
            "test_ind: 2, Epoch: 2566, train_loss: 1.0633392431344288, valid_loss: 1.0749769129304805\n",
            "test_ind: 2, Epoch: 2567, train_loss: 1.0976561879613573, valid_loss: 1.158431069463746\n",
            "test_ind: 2, Epoch: 2568, train_loss: 1.077370013946142, valid_loss: 1.111652003394233\n",
            "test_ind: 2, Epoch: 2569, train_loss: 1.152999464942519, valid_loss: 1.0695086612321032\n",
            "test_ind: 2, Epoch: 2570, train_loss: 1.0914851414625237, valid_loss: 1.176401707521531\n",
            "test_ind: 2, Epoch: 2571, train_loss: 1.1206518372013132, valid_loss: 1.2474214799723393\n",
            "test_ind: 2, Epoch: 2572, train_loss: 1.0900734713953784, valid_loss: 1.047182762385094\n",
            "test_ind: 2, Epoch: 2573, train_loss: 1.1517028767838435, valid_loss: 0.9679246573706299\n",
            "test_ind: 2, Epoch: 2574, train_loss: 1.1549903510982156, valid_loss: 1.1991116103962955\n",
            "test_ind: 2, Epoch: 2575, train_loss: 1.0338905441794979, valid_loss: 1.0357504719682569\n",
            "test_ind: 2, Epoch: 2576, train_loss: 1.09997360726707, valid_loss: 1.1937426264129813\n",
            "test_ind: 2, Epoch: 2577, train_loss: 1.086371350265749, valid_loss: 1.085674730800835\n",
            "test_ind: 2, Epoch: 2578, train_loss: 1.038040990277007, valid_loss: 1.0516638022202711\n",
            "test_ind: 2, Epoch: 2579, train_loss: 1.0739501521118688, valid_loss: 1.0894171641423152\n",
            "test_ind: 2, Epoch: 2580, train_loss: 1.07624965153302, valid_loss: 1.2041357541695619\n",
            "test_ind: 2, Epoch: 2581, train_loss: 1.1060654222682218, valid_loss: 1.1500167615732915\n",
            "test_ind: 2, Epoch: 2582, train_loss: 1.0768226965879781, valid_loss: 1.0986142987199656\n",
            "test_ind: 2, Epoch: 2583, train_loss: 1.144826687752125, valid_loss: 1.3078271513990527\n",
            "test_ind: 2, Epoch: 2584, train_loss: 1.1248396928267375, valid_loss: 1.0814487383915827\n",
            "test_ind: 2, Epoch: 2585, train_loss: 1.0358839424473834, valid_loss: 1.019029018206474\n",
            "test_ind: 2, Epoch: 2586, train_loss: 1.0695132063551394, valid_loss: 1.1294202118517667\n",
            "test_ind: 2, Epoch: 2587, train_loss: 1.0641676901865325, valid_loss: 1.1333681728765155\n",
            "test_ind: 2, Epoch: 2588, train_loss: 1.1077855446173368, valid_loss: 1.2576622249733689\n",
            "test_ind: 2, Epoch: 2589, train_loss: 1.0198462835857105, valid_loss: 1.080482691441506\n",
            "test_ind: 2, Epoch: 2590, train_loss: 1.054596624256652, valid_loss: 1.0702453243766414\n",
            "test_ind: 2, Epoch: 2591, train_loss: 1.1331841196203276, valid_loss: 1.1138643016163101\n",
            "test_ind: 2, Epoch: 2592, train_loss: 1.121803512373994, valid_loss: 1.200963281158708\n",
            "test_ind: 2, Epoch: 2593, train_loss: 1.123988544159805, valid_loss: 1.4455766800122385\n",
            "test_ind: 2, Epoch: 2594, train_loss: 1.0951072637625003, valid_loss: 1.2432366327682451\n",
            "test_ind: 2, Epoch: 2595, train_loss: 1.102674703199526, valid_loss: 1.1603738256329483\n",
            "test_ind: 2, Epoch: 2596, train_loss: 1.1230399830502096, valid_loss: 1.1598238170656383\n",
            "test_ind: 2, Epoch: 2597, train_loss: 1.085250715018451, valid_loss: 1.0083798499528498\n",
            "test_ind: 2, Epoch: 2598, train_loss: 1.0522922289903571, valid_loss: 1.0432379198210193\n",
            "test_ind: 2, Epoch: 2599, train_loss: 1.101000108610191, valid_loss: 1.8669935074287263\n",
            "test_ind: 2, Epoch: 2600, train_loss: 1.140461083490964, valid_loss: 0.9892738993011649\n",
            "test_ind: 2, Epoch: 2601, train_loss: 1.0743001433406008, valid_loss: 0.9947111117534148\n",
            "test_ind: 2, Epoch: 2602, train_loss: 1.0275425881515314, valid_loss: 1.0679669991517677\n",
            "test_ind: 2, Epoch: 2603, train_loss: 1.056986038954283, valid_loss: 1.0764125554989545\n",
            "test_ind: 2, Epoch: 2604, train_loss: 1.1264266476445501, valid_loss: 0.994607088572619\n",
            "test_ind: 2, Epoch: 2605, train_loss: 1.1257270616457107, valid_loss: 1.14847242526519\n",
            "test_ind: 2, Epoch: 2606, train_loss: 1.1308135725946962, valid_loss: 1.1750608529800024\n",
            "test_ind: 2, Epoch: 2607, train_loss: 1.044532244022076, valid_loss: 0.9986938513242282\n",
            "test_ind: 2, Epoch: 2608, train_loss: 1.0926423029795445, valid_loss: 1.062643328283587\n",
            "test_ind: 2, Epoch: 2609, train_loss: 1.0691534626065042, valid_loss: 1.0467312003132965\n",
            "test_ind: 2, Epoch: 2610, train_loss: 1.1437744756935895, valid_loss: 1.1604817871354585\n",
            "test_ind: 2, Epoch: 2611, train_loss: 1.086611151355624, valid_loss: 1.2390447566312264\n",
            "test_ind: 2, Epoch: 2612, train_loss: 1.0781106165444863, valid_loss: 1.1867428844810552\n",
            "test_ind: 2, Epoch: 2613, train_loss: 1.057711690919012, valid_loss: 1.2059827027497467\n",
            "test_ind: 2, Epoch: 2614, train_loss: 1.0906864847326323, valid_loss: 1.0859889793939401\n",
            "test_ind: 2, Epoch: 2615, train_loss: 1.0897061491963531, valid_loss: 1.0837962097591824\n",
            "test_ind: 2, Epoch: 2616, train_loss: 1.0246267542879806, valid_loss: 0.9779427628911119\n",
            "test_ind: 2, Epoch: 2617, train_loss: 1.0774285025746055, valid_loss: 0.9479434829831462\n",
            "test_ind: 2, Epoch: 2618, train_loss: 1.0441991039830396, valid_loss: 1.0427935816283918\n",
            "test_ind: 2, Epoch: 2619, train_loss: 1.0505278574661878, valid_loss: 1.1821517224325413\n",
            "test_ind: 2, Epoch: 2620, train_loss: 1.114716049612757, valid_loss: 1.0880085009115714\n",
            "test_ind: 2, Epoch: 2621, train_loss: 1.062719390829291, valid_loss: 1.1088188706639825\n",
            "test_ind: 2, Epoch: 2622, train_loss: 1.0602491257423914, valid_loss: 1.097611948975131\n",
            "test_ind: 2, Epoch: 2623, train_loss: 1.0473179004357638, valid_loss: 1.1272936431091396\n",
            "test_ind: 2, Epoch: 2624, train_loss: 1.0448138385529984, valid_loss: 0.9942421118418376\n",
            "test_ind: 2, Epoch: 2625, train_loss: 1.058225154423872, valid_loss: 1.0620535479651558\n",
            "test_ind: 2, Epoch: 2626, train_loss: 1.1157509590033001, valid_loss: 1.1784518729587565\n",
            "test_ind: 2, Epoch: 2627, train_loss: 1.054365344876238, valid_loss: 1.186819954815074\n",
            "test_ind: 2, Epoch: 2628, train_loss: 1.0893617735968697, valid_loss: 1.243369387765216\n",
            "test_ind: 2, Epoch: 2629, train_loss: 1.0947237476664051, valid_loss: 1.1448789199872575\n",
            "test_ind: 2, Epoch: 2630, train_loss: 1.0224782936027361, valid_loss: 1.0744787193091847\n",
            "test_ind: 2, Epoch: 2631, train_loss: 1.0830318554174527, valid_loss: 1.0856011627066848\n",
            "test_ind: 2, Epoch: 2632, train_loss: 1.1190149933524283, valid_loss: 1.1410849467981237\n",
            "test_ind: 2, Epoch: 2633, train_loss: 1.1224420294802413, valid_loss: 1.0718714172004633\n",
            "test_ind: 2, Epoch: 2634, train_loss: 1.0505632095300685, valid_loss: 0.9179447641399849\n",
            "Validation loss decreased (0.9370170875831887 --> 0.9179447641399849).  Saving model ...\n",
            "test_ind: 2, Epoch: 2635, train_loss: 1.1442008595860582, valid_loss: 1.3945864469577103\n",
            "test_ind: 2, Epoch: 2636, train_loss: 1.10430614878083, valid_loss: 1.0514934232771567\n",
            "test_ind: 2, Epoch: 2637, train_loss: 1.0938021937440037, valid_loss: 1.0933092971812626\n",
            "test_ind: 2, Epoch: 2638, train_loss: 1.0756832294427885, valid_loss: 1.1092638195070446\n",
            "test_ind: 2, Epoch: 2639, train_loss: 1.0434350928916098, valid_loss: 1.0232280109003398\n",
            "test_ind: 2, Epoch: 2640, train_loss: 1.0215567177177496, valid_loss: 0.9452073383874704\n",
            "test_ind: 2, Epoch: 2641, train_loss: 1.0031571286356349, valid_loss: 1.0877302263537023\n",
            "test_ind: 2, Epoch: 2642, train_loss: 1.101052025444487, valid_loss: 1.063046279456201\n",
            "test_ind: 2, Epoch: 2643, train_loss: 1.0849005125413482, valid_loss: 1.2269752908636022\n",
            "test_ind: 2, Epoch: 2644, train_loss: 1.062878796857307, valid_loss: 0.9873949122904372\n",
            "test_ind: 2, Epoch: 2645, train_loss: 1.0599798071644813, valid_loss: 1.0671811395900541\n",
            "test_ind: 2, Epoch: 2646, train_loss: 1.0335839673211658, valid_loss: 0.949197952563946\n",
            "test_ind: 2, Epoch: 2647, train_loss: 1.1109005940718981, valid_loss: 1.1281567061388935\n",
            "test_ind: 2, Epoch: 2648, train_loss: 1.0606305193923704, valid_loss: 1.066214268024151\n",
            "test_ind: 2, Epoch: 2649, train_loss: 1.109019297819871, valid_loss: 1.3136924046736498\n",
            "test_ind: 2, Epoch: 2650, train_loss: 1.102260676997113, valid_loss: 1.0417639305788566\n",
            "test_ind: 2, Epoch: 2651, train_loss: 1.0699760082893337, valid_loss: 1.1254316014781638\n",
            "test_ind: 2, Epoch: 2652, train_loss: 1.1411224378366644, valid_loss: 1.0615728329389524\n",
            "test_ind: 2, Epoch: 2653, train_loss: 1.0463795784192207, valid_loss: 0.9897134806695487\n",
            "test_ind: 2, Epoch: 2654, train_loss: 1.0898062379045477, valid_loss: 1.1105459368127024\n",
            "test_ind: 2, Epoch: 2655, train_loss: 1.0551767611888514, valid_loss: 1.1244694731513998\n",
            "test_ind: 2, Epoch: 2656, train_loss: 1.074393545913334, valid_loss: 1.1467620878138094\n",
            "test_ind: 2, Epoch: 2657, train_loss: 1.1187415186382994, valid_loss: 0.9046805434756809\n",
            "Validation loss decreased (0.9179447641399849 --> 0.9046805434756809).  Saving model ...\n",
            "test_ind: 2, Epoch: 2658, train_loss: 1.0492042795545353, valid_loss: 1.047986550888105\n",
            "test_ind: 2, Epoch: 2659, train_loss: 1.0420984393171433, valid_loss: 1.005891797209737\n",
            "test_ind: 2, Epoch: 2660, train_loss: 1.1309851157812425, valid_loss: 1.3399462924044356\n",
            "test_ind: 2, Epoch: 2661, train_loss: 1.0225718077997423, valid_loss: 0.9983180085478345\n",
            "test_ind: 2, Epoch: 2662, train_loss: 1.1300571431235147, valid_loss: 1.0468566696188728\n",
            "test_ind: 2, Epoch: 2663, train_loss: 1.0694988047861531, valid_loss: 1.2480052842034235\n",
            "test_ind: 2, Epoch: 2664, train_loss: 1.0647995087043058, valid_loss: 1.0352252806693398\n",
            "test_ind: 2, Epoch: 2665, train_loss: 1.100099917037761, valid_loss: 1.1739240962895234\n",
            "test_ind: 2, Epoch: 2666, train_loss: 1.127877549681342, valid_loss: 1.087744894190731\n",
            "test_ind: 2, Epoch: 2667, train_loss: 1.047263633152019, valid_loss: 0.9915358843626799\n",
            "test_ind: 2, Epoch: 2668, train_loss: 1.0828980695374897, valid_loss: 0.9995397061024637\n",
            "test_ind: 2, Epoch: 2669, train_loss: 1.028890735403425, valid_loss: 1.0027198859429427\n",
            "test_ind: 2, Epoch: 2670, train_loss: 1.0613926036631847, valid_loss: 1.0744670680445485\n",
            "test_ind: 2, Epoch: 2671, train_loss: 1.0186515821237736, valid_loss: 1.0076985739574813\n",
            "test_ind: 2, Epoch: 2672, train_loss: 1.1049922506580554, valid_loss: 0.9690371626123064\n",
            "test_ind: 2, Epoch: 2673, train_loss: 1.0588115158244076, valid_loss: 1.1059343638243497\n",
            "test_ind: 2, Epoch: 2674, train_loss: 1.1833486955503227, valid_loss: 0.9995243841426666\n",
            "test_ind: 2, Epoch: 2675, train_loss: 1.047830844083969, valid_loss: 1.0368726294264834\n",
            "test_ind: 2, Epoch: 2676, train_loss: 1.067848301794228, valid_loss: 1.0987560755846508\n",
            "test_ind: 2, Epoch: 2677, train_loss: 1.0722926550554528, valid_loss: 1.1186705226572151\n",
            "test_ind: 2, Epoch: 2678, train_loss: 1.0484800019495166, valid_loss: 1.217071059762243\n",
            "test_ind: 2, Epoch: 2679, train_loss: 1.077959716603061, valid_loss: 1.1289905924403092\n",
            "test_ind: 2, Epoch: 2680, train_loss: 1.069572402993951, valid_loss: 1.1830294132232666\n",
            "test_ind: 2, Epoch: 2681, train_loss: 1.0720255857179648, valid_loss: 1.0312743431482558\n",
            "test_ind: 2, Epoch: 2682, train_loss: 1.0075039612261998, valid_loss: 1.0842526026940413\n",
            "test_ind: 2, Epoch: 2683, train_loss: 1.124579680271638, valid_loss: 1.2866069584490567\n",
            "test_ind: 2, Epoch: 2684, train_loss: 1.0690577967554078, valid_loss: 1.1202110035127384\n",
            "test_ind: 2, Epoch: 2685, train_loss: 1.1150477482722356, valid_loss: 1.1077580214226008\n",
            "test_ind: 2, Epoch: 2686, train_loss: 1.0133593528472233, valid_loss: 0.9874538974544601\n",
            "test_ind: 2, Epoch: 2687, train_loss: 1.0131469830262128, valid_loss: 1.1046518287767373\n",
            "test_ind: 2, Epoch: 2688, train_loss: 1.0766121453595863, valid_loss: 1.0468098349720665\n",
            "test_ind: 2, Epoch: 2689, train_loss: 1.08866047134653, valid_loss: 1.03562054471073\n",
            "test_ind: 2, Epoch: 2690, train_loss: 0.9964991034718771, valid_loss: 0.972270949953302\n",
            "test_ind: 2, Epoch: 2691, train_loss: 1.0080838749098529, valid_loss: 1.074774952016325\n",
            "test_ind: 2, Epoch: 2692, train_loss: 1.0931118417669226, valid_loss: 1.1532305054515175\n",
            "test_ind: 2, Epoch: 2693, train_loss: 1.0344830274808328, valid_loss: 0.9903635510012636\n",
            "test_ind: 2, Epoch: 2694, train_loss: 1.1413138311246633, valid_loss: 1.005156962620227\n",
            "test_ind: 2, Epoch: 2695, train_loss: 1.040458160021819, valid_loss: 0.9877189516681552\n",
            "test_ind: 2, Epoch: 2696, train_loss: 1.073911851853953, valid_loss: 1.1618550258484321\n",
            "test_ind: 2, Epoch: 2697, train_loss: 1.0392376393900524, valid_loss: 1.062989262094525\n",
            "test_ind: 2, Epoch: 2698, train_loss: 1.0625834075587202, valid_loss: 1.085485061009725\n",
            "test_ind: 2, Epoch: 2699, train_loss: 1.0173331504760192, valid_loss: 0.9922597591693585\n",
            "test_ind: 2, Epoch: 2700, train_loss: 1.0945370876551355, valid_loss: 1.0577626771736688\n",
            "test_ind: 2, Epoch: 2701, train_loss: 1.030724548999174, valid_loss: 0.9440388305914029\n",
            "test_ind: 2, Epoch: 2702, train_loss: 1.0864839920630824, valid_loss: 1.0890196782571298\n",
            "test_ind: 2, Epoch: 2703, train_loss: 1.0098263630953042, valid_loss: 0.9057260877386457\n",
            "test_ind: 2, Epoch: 2704, train_loss: 1.0396581669705094, valid_loss: 1.2548135189588931\n",
            "test_ind: 2, Epoch: 2705, train_loss: 1.0371516058814039, valid_loss: 1.000542183547278\n",
            "test_ind: 2, Epoch: 2706, train_loss: 1.1053078482520093, valid_loss: 1.0433931343915455\n",
            "test_ind: 2, Epoch: 2707, train_loss: 1.0580410878089985, valid_loss: 0.9781016311754189\n",
            "test_ind: 2, Epoch: 2708, train_loss: 1.0159926717890408, valid_loss: 1.128853994896609\n",
            "test_ind: 2, Epoch: 2709, train_loss: 1.1273701786428887, valid_loss: 1.4046431504763088\n",
            "test_ind: 2, Epoch: 2710, train_loss: 1.059570566088612, valid_loss: 1.1682973743503928\n",
            "test_ind: 2, Epoch: 2711, train_loss: 1.0368850007016435, valid_loss: 0.9973605935729806\n",
            "test_ind: 2, Epoch: 2712, train_loss: 1.1176385252337622, valid_loss: 1.2503177053228742\n",
            "test_ind: 2, Epoch: 2713, train_loss: 1.0677420671848827, valid_loss: 1.0891821567828839\n",
            "test_ind: 2, Epoch: 2714, train_loss: 1.0096602849697682, valid_loss: 0.9841387686226781\n",
            "test_ind: 2, Epoch: 2715, train_loss: 1.0519910456448198, valid_loss: 1.0072936930208125\n",
            "test_ind: 2, Epoch: 2716, train_loss: 1.1030013278225776, valid_loss: 1.1703395191420856\n",
            "test_ind: 2, Epoch: 2717, train_loss: 1.0421412516410082, valid_loss: 0.9937632783525691\n",
            "test_ind: 2, Epoch: 2718, train_loss: 1.079724323375952, valid_loss: 1.0135725916620673\n",
            "test_ind: 2, Epoch: 2719, train_loss: 1.0394982632748422, valid_loss: 1.1982831764764594\n",
            "test_ind: 2, Epoch: 2720, train_loss: 1.1297154345064082, valid_loss: 1.0293626690182591\n",
            "test_ind: 2, Epoch: 2721, train_loss: 1.0709959471214419, valid_loss: 1.0180726723793225\n",
            "test_ind: 2, Epoch: 2722, train_loss: 1.0070166365939555, valid_loss: 1.0957400921063547\n",
            "test_ind: 2, Epoch: 2723, train_loss: 1.0299027389497384, valid_loss: 1.0060867210399054\n",
            "test_ind: 2, Epoch: 2724, train_loss: 1.0111920713585894, valid_loss: 1.114715074202274\n",
            "test_ind: 2, Epoch: 2725, train_loss: 1.025336080580129, valid_loss: 1.040331638776339\n",
            "test_ind: 2, Epoch: 2726, train_loss: 0.9975320403052872, valid_loss: 1.1475815290739053\n",
            "test_ind: 2, Epoch: 2727, train_loss: 1.0733495297481757, valid_loss: 1.1209503646589751\n",
            "test_ind: 2, Epoch: 2728, train_loss: 1.0358764525265887, valid_loss: 1.0072096053012078\n",
            "test_ind: 2, Epoch: 2729, train_loss: 1.0557766196168499, valid_loss: 1.0590404601518246\n",
            "test_ind: 2, Epoch: 2730, train_loss: 1.0971887390158455, valid_loss: 1.021596908569336\n",
            "test_ind: 2, Epoch: 2731, train_loss: 1.0199413107104904, valid_loss: 1.0212361282772489\n",
            "test_ind: 2, Epoch: 2732, train_loss: 1.0577113039699602, valid_loss: 1.0886204466860518\n",
            "test_ind: 2, Epoch: 2733, train_loss: 0.9780587332653976, valid_loss: 1.0668487311088801\n",
            "test_ind: 2, Epoch: 2734, train_loss: 1.0962302419874403, valid_loss: 1.0398164304912598\n",
            "test_ind: 2, Epoch: 2735, train_loss: 1.0664534931056067, valid_loss: 0.9720507143569467\n",
            "test_ind: 2, Epoch: 2736, train_loss: 1.0954792298029858, valid_loss: 1.0995724520452343\n",
            "test_ind: 2, Epoch: 2737, train_loss: 1.0483082520429678, valid_loss: 1.1849344170670904\n",
            "test_ind: 2, Epoch: 2738, train_loss: 1.0939195276099167, valid_loss: 0.9135172292377874\n",
            "test_ind: 2, Epoch: 2739, train_loss: 1.0103952920901016, valid_loss: 1.0323414843306582\n",
            "test_ind: 2, Epoch: 2740, train_loss: 1.0210419457409345, valid_loss: 1.000307929481876\n",
            "test_ind: 2, Epoch: 2741, train_loss: 0.9843923592725937, valid_loss: 1.1709253951015637\n",
            "test_ind: 2, Epoch: 2742, train_loss: 1.1271789252814632, valid_loss: 0.9951970957622909\n",
            "test_ind: 2, Epoch: 2743, train_loss: 1.0870773667736724, valid_loss: 1.0918596673894811\n",
            "test_ind: 2, Epoch: 2744, train_loss: 1.0470847981607811, valid_loss: 1.1516343738958028\n",
            "test_ind: 2, Epoch: 2745, train_loss: 1.0040013158423269, valid_loss: 1.0544393442974471\n",
            "test_ind: 2, Epoch: 2746, train_loss: 1.0155235634230482, valid_loss: 0.9967104444476615\n",
            "test_ind: 2, Epoch: 2747, train_loss: 1.0496196805694957, valid_loss: 1.0648640777990008\n",
            "test_ind: 2, Epoch: 2748, train_loss: 1.059782417411478, valid_loss: 1.0518028511960282\n",
            "test_ind: 2, Epoch: 2749, train_loss: 1.0358923552495916, valid_loss: 1.0098621886000672\n",
            "test_ind: 2, Epoch: 2750, train_loss: 1.0361288531213744, valid_loss: 1.0129865873233546\n",
            "test_ind: 2, Epoch: 2751, train_loss: 1.1000063924255081, valid_loss: 1.0287761892008986\n",
            "test_ind: 2, Epoch: 2752, train_loss: 1.0369326096993905, valid_loss: 1.021407242853757\n",
            "test_ind: 2, Epoch: 2753, train_loss: 1.1052783588398556, valid_loss: 1.2257099824073987\n",
            "test_ind: 2, Epoch: 2754, train_loss: 1.0618102695414369, valid_loss: 1.0029841087482594\n",
            "test_ind: 2, Epoch: 2755, train_loss: 1.057839379124945, valid_loss: 1.020997200256739\n",
            "test_ind: 2, Epoch: 2756, train_loss: 1.0419365755173555, valid_loss: 1.0534466433728862\n",
            "test_ind: 2, Epoch: 2757, train_loss: 1.0705075202844083, valid_loss: 1.0383763245367936\n",
            "test_ind: 2, Epoch: 2758, train_loss: 1.0617479406304284, valid_loss: 1.000471492778202\n",
            "test_ind: 2, Epoch: 2759, train_loss: 1.03026200948391, valid_loss: 1.0733800926099815\n",
            "test_ind: 2, Epoch: 2760, train_loss: 1.032058178303022, valid_loss: 1.053875014992521\n",
            "test_ind: 2, Epoch: 2761, train_loss: 1.0167963824041208, valid_loss: 1.0332688891310298\n",
            "test_ind: 2, Epoch: 2762, train_loss: 1.0727699398428399, valid_loss: 0.9760024119646122\n",
            "test_ind: 2, Epoch: 2763, train_loss: 1.0011055874801882, valid_loss: 1.014094169323261\n",
            "test_ind: 2, Epoch: 2764, train_loss: 1.043072363816322, valid_loss: 0.953993588091641\n",
            "test_ind: 2, Epoch: 2765, train_loss: 1.0324592293723924, valid_loss: 1.0572792845234233\n",
            "test_ind: 2, Epoch: 2766, train_loss: 1.0733713392744944, valid_loss: 1.1285199806561157\n",
            "test_ind: 2, Epoch: 2767, train_loss: 1.0958252720683388, valid_loss: 1.4366805220601226\n",
            "test_ind: 2, Epoch: 2768, train_loss: 1.137234891581739, valid_loss: 1.0992126233896977\n",
            "test_ind: 2, Epoch: 2769, train_loss: 1.0153793353527247, valid_loss: 0.9743133976928189\n",
            "test_ind: 2, Epoch: 2770, train_loss: 1.0921319033685233, valid_loss: 1.069183288476406\n",
            "test_ind: 2, Epoch: 2771, train_loss: 1.0858895409594007, valid_loss: 1.0843816190703301\n",
            "test_ind: 2, Epoch: 2772, train_loss: 0.9795932466374728, valid_loss: 1.0830089101764213\n",
            "test_ind: 2, Epoch: 2773, train_loss: 1.013752231344312, valid_loss: 1.090386045624388\n",
            "test_ind: 2, Epoch: 2774, train_loss: 1.0954711550434544, valid_loss: 1.0286353524254257\n",
            "test_ind: 2, Epoch: 2775, train_loss: 1.0438296808476462, valid_loss: 1.03698859119687\n",
            "test_ind: 2, Epoch: 2776, train_loss: 1.1091808233505642, valid_loss: 1.1829206787283266\n",
            "test_ind: 2, Epoch: 2777, train_loss: 1.0142281487909592, valid_loss: 0.9384272268355063\n",
            "test_ind: 2, Epoch: 2778, train_loss: 1.0311828678489752, valid_loss: 1.0204255621657414\n",
            "test_ind: 2, Epoch: 2779, train_loss: 1.0602527454934116, valid_loss: 1.0524868082117151\n",
            "test_ind: 2, Epoch: 2780, train_loss: 1.0364576402665995, valid_loss: 1.0355105277819512\n",
            "test_ind: 2, Epoch: 2781, train_loss: 0.9875289304756826, valid_loss: 0.996528328653754\n",
            "test_ind: 2, Epoch: 2782, train_loss: 0.96883207712418, valid_loss: 0.9699342617621789\n",
            "test_ind: 2, Epoch: 2783, train_loss: 0.9863758442635097, valid_loss: 1.1930895789056761\n",
            "test_ind: 2, Epoch: 2784, train_loss: 1.0494032295567584, valid_loss: 0.9292314568815747\n",
            "test_ind: 2, Epoch: 2785, train_loss: 1.0370074667935132, valid_loss: 0.9684601475370576\n",
            "test_ind: 2, Epoch: 2786, train_loss: 1.060089885905484, valid_loss: 1.1148881620151705\n",
            "test_ind: 2, Epoch: 2787, train_loss: 1.062501865914065, valid_loss: 0.9964442490852117\n",
            "test_ind: 2, Epoch: 2788, train_loss: 1.002071622656508, valid_loss: 1.2467811454055655\n",
            "test_ind: 2, Epoch: 2789, train_loss: 1.048460368530476, valid_loss: 1.2730920172145224\n",
            "test_ind: 2, Epoch: 2790, train_loss: 1.0822585688696966, valid_loss: 1.2078330183980133\n",
            "test_ind: 2, Epoch: 2791, train_loss: 0.9739790664665605, valid_loss: 0.9998763646834936\n",
            "test_ind: 2, Epoch: 2792, train_loss: 1.0486488190584826, valid_loss: 0.9361905168603968\n",
            "test_ind: 2, Epoch: 2793, train_loss: 1.0670587316877143, valid_loss: 1.238160411856453\n",
            "test_ind: 2, Epoch: 2794, train_loss: 1.1445792873813672, valid_loss: 1.3050647006075606\n",
            "test_ind: 2, Epoch: 2795, train_loss: 1.1022297468846567, valid_loss: 0.9625007929625334\n",
            "test_ind: 2, Epoch: 2796, train_loss: 0.9796175531059476, valid_loss: 1.2223670265273832\n",
            "test_ind: 2, Epoch: 2797, train_loss: 1.126158629387532, valid_loss: 1.1844327137341186\n",
            "test_ind: 2, Epoch: 2798, train_loss: 1.0285428247334045, valid_loss: 0.9586273713668867\n",
            "test_ind: 2, Epoch: 2799, train_loss: 1.1158790991403666, valid_loss: 1.0970345032520785\n",
            "test_ind: 2, Epoch: 2800, train_loss: 1.0584673892851684, valid_loss: 1.0942660919961087\n",
            "test_ind: 2, Epoch: 2801, train_loss: 1.065154709594089, valid_loss: 1.1865583226891325\n",
            "test_ind: 2, Epoch: 2802, train_loss: 1.0713455163062693, valid_loss: 0.9645569378833826\n",
            "test_ind: 2, Epoch: 2803, train_loss: 1.0289582166916285, valid_loss: 0.9702940720778246\n",
            "test_ind: 2, Epoch: 2804, train_loss: 1.012857677590134, valid_loss: 1.0305968030565484\n",
            "test_ind: 2, Epoch: 2805, train_loss: 1.0469560931097974, valid_loss: 1.2173537100821819\n",
            "test_ind: 2, Epoch: 2806, train_loss: 0.9685760228156137, valid_loss: 0.9096341520293146\n",
            "test_ind: 2, Epoch: 2807, train_loss: 1.0373460886485217, valid_loss: 0.9870691768124573\n",
            "test_ind: 2, Epoch: 2808, train_loss: 1.0446086362329756, valid_loss: 1.052051321393744\n",
            "test_ind: 2, Epoch: 2809, train_loss: 1.0424672255375673, valid_loss: 1.1034275928453843\n",
            "test_ind: 2, Epoch: 2810, train_loss: 1.0873102774986854, valid_loss: 0.9759726035289276\n",
            "test_ind: 2, Epoch: 2811, train_loss: 1.00372993300783, valid_loss: 0.9696581411225843\n",
            "test_ind: 2, Epoch: 2812, train_loss: 0.9713900084735547, valid_loss: 0.9572724005435606\n",
            "test_ind: 2, Epoch: 2813, train_loss: 0.9909958277898863, valid_loss: 1.1809297823838019\n",
            "test_ind: 2, Epoch: 2814, train_loss: 1.0358410329900236, valid_loss: 0.945179866589712\n",
            "test_ind: 2, Epoch: 2815, train_loss: 1.0458921263134153, valid_loss: 1.1260113199891528\n",
            "test_ind: 2, Epoch: 2816, train_loss: 1.103252886593738, valid_loss: 1.247851243385902\n",
            "test_ind: 2, Epoch: 2817, train_loss: 1.1749925432268598, valid_loss: 1.1490696835042404\n",
            "test_ind: 2, Epoch: 2818, train_loss: 0.9840530368790442, valid_loss: 0.9707307964987903\n",
            "test_ind: 2, Epoch: 2819, train_loss: 0.9802590777278513, valid_loss: 1.0700833233673008\n",
            "test_ind: 2, Epoch: 2820, train_loss: 1.0019047276133712, valid_loss: 1.007950941042343\n",
            "test_ind: 2, Epoch: 2821, train_loss: 1.0597145285701481, valid_loss: 0.9365982954997961\n",
            "test_ind: 2, Epoch: 2822, train_loss: 1.0176152766373536, valid_loss: 0.9449826461976749\n",
            "test_ind: 2, Epoch: 2823, train_loss: 1.025800061248533, valid_loss: 0.9680930562848039\n",
            "test_ind: 2, Epoch: 2824, train_loss: 0.9855751651644366, valid_loss: 1.0659026658093487\n",
            "test_ind: 2, Epoch: 2825, train_loss: 1.0038337091661473, valid_loss: 0.9298880630069309\n",
            "test_ind: 2, Epoch: 2826, train_loss: 0.9963693519150316, valid_loss: 1.0347360987269303\n",
            "test_ind: 2, Epoch: 2827, train_loss: 1.0103206140929366, valid_loss: 0.9799806249786986\n",
            "test_ind: 2, Epoch: 2828, train_loss: 1.1254253842552162, valid_loss: 1.0310498961695918\n",
            "test_ind: 2, Epoch: 2829, train_loss: 0.9668904865569199, valid_loss: 0.8720899178431585\n",
            "Validation loss decreased (0.9046805434756809 --> 0.8720899178431585).  Saving model ...\n",
            "test_ind: 2, Epoch: 2830, train_loss: 0.9871835681447955, valid_loss: 1.025526375512452\n",
            "test_ind: 2, Epoch: 2831, train_loss: 1.002526752176674, valid_loss: 0.9714856881361742\n",
            "test_ind: 2, Epoch: 2832, train_loss: 1.0262252863316115, valid_loss: 1.136153610343607\n",
            "test_ind: 2, Epoch: 2833, train_loss: 0.9621966650456558, valid_loss: 1.0696581761721533\n",
            "test_ind: 2, Epoch: 2834, train_loss: 1.0149925339255916, valid_loss: 1.024359764876189\n",
            "test_ind: 2, Epoch: 2835, train_loss: 1.0000327770073756, valid_loss: 0.9966830636701013\n",
            "test_ind: 2, Epoch: 2836, train_loss: 0.9556700479157856, valid_loss: 0.8970800736690858\n",
            "test_ind: 2, Epoch: 2837, train_loss: 0.9563430295257713, valid_loss: 0.9386494866123906\n",
            "test_ind: 2, Epoch: 2838, train_loss: 1.0022908081243067, valid_loss: 0.9467717949141804\n",
            "test_ind: 2, Epoch: 2839, train_loss: 1.040156058776073, valid_loss: 1.1074431071593889\n",
            "test_ind: 2, Epoch: 2840, train_loss: 0.9835027480057503, valid_loss: 0.9452328450999029\n",
            "test_ind: 2, Epoch: 2841, train_loss: 1.0307852182632835, valid_loss: 0.9483509641087632\n",
            "test_ind: 2, Epoch: 2842, train_loss: 0.9838918065979497, valid_loss: 0.9234386692699205\n",
            "test_ind: 2, Epoch: 2843, train_loss: 1.086087635100058, valid_loss: 0.9796457453670664\n",
            "test_ind: 2, Epoch: 2844, train_loss: 0.984115559377788, valid_loss: 0.9852837789432276\n",
            "test_ind: 2, Epoch: 2845, train_loss: 1.1695881609903103, valid_loss: 1.2462889611551224\n",
            "test_ind: 2, Epoch: 2846, train_loss: 1.036771982370505, valid_loss: 1.023915748650532\n",
            "test_ind: 2, Epoch: 2847, train_loss: 0.9818902115310133, valid_loss: 0.8753465016682942\n",
            "test_ind: 2, Epoch: 2848, train_loss: 1.0577519189032167, valid_loss: 1.023372118289654\n",
            "test_ind: 2, Epoch: 2849, train_loss: 0.9797480830892652, valid_loss: 0.9410188897722467\n",
            "test_ind: 2, Epoch: 2850, train_loss: 1.0324434763119545, valid_loss: 1.0469713965032854\n",
            "test_ind: 2, Epoch: 2851, train_loss: 1.0408212151396081, valid_loss: 1.2378420795810188\n",
            "test_ind: 2, Epoch: 2852, train_loss: 1.036333224712274, valid_loss: 1.086309367095643\n",
            "test_ind: 2, Epoch: 2853, train_loss: 1.1245666369866554, valid_loss: 1.0983452993920046\n",
            "test_ind: 2, Epoch: 2854, train_loss: 1.0079487162551084, valid_loss: 0.9969723265395205\n",
            "test_ind: 2, Epoch: 2855, train_loss: 0.9940051235477922, valid_loss: 1.0493214585502604\n",
            "test_ind: 2, Epoch: 2856, train_loss: 1.0415566780401884, valid_loss: 0.9628476328999229\n",
            "test_ind: 2, Epoch: 2857, train_loss: 1.0243241931864566, valid_loss: 0.9282548577017933\n",
            "test_ind: 2, Epoch: 2858, train_loss: 0.9725374210933675, valid_loss: 1.0350323524909821\n",
            "test_ind: 2, Epoch: 2859, train_loss: 1.0116362870588602, valid_loss: 1.1747592330997825\n",
            "test_ind: 2, Epoch: 2860, train_loss: 1.0113893343172864, valid_loss: 0.9723753141202138\n",
            "test_ind: 2, Epoch: 2861, train_loss: 0.9595232992543568, valid_loss: 0.928395066845451\n",
            "test_ind: 2, Epoch: 2862, train_loss: 1.0156165324045383, valid_loss: 1.1207524677287481\n",
            "test_ind: 2, Epoch: 2863, train_loss: 1.0033635715020914, valid_loss: 1.123374557223415\n",
            "test_ind: 2, Epoch: 2864, train_loss: 0.9917420405834376, valid_loss: 1.0125192359641746\n",
            "test_ind: 2, Epoch: 2865, train_loss: 0.9788852007413974, valid_loss: 1.054234093750304\n",
            "test_ind: 2, Epoch: 2866, train_loss: 0.9963488415775137, valid_loss: 0.957047984131381\n",
            "test_ind: 2, Epoch: 2867, train_loss: 0.9884205455453985, valid_loss: 1.017910385403538\n",
            "test_ind: 2, Epoch: 2868, train_loss: 0.9864938741395954, valid_loss: 0.8841698027064657\n",
            "test_ind: 2, Epoch: 2869, train_loss: 0.9887779669204668, valid_loss: 1.043952043919142\n",
            "test_ind: 2, Epoch: 2870, train_loss: 1.0394200420560773, valid_loss: 1.0811807715315425\n",
            "test_ind: 2, Epoch: 2871, train_loss: 0.9996677475211061, valid_loss: 0.9796787958878737\n",
            "test_ind: 2, Epoch: 2872, train_loss: 1.1173891278526835, valid_loss: 1.0577487864046016\n",
            "test_ind: 2, Epoch: 2873, train_loss: 0.9538321617322091, valid_loss: 0.9217148778105733\n",
            "test_ind: 2, Epoch: 2874, train_loss: 1.024736418230468, valid_loss: 0.9503280416852729\n",
            "test_ind: 2, Epoch: 2875, train_loss: 0.9984281024588706, valid_loss: 0.9407231834878949\n",
            "test_ind: 2, Epoch: 2876, train_loss: 1.0024614961285423, valid_loss: 1.051089775188696\n",
            "test_ind: 2, Epoch: 2877, train_loss: 1.0497157105013855, valid_loss: 1.0397377781718544\n",
            "test_ind: 2, Epoch: 2878, train_loss: 1.001661556059139, valid_loss: 0.8857856347010686\n",
            "test_ind: 2, Epoch: 2879, train_loss: 1.0532482216047994, valid_loss: 0.9538595846235922\n",
            "test_ind: 2, Epoch: 2880, train_loss: 1.0017972209854342, valid_loss: 0.9438495466172525\n",
            "test_ind: 2, Epoch: 2881, train_loss: 0.9622691916151491, valid_loss: 0.9417185851311751\n",
            "test_ind: 2, Epoch: 2882, train_loss: 1.0019695061450897, valid_loss: 1.0248143638980354\n",
            "test_ind: 2, Epoch: 2883, train_loss: 1.003385399141882, valid_loss: 1.017221794508801\n",
            "test_ind: 2, Epoch: 2884, train_loss: 0.9650491488511973, valid_loss: 0.9843379371186607\n",
            "test_ind: 2, Epoch: 2885, train_loss: 0.9848313109713969, valid_loss: 1.0227695740865506\n",
            "test_ind: 2, Epoch: 2886, train_loss: 0.9576758652331143, valid_loss: 0.8899815884071198\n",
            "test_ind: 2, Epoch: 2887, train_loss: 1.071860221263237, valid_loss: 0.9752366943576737\n",
            "test_ind: 2, Epoch: 2888, train_loss: 1.022485912129184, valid_loss: 1.049551560328557\n",
            "test_ind: 2, Epoch: 2889, train_loss: 1.0585182743307986, valid_loss: 1.0585327956751203\n",
            "test_ind: 2, Epoch: 2890, train_loss: 1.0853011449630896, valid_loss: 1.2594288292094173\n",
            "test_ind: 2, Epoch: 2891, train_loss: 0.9773903341374846, valid_loss: 1.1530748833278646\n",
            "test_ind: 2, Epoch: 2892, train_loss: 0.9515459847699317, valid_loss: 0.9477327379406009\n",
            "test_ind: 2, Epoch: 2893, train_loss: 1.0005311839148077, valid_loss: 0.9899885301236753\n",
            "test_ind: 2, Epoch: 2894, train_loss: 1.1599779090537192, valid_loss: 1.2820369552003692\n",
            "test_ind: 2, Epoch: 2895, train_loss: 0.9925526544692284, valid_loss: 1.0444604875015737\n",
            "test_ind: 2, Epoch: 2896, train_loss: 1.0394324952946996, valid_loss: 0.9802620974701015\n",
            "test_ind: 2, Epoch: 2897, train_loss: 0.9863511893371117, valid_loss: 1.0054066099672236\n",
            "test_ind: 2, Epoch: 2898, train_loss: 0.9929811716758967, valid_loss: 0.9408569485373646\n",
            "test_ind: 2, Epoch: 2899, train_loss: 1.0485631631650139, valid_loss: 1.0234223627976202\n",
            "test_ind: 2, Epoch: 2900, train_loss: 1.0375440854292648, valid_loss: 1.3013459413479538\n",
            "test_ind: 2, Epoch: 2901, train_loss: 0.9765925092688088, valid_loss: 1.0447590262801558\n",
            "test_ind: 2, Epoch: 2902, train_loss: 1.0354930971875602, valid_loss: 0.9843331721433547\n",
            "test_ind: 2, Epoch: 2903, train_loss: 1.052735617357781, valid_loss: 0.9428876329351354\n",
            "test_ind: 2, Epoch: 2904, train_loss: 0.9965999599648788, valid_loss: 0.9725061563345102\n",
            "test_ind: 2, Epoch: 2905, train_loss: 0.9843890800548527, valid_loss: 1.112266666868813\n",
            "test_ind: 2, Epoch: 2906, train_loss: 1.0954761645506366, valid_loss: 1.19891876984186\n",
            "test_ind: 2, Epoch: 2907, train_loss: 1.0166121226543487, valid_loss: 0.9699933406634208\n",
            "test_ind: 2, Epoch: 2908, train_loss: 1.0209982850273112, valid_loss: 0.9990009004913505\n",
            "test_ind: 2, Epoch: 2909, train_loss: 0.9692367678693896, valid_loss: 1.130117532534477\n",
            "test_ind: 2, Epoch: 2910, train_loss: 1.0036554970519382, valid_loss: 0.9883728652258543\n",
            "test_ind: 2, Epoch: 2911, train_loss: 1.0069281337154783, valid_loss: 1.0793189044691558\n",
            "test_ind: 2, Epoch: 2912, train_loss: 0.9818457572208851, valid_loss: 1.011595958997721\n",
            "test_ind: 2, Epoch: 2913, train_loss: 0.9837807463331666, valid_loss: 0.9486884602114685\n",
            "test_ind: 2, Epoch: 2914, train_loss: 1.0104825618939521, valid_loss: 0.9277390489551078\n",
            "test_ind: 2, Epoch: 2915, train_loss: 1.104458164059312, valid_loss: 1.0829936269341711\n",
            "test_ind: 2, Epoch: 2916, train_loss: 1.0186315320948702, valid_loss: 0.965648052699206\n",
            "test_ind: 2, Epoch: 2917, train_loss: 0.9829390893521359, valid_loss: 1.0871454215796925\n",
            "test_ind: 2, Epoch: 2918, train_loss: 0.9981421167694265, valid_loss: 0.919126001857964\n",
            "test_ind: 2, Epoch: 2919, train_loss: 1.0024542917213548, valid_loss: 0.9138881304325202\n",
            "test_ind: 2, Epoch: 2920, train_loss: 0.9607919752314785, valid_loss: 1.0069645510779486\n",
            "test_ind: 2, Epoch: 2921, train_loss: 0.9852583734398215, valid_loss: 1.0346619396807462\n",
            "test_ind: 2, Epoch: 2922, train_loss: 0.964150600623541, valid_loss: 0.9675102525966459\n",
            "test_ind: 2, Epoch: 2923, train_loss: 0.9926500671383095, valid_loss: 0.9311955932878021\n",
            "test_ind: 2, Epoch: 2924, train_loss: 0.9827233218965594, valid_loss: 0.9342893616765992\n",
            "test_ind: 2, Epoch: 2925, train_loss: 1.0308790388043905, valid_loss: 1.0734148310799885\n",
            "test_ind: 2, Epoch: 2926, train_loss: 1.0049418008338806, valid_loss: 1.144093878248818\n",
            "test_ind: 2, Epoch: 2927, train_loss: 1.0201953666502255, valid_loss: 0.9454972417945535\n",
            "test_ind: 2, Epoch: 2928, train_loss: 0.9963386142582182, valid_loss: 1.0197701528880672\n",
            "test_ind: 2, Epoch: 2929, train_loss: 1.038408517158269, valid_loss: 1.071949136902464\n",
            "test_ind: 2, Epoch: 2930, train_loss: 0.9691869519714615, valid_loss: 1.065657329015922\n",
            "test_ind: 2, Epoch: 2931, train_loss: 1.0100302266033059, valid_loss: 1.0063470646187112\n",
            "test_ind: 2, Epoch: 2932, train_loss: 1.0196516896471564, valid_loss: 0.923154698477851\n",
            "test_ind: 2, Epoch: 2933, train_loss: 0.9896044550005414, valid_loss: 1.0840536807677006\n",
            "test_ind: 2, Epoch: 2934, train_loss: 0.9634858810211067, valid_loss: 1.1178680797587772\n",
            "test_ind: 2, Epoch: 2935, train_loss: 0.9916731385197509, valid_loss: 0.9792029130832423\n",
            "test_ind: 2, Epoch: 2936, train_loss: 0.9820382076564111, valid_loss: 0.9020516070884856\n",
            "test_ind: 2, Epoch: 2937, train_loss: 1.041513706091349, valid_loss: 1.1624822324497408\n",
            "test_ind: 2, Epoch: 2938, train_loss: 1.0223193832046062, valid_loss: 1.0255958497354447\n",
            "test_ind: 2, Epoch: 2939, train_loss: 1.0230110690125034, valid_loss: 1.1194232039981418\n",
            "test_ind: 2, Epoch: 2940, train_loss: 1.0071177222223817, valid_loss: 0.9033002228478759\n",
            "test_ind: 2, Epoch: 2941, train_loss: 1.0064783370279744, valid_loss: 1.1501979855051068\n",
            "test_ind: 2, Epoch: 2942, train_loss: 1.003601960873219, valid_loss: 0.8964198267357981\n",
            "test_ind: 2, Epoch: 2943, train_loss: 0.9742616090113394, valid_loss: 0.9556898697149379\n",
            "test_ind: 2, Epoch: 2944, train_loss: 0.9744573282493599, valid_loss: 0.875102681651754\n",
            "test_ind: 2, Epoch: 2945, train_loss: 0.9457080194413492, valid_loss: 1.1777855168041\n",
            "test_ind: 2, Epoch: 2946, train_loss: 1.0181218413998712, valid_loss: 1.0813282521022352\n",
            "test_ind: 2, Epoch: 2947, train_loss: 1.0385345737931848, valid_loss: 1.0769842418170723\n",
            "test_ind: 2, Epoch: 2948, train_loss: 0.9476000750959203, valid_loss: 0.938254297289074\n",
            "test_ind: 2, Epoch: 2949, train_loss: 0.9520434065308893, valid_loss: 0.8956613160266496\n",
            "test_ind: 2, Epoch: 2950, train_loss: 0.9923138278841632, valid_loss: 0.9696446225853727\n",
            "test_ind: 2, Epoch: 2951, train_loss: 0.987767381659034, valid_loss: 0.9736357801660174\n",
            "test_ind: 2, Epoch: 2952, train_loss: 0.9530462532641203, valid_loss: 0.8934519501493187\n",
            "test_ind: 2, Epoch: 2953, train_loss: 0.9495882639291733, valid_loss: 1.0365562146885103\n",
            "test_ind: 2, Epoch: 2954, train_loss: 0.9777267531678434, valid_loss: 1.2692142490647798\n",
            "test_ind: 2, Epoch: 2955, train_loss: 1.007530135646505, valid_loss: 1.0772849419857362\n",
            "test_ind: 2, Epoch: 2956, train_loss: 0.930926637205756, valid_loss: 0.9900673726345399\n",
            "test_ind: 2, Epoch: 2957, train_loss: 0.9944868613059252, valid_loss: 1.0851718523563483\n",
            "test_ind: 2, Epoch: 2958, train_loss: 1.0146859541238205, valid_loss: 0.9846104611019124\n",
            "test_ind: 2, Epoch: 2959, train_loss: 0.9789658188253838, valid_loss: 0.9746705426110163\n",
            "test_ind: 2, Epoch: 2960, train_loss: 0.9593640732289719, valid_loss: 0.8775541245767533\n",
            "test_ind: 2, Epoch: 2961, train_loss: 0.9786580328927761, valid_loss: 0.9278398809949218\n",
            "test_ind: 2, Epoch: 2962, train_loss: 1.045577116501637, valid_loss: 1.1104571649491617\n",
            "test_ind: 2, Epoch: 2963, train_loss: 0.9903374325063039, valid_loss: 1.3078982014941354\n",
            "test_ind: 2, Epoch: 2964, train_loss: 1.0266775163377, valid_loss: 0.9796097910302317\n",
            "test_ind: 2, Epoch: 2965, train_loss: 0.9939606527091204, valid_loss: 0.9664598319605204\n",
            "test_ind: 2, Epoch: 2966, train_loss: 1.0189686865322045, valid_loss: 1.0197928868807278\n",
            "test_ind: 2, Epoch: 2967, train_loss: 0.9907804955104818, valid_loss: 0.9453230610600224\n",
            "test_ind: 2, Epoch: 2968, train_loss: 1.000288745729785, valid_loss: 1.0274666150410972\n",
            "test_ind: 2, Epoch: 2969, train_loss: 1.000997558504994, valid_loss: 0.9411135270045354\n",
            "test_ind: 2, Epoch: 2970, train_loss: 1.0721532164136227, valid_loss: 1.2716733030444196\n",
            "test_ind: 2, Epoch: 2971, train_loss: 0.9417178266295229, valid_loss: 0.9983331491464903\n",
            "test_ind: 2, Epoch: 2972, train_loss: 0.9243235040141193, valid_loss: 1.0641996425780815\n",
            "test_ind: 2, Epoch: 2973, train_loss: 1.0924791248209231, valid_loss: 0.9710993481497479\n",
            "test_ind: 2, Epoch: 2974, train_loss: 1.0107046407625322, valid_loss: 1.0754172448758725\n",
            "test_ind: 2, Epoch: 2975, train_loss: 1.0070573289623062, valid_loss: 1.0876946829662704\n",
            "test_ind: 2, Epoch: 2976, train_loss: 1.0270541219176956, valid_loss: 1.2217020961294147\n",
            "test_ind: 2, Epoch: 2977, train_loss: 1.0150281667482932, valid_loss: 1.0251060639351521\n",
            "test_ind: 2, Epoch: 2978, train_loss: 1.0284638051633481, valid_loss: 0.9503209665629937\n",
            "test_ind: 2, Epoch: 2979, train_loss: 0.9845884034210233, valid_loss: 0.9603231177370772\n",
            "test_ind: 2, Epoch: 2980, train_loss: 0.9331076344420314, valid_loss: 0.9350012686857132\n",
            "test_ind: 2, Epoch: 2981, train_loss: 0.9592475872999809, valid_loss: 0.8807713958272907\n",
            "test_ind: 2, Epoch: 2982, train_loss: 1.0491192533306475, valid_loss: 0.9979699357622369\n",
            "test_ind: 2, Epoch: 2983, train_loss: 0.9680340337617444, valid_loss: 0.9293790067362988\n",
            "test_ind: 2, Epoch: 2984, train_loss: 1.1046034046727369, valid_loss: 1.184598481213605\n",
            "test_ind: 2, Epoch: 2985, train_loss: 1.002641188566275, valid_loss: 0.94678685128519\n",
            "test_ind: 2, Epoch: 2986, train_loss: 1.106189744085328, valid_loss: 1.2593407393181086\n",
            "test_ind: 2, Epoch: 2987, train_loss: 1.0409855007106423, valid_loss: 0.9776087921229523\n",
            "test_ind: 2, Epoch: 2988, train_loss: 0.9770619611115198, valid_loss: 0.9017880390851926\n",
            "test_ind: 2, Epoch: 2989, train_loss: 0.9707827151444336, valid_loss: 0.9958123705665609\n",
            "test_ind: 2, Epoch: 2990, train_loss: 0.960640170522112, valid_loss: 0.8771648264338828\n",
            "test_ind: 2, Epoch: 2991, train_loss: 0.9773571649734338, valid_loss: 1.0308160381099776\n",
            "test_ind: 2, Epoch: 2992, train_loss: 0.9531133546222423, valid_loss: 0.9895128626429456\n",
            "test_ind: 2, Epoch: 2993, train_loss: 1.0318332599212416, valid_loss: 1.1246515563410573\n",
            "test_ind: 2, Epoch: 2994, train_loss: 0.9476518859664033, valid_loss: 1.0314375009292212\n",
            "test_ind: 2, Epoch: 2995, train_loss: 1.015396122239594, valid_loss: 0.9760368223543521\n",
            "test_ind: 2, Epoch: 2996, train_loss: 0.9328369670896902, valid_loss: 0.8811686154444334\n",
            "test_ind: 2, Epoch: 2997, train_loss: 0.937815495479254, valid_loss: 1.078253263761515\n",
            "test_ind: 2, Epoch: 2998, train_loss: 0.9991913539165558, valid_loss: 1.2549800981483568\n",
            "test_ind: 2, Epoch: 2999, train_loss: 1.0146881481181522, valid_loss: 1.091238797560037\n",
            "test_ind: 2, Epoch: 3000, train_loss: 0.986656428062678, valid_loss: 0.9803237317294478\n",
            "Network 3 : start!!\n",
            "test_ind: 3, Epoch: 1, train_loss: 33.33549514228915, valid_loss: 7.220005282649288\n",
            "Validation loss decreased (inf --> 7.220005282649288).  Saving model ...\n",
            "test_ind: 3, Epoch: 2, train_loss: 4.600816308716197, valid_loss: 4.329554840370461\n",
            "Validation loss decreased (7.220005282649288 --> 4.329554840370461).  Saving model ...\n",
            "test_ind: 3, Epoch: 3, train_loss: 4.323293297379106, valid_loss: 3.74343674271195\n",
            "Validation loss decreased (4.329554840370461 --> 3.74343674271195).  Saving model ...\n",
            "test_ind: 3, Epoch: 4, train_loss: 4.155774822941533, valid_loss: 4.726686336376049\n",
            "test_ind: 3, Epoch: 5, train_loss: 4.257124806627815, valid_loss: 4.678458814267759\n",
            "test_ind: 3, Epoch: 6, train_loss: 4.071959789888359, valid_loss: 3.4851638476053877\n",
            "Validation loss decreased (3.74343674271195 --> 3.4851638476053877).  Saving model ...\n",
            "test_ind: 3, Epoch: 7, train_loss: 4.016879676300803, valid_loss: 4.059633943769667\n",
            "test_ind: 3, Epoch: 8, train_loss: 3.480253207830735, valid_loss: 3.659540052767153\n",
            "test_ind: 3, Epoch: 9, train_loss: 3.8593977056903603, valid_loss: 3.3567136658562555\n",
            "Validation loss decreased (3.4851638476053877 --> 3.3567136658562555).  Saving model ...\n",
            "test_ind: 3, Epoch: 10, train_loss: 3.6249971272032933, valid_loss: 3.5453344451056585\n",
            "test_ind: 3, Epoch: 11, train_loss: 3.9059220066776983, valid_loss: 3.260576742666739\n",
            "Validation loss decreased (3.3567136658562555 --> 3.260576742666739).  Saving model ...\n",
            "test_ind: 3, Epoch: 12, train_loss: 3.438814734235222, valid_loss: 3.8405782734906233\n",
            "test_ind: 3, Epoch: 13, train_loss: 3.6122198634677463, valid_loss: 3.254467699262831\n",
            "Validation loss decreased (3.260576742666739 --> 3.254467699262831).  Saving model ...\n",
            "test_ind: 3, Epoch: 14, train_loss: 3.5997427540060913, valid_loss: 4.381489312207258\n",
            "test_ind: 3, Epoch: 15, train_loss: 3.143385045322371, valid_loss: 2.945022812596074\n",
            "Validation loss decreased (3.254467699262831 --> 2.945022812596074).  Saving model ...\n",
            "test_ind: 3, Epoch: 16, train_loss: 3.681755207203053, valid_loss: 3.14469051361084\n",
            "test_ind: 3, Epoch: 17, train_loss: 3.504409065953008, valid_loss: 2.7104247587698476\n",
            "Validation loss decreased (2.945022812596074 --> 2.7104247587698476).  Saving model ...\n",
            "test_ind: 3, Epoch: 18, train_loss: 3.0958425910384566, valid_loss: 2.7926440592165345\n",
            "test_ind: 3, Epoch: 19, train_loss: 3.483842172740418, valid_loss: 2.6657912466261124\n",
            "Validation loss decreased (2.7104247587698476 --> 2.6657912466261124).  Saving model ...\n",
            "test_ind: 3, Epoch: 20, train_loss: 4.178938059159267, valid_loss: 3.806125570226599\n",
            "test_ind: 3, Epoch: 21, train_loss: 3.21529182386987, valid_loss: 3.775870093592891\n",
            "test_ind: 3, Epoch: 22, train_loss: 3.271882116058726, valid_loss: 3.441632571043792\n",
            "test_ind: 3, Epoch: 23, train_loss: 3.1836880401328758, valid_loss: 3.764900083895083\n",
            "test_ind: 3, Epoch: 24, train_loss: 3.2826145136797873, valid_loss: 3.8317276636759443\n",
            "test_ind: 3, Epoch: 25, train_loss: 3.720268867633961, valid_loss: 3.52240460007279\n",
            "test_ind: 3, Epoch: 26, train_loss: 3.2991309519167293, valid_loss: 3.2319970484133123\n",
            "test_ind: 3, Epoch: 27, train_loss: 3.146651774276922, valid_loss: 2.483738581339518\n",
            "Validation loss decreased (2.6657912466261124 --> 2.483738581339518).  Saving model ...\n",
            "test_ind: 3, Epoch: 28, train_loss: 3.16151961573848, valid_loss: 3.2323644249527543\n",
            "test_ind: 3, Epoch: 29, train_loss: 3.307753874931807, valid_loss: 3.014025529225667\n",
            "test_ind: 3, Epoch: 30, train_loss: 2.9667570797013654, valid_loss: 3.6325105561150446\n",
            "test_ind: 3, Epoch: 31, train_loss: 2.900121123702438, valid_loss: 2.6711594970137984\n",
            "test_ind: 3, Epoch: 32, train_loss: 3.018290284239215, valid_loss: 2.7563360532124843\n",
            "test_ind: 3, Epoch: 33, train_loss: 3.1582848054391364, valid_loss: 6.111802330723516\n",
            "test_ind: 3, Epoch: 34, train_loss: 3.26569265200768, valid_loss: 5.022840941393817\n",
            "test_ind: 3, Epoch: 35, train_loss: 3.0171312108451938, valid_loss: 3.5299193417584456\n",
            "test_ind: 3, Epoch: 36, train_loss: 3.086208920419952, valid_loss: 2.451897532851608\n",
            "Validation loss decreased (2.483738581339518 --> 2.451897532851608).  Saving model ...\n",
            "test_ind: 3, Epoch: 37, train_loss: 3.17749685122643, valid_loss: 2.476243813832601\n",
            "test_ind: 3, Epoch: 38, train_loss: 2.9590543052296576, valid_loss: 2.807578369423195\n",
            "test_ind: 3, Epoch: 39, train_loss: 3.0224575289973505, valid_loss: 2.9253685915911642\n",
            "test_ind: 3, Epoch: 40, train_loss: 3.148092452390694, valid_loss: 3.325192133585612\n",
            "test_ind: 3, Epoch: 41, train_loss: 2.78664317543124, valid_loss: 2.4035407348915383\n",
            "Validation loss decreased (2.451897532851608 --> 2.4035407348915383).  Saving model ...\n",
            "test_ind: 3, Epoch: 42, train_loss: 2.865558989254045, valid_loss: 2.6552154576336893\n",
            "test_ind: 3, Epoch: 43, train_loss: 2.954139738907048, valid_loss: 3.6766257286071777\n",
            "test_ind: 3, Epoch: 44, train_loss: 2.8653271639788587, valid_loss: 2.7985866334703235\n",
            "test_ind: 3, Epoch: 45, train_loss: 2.796181637563823, valid_loss: 2.155117547070539\n",
            "Validation loss decreased (2.4035407348915383 --> 2.155117547070539).  Saving model ...\n",
            "test_ind: 3, Epoch: 46, train_loss: 3.2238268263546037, valid_loss: 2.513618822450991\n",
            "test_ind: 3, Epoch: 47, train_loss: 3.1205077642275962, valid_loss: 4.0628806926586005\n",
            "test_ind: 3, Epoch: 48, train_loss: 2.9473564477614413, valid_loss: 2.515351277810556\n",
            "test_ind: 3, Epoch: 49, train_loss: 2.5708604859717097, valid_loss: 3.437725367369475\n",
            "test_ind: 3, Epoch: 50, train_loss: 2.776117719249961, valid_loss: 3.469292022563793\n",
            "test_ind: 3, Epoch: 51, train_loss: 3.091325553846948, valid_loss: 2.6380177250614874\n",
            "test_ind: 3, Epoch: 52, train_loss: 2.8407569814611358, valid_loss: 3.9326988326178656\n",
            "test_ind: 3, Epoch: 53, train_loss: 2.8213250843095192, valid_loss: 2.1357832308168767\n",
            "Validation loss decreased (2.155117547070539 --> 2.1357832308168767).  Saving model ...\n",
            "test_ind: 3, Epoch: 54, train_loss: 2.9721675272341126, valid_loss: 2.5401880652816207\n",
            "test_ind: 3, Epoch: 55, train_loss: 2.65477810965644, valid_loss: 2.5721846686469183\n",
            "test_ind: 3, Epoch: 56, train_loss: 2.8370569017198353, valid_loss: 2.391105298642759\n",
            "test_ind: 3, Epoch: 57, train_loss: 2.882671326766779, valid_loss: 2.638154594986527\n",
            "test_ind: 3, Epoch: 58, train_loss: 3.3593794328195083, valid_loss: 2.6412050105907303\n",
            "test_ind: 3, Epoch: 59, train_loss: 2.7470489490179366, valid_loss: 2.2770048600656017\n",
            "test_ind: 3, Epoch: 60, train_loss: 3.1228921854937512, valid_loss: 2.8579739111441156\n",
            "test_ind: 3, Epoch: 61, train_loss: 2.591228231971647, valid_loss: 3.5121748005902327\n",
            "test_ind: 3, Epoch: 62, train_loss: 2.8110031551784944, valid_loss: 2.5459606735794633\n",
            "test_ind: 3, Epoch: 63, train_loss: 2.7688290572460783, valid_loss: 3.254820753026892\n",
            "test_ind: 3, Epoch: 64, train_loss: 2.939164426591661, valid_loss: 2.959741963280572\n",
            "test_ind: 3, Epoch: 65, train_loss: 3.0851668428491665, valid_loss: 3.155733938570376\n",
            "test_ind: 3, Epoch: 66, train_loss: 3.1115541987948947, valid_loss: 2.900545261524342\n",
            "test_ind: 3, Epoch: 67, train_loss: 3.0222179094950357, valid_loss: 3.520316459514477\n",
            "test_ind: 3, Epoch: 68, train_loss: 3.190162122985463, valid_loss: 3.850228009400544\n",
            "test_ind: 3, Epoch: 69, train_loss: 2.7672428672696343, valid_loss: 2.4543642821135343\n",
            "test_ind: 3, Epoch: 70, train_loss: 3.3895947315074775, valid_loss: 4.229552975407353\n",
            "test_ind: 3, Epoch: 71, train_loss: 2.9111991693944104, valid_loss: 3.223215103149414\n",
            "test_ind: 3, Epoch: 72, train_loss: 2.821116359145553, valid_loss: 2.842011080847846\n",
            "test_ind: 3, Epoch: 73, train_loss: 2.898388067881266, valid_loss: 2.495274791011104\n",
            "test_ind: 3, Epoch: 74, train_loss: 2.7593256573618197, valid_loss: 2.538057274288602\n",
            "test_ind: 3, Epoch: 75, train_loss: 2.6530187748096608, valid_loss: 2.9016576872931585\n",
            "test_ind: 3, Epoch: 76, train_loss: 3.13444533760165, valid_loss: 2.9240855111016164\n",
            "test_ind: 3, Epoch: 77, train_loss: 3.03046875824163, valid_loss: 2.9880219388891147\n",
            "test_ind: 3, Epoch: 78, train_loss: 2.9397222142160677, valid_loss: 3.185982598198785\n",
            "test_ind: 3, Epoch: 79, train_loss: 2.733020764810068, valid_loss: 2.9515711113258645\n",
            "test_ind: 3, Epoch: 80, train_loss: 2.782928655176987, valid_loss: 3.790278081540708\n",
            "test_ind: 3, Epoch: 81, train_loss: 3.150664311868173, valid_loss: 3.729066636827257\n",
            "test_ind: 3, Epoch: 82, train_loss: 3.1077514872138883, valid_loss: 3.5927865416915328\n",
            "test_ind: 3, Epoch: 83, train_loss: 2.748839307714392, valid_loss: 2.362442634723805\n",
            "test_ind: 3, Epoch: 84, train_loss: 2.7398209689575945, valid_loss: 3.6892860377276384\n",
            "test_ind: 3, Epoch: 85, train_loss: 2.783702520676601, valid_loss: 3.1534861458672414\n",
            "test_ind: 3, Epoch: 86, train_loss: 2.7504025388647007, valid_loss: 3.396684946837249\n",
            "test_ind: 3, Epoch: 87, train_loss: 2.688736350448043, valid_loss: 2.865425039220739\n",
            "test_ind: 3, Epoch: 88, train_loss: 2.6289923514848876, valid_loss: 2.7397782007853193\n",
            "test_ind: 3, Epoch: 89, train_loss: 2.780793737482142, valid_loss: 2.9589377685829445\n",
            "test_ind: 3, Epoch: 90, train_loss: 2.5588781745345504, valid_loss: 3.048008918762207\n",
            "test_ind: 3, Epoch: 91, train_loss: 2.9924544287316595, valid_loss: 2.703274638564498\n",
            "test_ind: 3, Epoch: 92, train_loss: 2.7134377632612066, valid_loss: 3.108902436715585\n",
            "test_ind: 3, Epoch: 93, train_loss: 2.716251296761595, valid_loss: 3.1298293007744684\n",
            "test_ind: 3, Epoch: 94, train_loss: 2.878247949812147, valid_loss: 2.682698779635959\n",
            "test_ind: 3, Epoch: 95, train_loss: 2.6225461430019803, valid_loss: 2.4968293507893877\n",
            "test_ind: 3, Epoch: 96, train_loss: 2.5898649663101008, valid_loss: 2.6679539150661893\n",
            "test_ind: 3, Epoch: 97, train_loss: 2.832398667747592, valid_loss: 3.5483528949596264\n",
            "test_ind: 3, Epoch: 98, train_loss: 2.5547896726631825, valid_loss: 2.2520440772727683\n",
            "test_ind: 3, Epoch: 99, train_loss: 3.1089940483187455, valid_loss: 2.8301020904823586\n",
            "test_ind: 3, Epoch: 100, train_loss: 3.054452725398688, valid_loss: 2.823684710043448\n",
            "test_ind: 3, Epoch: 101, train_loss: 2.8479470676845975, valid_loss: 3.181049082014296\n",
            "test_ind: 3, Epoch: 102, train_loss: 2.9933231082963347, valid_loss: 2.3473948019522206\n",
            "test_ind: 3, Epoch: 103, train_loss: 2.963704379988305, valid_loss: 2.5466237951208046\n",
            "test_ind: 3, Epoch: 104, train_loss: 2.5170842512154286, valid_loss: 2.807077372515643\n",
            "test_ind: 3, Epoch: 105, train_loss: 2.5230814321541493, valid_loss: 2.2659387058681912\n",
            "test_ind: 3, Epoch: 106, train_loss: 2.4548750806737827, valid_loss: 2.6509944244667336\n",
            "test_ind: 3, Epoch: 107, train_loss: 2.7342096022617675, valid_loss: 3.183158786208541\n",
            "test_ind: 3, Epoch: 108, train_loss: 3.2693072837076067, valid_loss: 3.098271052042643\n",
            "test_ind: 3, Epoch: 109, train_loss: 2.646631876627604, valid_loss: 2.466955061312075\n",
            "test_ind: 3, Epoch: 110, train_loss: 3.0644316378934886, valid_loss: 2.3246884522614657\n",
            "test_ind: 3, Epoch: 111, train_loss: 3.1145690282185874, valid_loss: 2.662157252982811\n",
            "test_ind: 3, Epoch: 112, train_loss: 2.7891413665112155, valid_loss: 2.6127343001189054\n",
            "test_ind: 3, Epoch: 113, train_loss: 2.7697549396091032, valid_loss: 2.338475174374051\n",
            "test_ind: 3, Epoch: 114, train_loss: 2.789771044695819, valid_loss: 2.8674288149233216\n",
            "test_ind: 3, Epoch: 115, train_loss: 2.6774029614012917, valid_loss: 2.1483043564690485\n",
            "test_ind: 3, Epoch: 116, train_loss: 2.9735695638774358, valid_loss: 3.7510186301337347\n",
            "test_ind: 3, Epoch: 117, train_loss: 3.1162314591584384, valid_loss: 4.214772718924063\n",
            "test_ind: 3, Epoch: 118, train_loss: 2.691382255083249, valid_loss: 2.0907059245639377\n",
            "Validation loss decreased (2.1357832308168767 --> 2.0907059245639377).  Saving model ...\n",
            "test_ind: 3, Epoch: 119, train_loss: 2.7862460583816344, valid_loss: 2.9689909793712475\n",
            "test_ind: 3, Epoch: 120, train_loss: 2.955582630487136, valid_loss: 2.1643270033377187\n",
            "test_ind: 3, Epoch: 121, train_loss: 2.7407899197237, valid_loss: 3.636958687393754\n",
            "test_ind: 3, Epoch: 122, train_loss: 2.9217555316877952, valid_loss: 2.8652300481443054\n",
            "test_ind: 3, Epoch: 123, train_loss: 2.9551670875078373, valid_loss: 3.92536754961367\n",
            "test_ind: 3, Epoch: 124, train_loss: 2.6796848979997048, valid_loss: 2.6595410947446467\n",
            "test_ind: 3, Epoch: 125, train_loss: 2.564567171497109, valid_loss: 2.4226156340705027\n",
            "test_ind: 3, Epoch: 126, train_loss: 2.4672394681859897, valid_loss: 2.388688564300537\n",
            "test_ind: 3, Epoch: 127, train_loss: 2.6793814470738537, valid_loss: 2.20133368174235\n",
            "test_ind: 3, Epoch: 128, train_loss: 2.856124883816566, valid_loss: 2.2822868912308305\n",
            "test_ind: 3, Epoch: 129, train_loss: 2.9724871965102206, valid_loss: 2.14929379357232\n",
            "test_ind: 3, Epoch: 130, train_loss: 2.507708208060559, valid_loss: 2.8424181055139615\n",
            "test_ind: 3, Epoch: 131, train_loss: 2.881247349727301, valid_loss: 3.318184640672472\n",
            "test_ind: 3, Epoch: 132, train_loss: 2.5157484537289467, valid_loss: 2.1848574214511447\n",
            "test_ind: 3, Epoch: 133, train_loss: 2.6141754432960793, valid_loss: 2.656198854799624\n",
            "test_ind: 3, Epoch: 134, train_loss: 2.660568019490183, valid_loss: 2.6155733885588472\n",
            "test_ind: 3, Epoch: 135, train_loss: 2.69106478161282, valid_loss: 2.4160683773182052\n",
            "test_ind: 3, Epoch: 136, train_loss: 2.5613872151315946, valid_loss: 2.1390283549273454\n",
            "test_ind: 3, Epoch: 137, train_loss: 2.700098226099838, valid_loss: 2.4189099735683866\n",
            "test_ind: 3, Epoch: 138, train_loss: 2.7655685683827342, valid_loss: 2.5324531661139593\n",
            "test_ind: 3, Epoch: 139, train_loss: 2.497810834719811, valid_loss: 2.390697797139486\n",
            "test_ind: 3, Epoch: 140, train_loss: 2.94154582200227, valid_loss: 2.4944612538373026\n",
            "test_ind: 3, Epoch: 141, train_loss: 2.742298420564628, valid_loss: 2.479073612778275\n",
            "test_ind: 3, Epoch: 142, train_loss: 2.630192797860982, valid_loss: 2.6798341009351945\n",
            "test_ind: 3, Epoch: 143, train_loss: 2.551233179775285, valid_loss: 2.025184984560366\n",
            "Validation loss decreased (2.0907059245639377 --> 2.025184984560366).  Saving model ...\n",
            "test_ind: 3, Epoch: 144, train_loss: 2.573144477090718, valid_loss: 2.121082058659306\n",
            "test_ind: 3, Epoch: 145, train_loss: 2.513607219413475, valid_loss: 2.4302344675417302\n",
            "test_ind: 3, Epoch: 146, train_loss: 2.7101195417804482, valid_loss: 2.659206566987214\n",
            "test_ind: 3, Epoch: 147, train_loss: 2.773733957314197, valid_loss: 2.7756793057477034\n",
            "test_ind: 3, Epoch: 148, train_loss: 2.7589123101882, valid_loss: 3.235485695026539\n",
            "test_ind: 3, Epoch: 149, train_loss: 2.5370696444570284, valid_loss: 2.547151600873029\n",
            "test_ind: 3, Epoch: 150, train_loss: 2.9321498046686623, valid_loss: 2.4934744658293546\n",
            "test_ind: 3, Epoch: 151, train_loss: 2.915558485337245, valid_loss: 5.288275047584817\n",
            "test_ind: 3, Epoch: 152, train_loss: 2.9179708516156233, valid_loss: 2.6490351535655834\n",
            "test_ind: 3, Epoch: 153, train_loss: 2.6203374332851834, valid_loss: 2.7496776050991483\n",
            "test_ind: 3, Epoch: 154, train_loss: 2.4093375323731223, valid_loss: 2.924498822953966\n",
            "test_ind: 3, Epoch: 155, train_loss: 3.0144367630099076, valid_loss: 3.3528171468664096\n",
            "test_ind: 3, Epoch: 156, train_loss: 2.41611392998401, valid_loss: 2.4828559734203197\n",
            "test_ind: 3, Epoch: 157, train_loss: 2.756281793853383, valid_loss: 2.432139520291929\n",
            "test_ind: 3, Epoch: 158, train_loss: 2.5788782319904846, valid_loss: 2.7544758408157914\n",
            "test_ind: 3, Epoch: 159, train_loss: 2.513303456483064, valid_loss: 2.6010288838986995\n",
            "test_ind: 3, Epoch: 160, train_loss: 2.583404552789382, valid_loss: 1.9127756048131872\n",
            "Validation loss decreased (2.025184984560366 --> 1.9127756048131872).  Saving model ...\n",
            "test_ind: 3, Epoch: 161, train_loss: 2.5289226166995955, valid_loss: 3.111392498016357\n",
            "test_ind: 3, Epoch: 162, train_loss: 2.6005565031075184, valid_loss: 2.6551541928891784\n",
            "test_ind: 3, Epoch: 163, train_loss: 2.4279674070852777, valid_loss: 2.1320691815129034\n",
            "test_ind: 3, Epoch: 164, train_loss: 2.3657212551729176, valid_loss: 2.1964490148756237\n",
            "test_ind: 3, Epoch: 165, train_loss: 2.3969572326283393, valid_loss: 2.1770853643064143\n",
            "test_ind: 3, Epoch: 166, train_loss: 3.0079640812344026, valid_loss: 2.474045135356762\n",
            "test_ind: 3, Epoch: 167, train_loss: 2.447744234108631, valid_loss: 2.134301927354601\n",
            "test_ind: 3, Epoch: 168, train_loss: 2.6518811061058516, valid_loss: 4.2355660862392845\n",
            "test_ind: 3, Epoch: 169, train_loss: 2.5501450314933876, valid_loss: 2.617371488500525\n",
            "test_ind: 3, Epoch: 170, train_loss: 2.479256983156557, valid_loss: 2.8228127868087207\n",
            "test_ind: 3, Epoch: 171, train_loss: 2.7495378447167664, valid_loss: 2.9792323642306857\n",
            "test_ind: 3, Epoch: 172, train_loss: 2.5372348479282714, valid_loss: 3.042149225870768\n",
            "test_ind: 3, Epoch: 173, train_loss: 2.5483246791509937, valid_loss: 2.569383656537091\n",
            "test_ind: 3, Epoch: 174, train_loss: 2.5374491067580234, valid_loss: 2.8225060922128185\n",
            "test_ind: 3, Epoch: 175, train_loss: 2.5055663144146956, valid_loss: 2.0884091412579573\n",
            "test_ind: 3, Epoch: 176, train_loss: 2.6103093889024525, valid_loss: 2.3408712634333857\n",
            "test_ind: 3, Epoch: 177, train_loss: 2.4094992566991738, valid_loss: 2.2019617469222457\n",
            "test_ind: 3, Epoch: 178, train_loss: 2.560646722346176, valid_loss: 2.888946391918041\n",
            "test_ind: 3, Epoch: 179, train_loss: 2.656483261673539, valid_loss: 2.584690729777018\n",
            "test_ind: 3, Epoch: 180, train_loss: 2.7221294744515125, valid_loss: 2.3377807758472584\n",
            "test_ind: 3, Epoch: 181, train_loss: 2.532523055135468, valid_loss: 2.525109873877631\n",
            "test_ind: 3, Epoch: 182, train_loss: 2.3345596113322693, valid_loss: 3.32219538865266\n",
            "test_ind: 3, Epoch: 183, train_loss: 2.4671501171441728, valid_loss: 2.337861326005724\n",
            "test_ind: 3, Epoch: 184, train_loss: 2.4354059490156756, valid_loss: 2.03655355947989\n",
            "test_ind: 3, Epoch: 185, train_loss: 2.419604083638132, valid_loss: 3.395891295539008\n",
            "test_ind: 3, Epoch: 186, train_loss: 2.502340781835862, valid_loss: 2.061151734104863\n",
            "test_ind: 3, Epoch: 187, train_loss: 2.5413494051238636, valid_loss: 3.017105950249566\n",
            "test_ind: 3, Epoch: 188, train_loss: 2.384812584629765, valid_loss: 2.6922577575401023\n",
            "test_ind: 3, Epoch: 189, train_loss: 2.3549456125424237, valid_loss: 2.747920036315918\n",
            "test_ind: 3, Epoch: 190, train_loss: 2.565139399634467, valid_loss: 3.017806123804163\n",
            "test_ind: 3, Epoch: 191, train_loss: 2.677687703827281, valid_loss: 2.0187428792317705\n",
            "test_ind: 3, Epoch: 192, train_loss: 2.77402052467252, valid_loss: 3.1032157827306674\n",
            "test_ind: 3, Epoch: 193, train_loss: 2.4751272849094716, valid_loss: 2.2402235490304454\n",
            "test_ind: 3, Epoch: 194, train_loss: 2.40697189024937, valid_loss: 2.2765415156329123\n",
            "test_ind: 3, Epoch: 195, train_loss: 2.3341375221440823, valid_loss: 2.3795624485722295\n",
            "test_ind: 3, Epoch: 196, train_loss: 2.513498942057291, valid_loss: 2.441470587695086\n",
            "test_ind: 3, Epoch: 197, train_loss: 2.6261834038628473, valid_loss: 2.9401396822046353\n",
            "test_ind: 3, Epoch: 198, train_loss: 2.455549870008304, valid_loss: 2.6659434813040277\n",
            "test_ind: 3, Epoch: 199, train_loss: 2.8353194837216975, valid_loss: 2.160127145272714\n",
            "test_ind: 3, Epoch: 200, train_loss: 2.5526005014961153, valid_loss: 2.8918580479092064\n",
            "test_ind: 3, Epoch: 201, train_loss: 2.7701487953280224, valid_loss: 3.0406321772822626\n",
            "test_ind: 3, Epoch: 202, train_loss: 2.4976945453219943, valid_loss: 2.9192416579635054\n",
            "test_ind: 3, Epoch: 203, train_loss: 2.1657746338549955, valid_loss: 2.015344089931912\n",
            "test_ind: 3, Epoch: 204, train_loss: 2.317087208783185, valid_loss: 2.7817812495761447\n",
            "test_ind: 3, Epoch: 205, train_loss: 2.5218435570045754, valid_loss: 3.8189779387580023\n",
            "test_ind: 3, Epoch: 206, train_loss: 2.822776659035388, valid_loss: 2.0797619819641113\n",
            "test_ind: 3, Epoch: 207, train_loss: 2.4843555085452986, valid_loss: 2.798317591349284\n",
            "test_ind: 3, Epoch: 208, train_loss: 2.299981906090254, valid_loss: 1.935325852146855\n",
            "test_ind: 3, Epoch: 209, train_loss: 2.255021489696738, valid_loss: 2.2321493360731335\n",
            "test_ind: 3, Epoch: 210, train_loss: 2.3288631792421692, valid_loss: 2.232141123877631\n",
            "test_ind: 3, Epoch: 211, train_loss: 2.4803107461811584, valid_loss: 2.4014839772824885\n",
            "test_ind: 3, Epoch: 212, train_loss: 2.350139965245753, valid_loss: 2.368349057656747\n",
            "test_ind: 3, Epoch: 213, train_loss: 2.5270051661832835, valid_loss: 1.9725679644831906\n",
            "test_ind: 3, Epoch: 214, train_loss: 2.525959050213849, valid_loss: 2.053867887567591\n",
            "test_ind: 3, Epoch: 215, train_loss: 2.343632474357699, valid_loss: 2.6514972757410122\n",
            "test_ind: 3, Epoch: 216, train_loss: 2.2934623117800115, valid_loss: 2.151547061072456\n",
            "test_ind: 3, Epoch: 217, train_loss: 2.584114887096264, valid_loss: 2.2565193706088595\n",
            "test_ind: 3, Epoch: 218, train_loss: 2.667207659026723, valid_loss: 3.142721635323984\n",
            "test_ind: 3, Epoch: 219, train_loss: 2.239299250237736, valid_loss: 1.905744199399595\n",
            "Validation loss decreased (1.9127756048131872 --> 1.905744199399595).  Saving model ...\n",
            "test_ind: 3, Epoch: 220, train_loss: 2.2940474557287898, valid_loss: 1.8820012410481772\n",
            "Validation loss decreased (1.905744199399595 --> 1.8820012410481772).  Saving model ...\n",
            "test_ind: 3, Epoch: 221, train_loss: 2.39762172581237, valid_loss: 2.7948328477365\n",
            "test_ind: 3, Epoch: 222, train_loss: 2.5473119241219977, valid_loss: 2.39108203958582\n",
            "test_ind: 3, Epoch: 223, train_loss: 2.3046659540247028, valid_loss: 1.9912578441478588\n",
            "test_ind: 3, Epoch: 224, train_loss: 2.4295529318444524, valid_loss: 2.1588523476212114\n",
            "test_ind: 3, Epoch: 225, train_loss: 2.369117336508668, valid_loss: 2.7040750715467667\n",
            "test_ind: 3, Epoch: 226, train_loss: 2.2555722719357334, valid_loss: 2.61013541398225\n",
            "test_ind: 3, Epoch: 227, train_loss: 2.583522620024504, valid_loss: 3.2375760961461952\n",
            "test_ind: 3, Epoch: 228, train_loss: 2.3976767857869468, valid_loss: 2.140777835139522\n",
            "test_ind: 3, Epoch: 229, train_loss: 2.219700542497046, valid_loss: 2.1118428442213273\n",
            "test_ind: 3, Epoch: 230, train_loss: 2.410030470954047, valid_loss: 2.3137588500976562\n",
            "test_ind: 3, Epoch: 231, train_loss: 2.2229278823475775, valid_loss: 2.860963803750497\n",
            "test_ind: 3, Epoch: 232, train_loss: 2.5942113723283935, valid_loss: 2.0820877110516585\n",
            "test_ind: 3, Epoch: 233, train_loss: 2.210687678537251, valid_loss: 2.9902869153905796\n",
            "test_ind: 3, Epoch: 234, train_loss: 2.4023469701225375, valid_loss: 2.220923741658529\n",
            "test_ind: 3, Epoch: 235, train_loss: 2.1560118698779447, valid_loss: 1.9640994425173162\n",
            "test_ind: 3, Epoch: 236, train_loss: 2.369731184876995, valid_loss: 2.2939080662197537\n",
            "test_ind: 3, Epoch: 237, train_loss: 2.399103029274646, valid_loss: 2.3642618214642557\n",
            "test_ind: 3, Epoch: 238, train_loss: 2.7012785216908393, valid_loss: 2.378501786126031\n",
            "test_ind: 3, Epoch: 239, train_loss: 2.3108216627144516, valid_loss: 2.7354773238853176\n",
            "test_ind: 3, Epoch: 240, train_loss: 2.4528474277920194, valid_loss: 2.2128406277409307\n",
            "test_ind: 3, Epoch: 241, train_loss: 2.196923197051625, valid_loss: 2.299726609830503\n",
            "test_ind: 3, Epoch: 242, train_loss: 2.9951005806157625, valid_loss: 3.0310858797144005\n",
            "test_ind: 3, Epoch: 243, train_loss: 2.428918308681912, valid_loss: 2.976986885070801\n",
            "test_ind: 3, Epoch: 244, train_loss: 2.7182509163279596, valid_loss: 2.7189648769519947\n",
            "test_ind: 3, Epoch: 245, train_loss: 2.447720633612739, valid_loss: 2.6325142825091326\n",
            "test_ind: 3, Epoch: 246, train_loss: 2.782558605994707, valid_loss: 2.7295469884519226\n",
            "test_ind: 3, Epoch: 247, train_loss: 2.2987952055754484, valid_loss: 2.2071863633615\n",
            "test_ind: 3, Epoch: 248, train_loss: 2.4316756637008097, valid_loss: 2.599217944675022\n",
            "test_ind: 3, Epoch: 249, train_loss: 2.4670807108467003, valid_loss: 2.717368479128237\n",
            "test_ind: 3, Epoch: 250, train_loss: 2.2714717182112327, valid_loss: 2.5691042299623845\n",
            "test_ind: 3, Epoch: 251, train_loss: 2.4539074073603118, valid_loss: 3.612661750228317\n",
            "test_ind: 3, Epoch: 252, train_loss: 2.517070481806625, valid_loss: 2.401278195557771\n",
            "test_ind: 3, Epoch: 253, train_loss: 2.381106435516734, valid_loss: 1.9366674423217773\n",
            "test_ind: 3, Epoch: 254, train_loss: 2.5372642234519676, valid_loss: 2.9156544650042497\n",
            "test_ind: 3, Epoch: 255, train_loss: 2.2318893479712214, valid_loss: 2.1652091344197593\n",
            "test_ind: 3, Epoch: 256, train_loss: 2.363336533675959, valid_loss: 2.206393877665202\n",
            "test_ind: 3, Epoch: 257, train_loss: 2.2232908260675126, valid_loss: 3.1652771455270274\n",
            "test_ind: 3, Epoch: 258, train_loss: 2.5708332650455428, valid_loss: 3.1464490890502925\n",
            "test_ind: 3, Epoch: 259, train_loss: 2.3391194225829324, valid_loss: 3.3922724370603206\n",
            "test_ind: 3, Epoch: 260, train_loss: 2.4312999219070246, valid_loss: 2.1245643651043924\n",
            "test_ind: 3, Epoch: 261, train_loss: 2.107987403869629, valid_loss: 2.180980064250805\n",
            "test_ind: 3, Epoch: 262, train_loss: 2.5497716503378784, valid_loss: 3.1716352038913307\n",
            "test_ind: 3, Epoch: 263, train_loss: 2.1836678363658764, valid_loss: 1.9777165518866644\n",
            "test_ind: 3, Epoch: 264, train_loss: 2.172351990217044, valid_loss: 2.1639870007832847\n",
            "test_ind: 3, Epoch: 265, train_loss: 2.277923790025122, valid_loss: 1.9641988895557545\n",
            "test_ind: 3, Epoch: 266, train_loss: 2.1611811496593334, valid_loss: 2.172781644044099\n",
            "test_ind: 3, Epoch: 267, train_loss: 2.262778988590947, valid_loss: 1.8610744123105647\n",
            "Validation loss decreased (1.8820012410481772 --> 1.8610744123105647).  Saving model ...\n",
            "test_ind: 3, Epoch: 268, train_loss: 2.2610735069086525, valid_loss: 2.160678598615858\n",
            "test_ind: 3, Epoch: 269, train_loss: 2.091766522254473, valid_loss: 2.2873658250879356\n",
            "test_ind: 3, Epoch: 270, train_loss: 2.1766849976998786, valid_loss: 2.4722750628436057\n",
            "test_ind: 3, Epoch: 271, train_loss: 2.5625820689731174, valid_loss: 2.5673707326253257\n",
            "test_ind: 3, Epoch: 272, train_loss: 2.5692873295442555, valid_loss: 3.3107272254096136\n",
            "test_ind: 3, Epoch: 273, train_loss: 2.2200172977682984, valid_loss: 2.365064744596128\n",
            "test_ind: 3, Epoch: 274, train_loss: 2.4623663984699014, valid_loss: 2.1830182605319552\n",
            "test_ind: 3, Epoch: 275, train_loss: 2.2193591153180154, valid_loss: 2.040362269790084\n",
            "test_ind: 3, Epoch: 276, train_loss: 2.665281554799021, valid_loss: 4.033942646450466\n",
            "test_ind: 3, Epoch: 277, train_loss: 2.468935990039213, valid_loss: 2.374799693072284\n",
            "test_ind: 3, Epoch: 278, train_loss: 2.432083965819559, valid_loss: 2.1565722006338612\n",
            "test_ind: 3, Epoch: 279, train_loss: 2.2588140169779463, valid_loss: 2.164695580800374\n",
            "test_ind: 3, Epoch: 280, train_loss: 2.400630833190164, valid_loss: 2.6960726314120818\n",
            "test_ind: 3, Epoch: 281, train_loss: 2.3945607491481447, valid_loss: 3.3644611217357494\n",
            "test_ind: 3, Epoch: 282, train_loss: 2.439480722686391, valid_loss: 2.7927843906261303\n",
            "test_ind: 3, Epoch: 283, train_loss: 2.343490759531657, valid_loss: 2.280670854780409\n",
            "test_ind: 3, Epoch: 284, train_loss: 2.3821787893036266, valid_loss: 2.0437966805917243\n",
            "test_ind: 3, Epoch: 285, train_loss: 2.602827007387891, valid_loss: 2.4596952155784324\n",
            "test_ind: 3, Epoch: 286, train_loss: 2.2926202055848677, valid_loss: 2.3180815908643932\n",
            "test_ind: 3, Epoch: 287, train_loss: 2.333295156926285, valid_loss: 2.4199890383967646\n",
            "test_ind: 3, Epoch: 288, train_loss: 2.2141230724475998, valid_loss: 2.2022472310949257\n",
            "test_ind: 3, Epoch: 289, train_loss: 2.124665637075165, valid_loss: 2.133021495960377\n",
            "test_ind: 3, Epoch: 290, train_loss: 2.1036410979282714, valid_loss: 1.9195176407142922\n",
            "test_ind: 3, Epoch: 291, train_loss: 2.5284256581906916, valid_loss: 2.426103309348777\n",
            "test_ind: 3, Epoch: 292, train_loss: 2.209978533379825, valid_loss: 2.2452348603142633\n",
            "test_ind: 3, Epoch: 293, train_loss: 2.109207424116723, valid_loss: 2.5553873026812517\n",
            "test_ind: 3, Epoch: 294, train_loss: 2.3674303278510953, valid_loss: 2.2668040416858815\n",
            "test_ind: 3, Epoch: 295, train_loss: 2.406135924068498, valid_loss: 1.944040545710811\n",
            "test_ind: 3, Epoch: 296, train_loss: 2.381938151371332, valid_loss: 2.315309206644694\n",
            "test_ind: 3, Epoch: 297, train_loss: 2.2376219784771956, valid_loss: 2.1069506009419756\n",
            "test_ind: 3, Epoch: 298, train_loss: 2.2080115624416026, valid_loss: 2.12319603672734\n",
            "test_ind: 3, Epoch: 299, train_loss: 2.102526594091345, valid_loss: 2.658282244646991\n",
            "test_ind: 3, Epoch: 300, train_loss: 2.295350045333674, valid_loss: 2.6919010303638595\n",
            "test_ind: 3, Epoch: 301, train_loss: 2.18011498156889, valid_loss: 3.8053017722235785\n",
            "test_ind: 3, Epoch: 302, train_loss: 2.3726757897271047, valid_loss: 2.6918457878960504\n",
            "test_ind: 3, Epoch: 303, train_loss: 2.0535593209443266, valid_loss: 1.8793581150196215\n",
            "test_ind: 3, Epoch: 304, train_loss: 2.2005696708773392, valid_loss: 2.304638385772705\n",
            "test_ind: 3, Epoch: 305, train_loss: 2.249165328932397, valid_loss: 2.065737741964835\n",
            "test_ind: 3, Epoch: 306, train_loss: 2.2046531041463213, valid_loss: 3.1454176549558284\n",
            "test_ind: 3, Epoch: 307, train_loss: 2.272939493626724, valid_loss: 2.0792226084956416\n",
            "test_ind: 3, Epoch: 308, train_loss: 2.1690030097961426, valid_loss: 1.888143433464898\n",
            "test_ind: 3, Epoch: 309, train_loss: 2.437346311263096, valid_loss: 2.7244668360109685\n",
            "test_ind: 3, Epoch: 310, train_loss: 2.455224543441961, valid_loss: 1.8933374970047563\n",
            "test_ind: 3, Epoch: 311, train_loss: 2.1262237054330333, valid_loss: 1.9151779104162145\n",
            "test_ind: 3, Epoch: 312, train_loss: 2.19350673534252, valid_loss: 1.8994814025031195\n",
            "test_ind: 3, Epoch: 313, train_loss: 2.1884655540372115, valid_loss: 2.1217314755475076\n",
            "test_ind: 3, Epoch: 314, train_loss: 2.210208533722678, valid_loss: 2.7153970930311413\n",
            "test_ind: 3, Epoch: 315, train_loss: 2.114855778070144, valid_loss: 2.3020367798981844\n",
            "test_ind: 3, Epoch: 316, train_loss: 2.3332374596301424, valid_loss: 2.5759913656446667\n",
            "test_ind: 3, Epoch: 317, train_loss: 2.217603777661736, valid_loss: 2.1342989250465676\n",
            "test_ind: 3, Epoch: 318, train_loss: 2.31292278383985, valid_loss: 2.187404067428024\n",
            "test_ind: 3, Epoch: 319, train_loss: 2.2156005435519748, valid_loss: 2.490546155858923\n",
            "test_ind: 3, Epoch: 320, train_loss: 2.0864627685075927, valid_loss: 1.9991702680234553\n",
            "test_ind: 3, Epoch: 321, train_loss: 2.153988661589446, valid_loss: 2.085910126014992\n",
            "test_ind: 3, Epoch: 322, train_loss: 1.9820298501002933, valid_loss: 1.8206166867856626\n",
            "Validation loss decreased (1.8610744123105647 --> 1.8206166867856626).  Saving model ...\n",
            "test_ind: 3, Epoch: 323, train_loss: 2.0544298313282154, valid_loss: 1.803064858471906\n",
            "Validation loss decreased (1.8206166867856626 --> 1.803064858471906).  Saving model ...\n",
            "test_ind: 3, Epoch: 324, train_loss: 2.0860460187182013, valid_loss: 2.2082386016845703\n",
            "test_ind: 3, Epoch: 325, train_loss: 2.221476749137596, valid_loss: 2.736490903077302\n",
            "test_ind: 3, Epoch: 326, train_loss: 2.2344050348540883, valid_loss: 2.356296963161892\n",
            "test_ind: 3, Epoch: 327, train_loss: 1.9781229113355094, valid_loss: 2.346163378821479\n",
            "test_ind: 3, Epoch: 328, train_loss: 2.462103761272666, valid_loss: 2.1524221279002997\n",
            "test_ind: 3, Epoch: 329, train_loss: 2.4879380920786915, valid_loss: 1.8174999554951987\n",
            "test_ind: 3, Epoch: 330, train_loss: 2.1834396609553584, valid_loss: 2.3719961378309464\n",
            "test_ind: 3, Epoch: 331, train_loss: 2.1804498684259106, valid_loss: 2.539182327411793\n",
            "test_ind: 3, Epoch: 332, train_loss: 2.333270143579554, valid_loss: 3.6443665115921586\n",
            "test_ind: 3, Epoch: 333, train_loss: 2.383304325150855, valid_loss: 1.7836266800209328\n",
            "Validation loss decreased (1.803064858471906 --> 1.7836266800209328).  Saving model ...\n",
            "test_ind: 3, Epoch: 334, train_loss: 2.0109844914189092, valid_loss: 1.774165153503418\n",
            "Validation loss decreased (1.7836266800209328 --> 1.774165153503418).  Saving model ...\n",
            "test_ind: 3, Epoch: 335, train_loss: 2.3796390133139527, valid_loss: 3.0546388272885925\n",
            "test_ind: 3, Epoch: 336, train_loss: 2.1585466479077753, valid_loss: 2.528928350519251\n",
            "test_ind: 3, Epoch: 337, train_loss: 1.958656187410708, valid_loss: 2.346630502630163\n",
            "test_ind: 3, Epoch: 338, train_loss: 2.168288054289641, valid_loss: 1.8043668181807906\n",
            "test_ind: 3, Epoch: 339, train_loss: 2.0285770569318604, valid_loss: 2.179113776595504\n",
            "test_ind: 3, Epoch: 340, train_loss: 2.012725506299808, valid_loss: 2.1092044689037186\n",
            "test_ind: 3, Epoch: 341, train_loss: 2.1044060977888694, valid_loss: 2.0350802033035844\n",
            "test_ind: 3, Epoch: 342, train_loss: 2.2227449358245477, valid_loss: 2.1967888938056097\n",
            "test_ind: 3, Epoch: 343, train_loss: 2.0262537767857682, valid_loss: 1.994090203885679\n",
            "test_ind: 3, Epoch: 344, train_loss: 2.173180232813329, valid_loss: 2.4808385283858687\n",
            "test_ind: 3, Epoch: 345, train_loss: 2.231372073844627, valid_loss: 2.0844375998885543\n",
            "test_ind: 3, Epoch: 346, train_loss: 2.2408353016700278, valid_loss: 2.218867796438712\n",
            "test_ind: 3, Epoch: 347, train_loss: 2.068098774662724, valid_loss: 2.056789751406069\n",
            "test_ind: 3, Epoch: 348, train_loss: 2.129823337366552, valid_loss: 1.9236918908578378\n",
            "test_ind: 3, Epoch: 349, train_loss: 2.1273255524811923, valid_loss: 2.0223559980039245\n",
            "test_ind: 3, Epoch: 350, train_loss: 2.301466529752001, valid_loss: 2.4084621182194463\n",
            "test_ind: 3, Epoch: 351, train_loss: 1.9771660639915938, valid_loss: 1.9786606188173645\n",
            "test_ind: 3, Epoch: 352, train_loss: 2.3159660645473155, valid_loss: 2.228899849785699\n",
            "test_ind: 3, Epoch: 353, train_loss: 2.0747984309255343, valid_loss: 2.0394755292821816\n",
            "test_ind: 3, Epoch: 354, train_loss: 2.1047048097775307, valid_loss: 2.220992512173123\n",
            "test_ind: 3, Epoch: 355, train_loss: 2.0876436881077143, valid_loss: 2.5020240854333946\n",
            "test_ind: 3, Epoch: 356, train_loss: 2.2013172691251026, valid_loss: 2.9823564776667846\n",
            "test_ind: 3, Epoch: 357, train_loss: 2.305530995498469, valid_loss: 2.425827626828794\n",
            "test_ind: 3, Epoch: 358, train_loss: 2.1437612816139504, valid_loss: 2.07557181958799\n",
            "test_ind: 3, Epoch: 359, train_loss: 2.002927933210208, valid_loss: 1.881865766313341\n",
            "test_ind: 3, Epoch: 360, train_loss: 2.271210193634033, valid_loss: 2.017009046342638\n",
            "test_ind: 3, Epoch: 361, train_loss: 2.041233727961411, valid_loss: 2.392394595675998\n",
            "test_ind: 3, Epoch: 362, train_loss: 2.0577325408841354, valid_loss: 1.997658376340513\n",
            "test_ind: 3, Epoch: 363, train_loss: 1.9824227462580175, valid_loss: 1.8402698834737141\n",
            "test_ind: 3, Epoch: 364, train_loss: 2.0419441564583485, valid_loss: 2.2012429943791143\n",
            "test_ind: 3, Epoch: 365, train_loss: 2.258668887762376, valid_loss: 2.3570467277809426\n",
            "test_ind: 3, Epoch: 366, train_loss: 2.1164785785439575, valid_loss: 1.8161616325378418\n",
            "test_ind: 3, Epoch: 367, train_loss: 2.1464552055170505, valid_loss: 2.0506191606874817\n",
            "test_ind: 3, Epoch: 368, train_loss: 2.2121468708838945, valid_loss: 3.117422068560565\n",
            "test_ind: 3, Epoch: 369, train_loss: 2.166133433212469, valid_loss: 2.158588656672725\n",
            "test_ind: 3, Epoch: 370, train_loss: 2.1713557125609597, valid_loss: 2.2187968360053167\n",
            "test_ind: 3, Epoch: 371, train_loss: 2.2442437689981345, valid_loss: 3.099553443767406\n",
            "test_ind: 3, Epoch: 372, train_loss: 1.9732849862840443, valid_loss: 1.9153121135852955\n",
            "test_ind: 3, Epoch: 373, train_loss: 2.1827242227248203, valid_loss: 2.0070158463937267\n",
            "test_ind: 3, Epoch: 374, train_loss: 2.1578189178749367, valid_loss: 2.171297797450313\n",
            "test_ind: 3, Epoch: 375, train_loss: 1.919574784643856, valid_loss: 1.6838871814586498\n",
            "Validation loss decreased (1.774165153503418 --> 1.6838871814586498).  Saving model ...\n",
            "test_ind: 3, Epoch: 376, train_loss: 2.2417753125414435, valid_loss: 1.8603507147894969\n",
            "test_ind: 3, Epoch: 377, train_loss: 2.1822157023865496, valid_loss: 2.0689969416017884\n",
            "test_ind: 3, Epoch: 378, train_loss: 2.304368596018097, valid_loss: 1.822668269828514\n",
            "test_ind: 3, Epoch: 379, train_loss: 2.2511858940124516, valid_loss: 1.6753608209115485\n",
            "Validation loss decreased (1.6838871814586498 --> 1.6753608209115485).  Saving model ...\n",
            "test_ind: 3, Epoch: 380, train_loss: 2.1997406924212424, valid_loss: 1.8411718827706798\n",
            "test_ind: 3, Epoch: 381, train_loss: 2.1568556950416093, valid_loss: 2.018290254804823\n",
            "test_ind: 3, Epoch: 382, train_loss: 2.17803359914709, valid_loss: 2.0156155692206488\n",
            "test_ind: 3, Epoch: 383, train_loss: 2.3327579145078308, valid_loss: 2.1689717328106917\n",
            "test_ind: 3, Epoch: 384, train_loss: 2.011624030124994, valid_loss: 1.8097609060781974\n",
            "test_ind: 3, Epoch: 385, train_loss: 2.281973332534602, valid_loss: 1.8997225408200866\n",
            "test_ind: 3, Epoch: 386, train_loss: 1.9373773292258933, valid_loss: 1.8067038324144151\n",
            "test_ind: 3, Epoch: 387, train_loss: 2.1115789825533646, valid_loss: 2.184096618934914\n",
            "test_ind: 3, Epoch: 388, train_loss: 2.2960857932950245, valid_loss: 3.3470236813580545\n",
            "test_ind: 3, Epoch: 389, train_loss: 1.974395375192901, valid_loss: 1.827711158328586\n",
            "test_ind: 3, Epoch: 390, train_loss: 1.8966220455405152, valid_loss: 1.7237793604532878\n",
            "test_ind: 3, Epoch: 391, train_loss: 1.947483875133373, valid_loss: 1.7790614940502025\n",
            "test_ind: 3, Epoch: 392, train_loss: 2.013583218609845, valid_loss: 2.3356568018595376\n",
            "test_ind: 3, Epoch: 393, train_loss: 2.0177179913461942, valid_loss: 2.0036651646649393\n",
            "test_ind: 3, Epoch: 394, train_loss: 2.3879057625193654, valid_loss: 2.4425786336263022\n",
            "test_ind: 3, Epoch: 395, train_loss: 2.0474612447950573, valid_loss: 1.9661697634944209\n",
            "test_ind: 3, Epoch: 396, train_loss: 2.130545639697416, valid_loss: 1.9343645837571886\n",
            "test_ind: 3, Epoch: 397, train_loss: 2.1616507400701077, valid_loss: 1.7074804835849338\n",
            "test_ind: 3, Epoch: 398, train_loss: 2.0753227457588106, valid_loss: 1.8742730882432725\n",
            "test_ind: 3, Epoch: 399, train_loss: 2.4237776509037725, valid_loss: 2.2456986992447465\n",
            "test_ind: 3, Epoch: 400, train_loss: 2.117366160875485, valid_loss: 2.9304467660409435\n",
            "test_ind: 3, Epoch: 401, train_loss: 2.281495312113821, valid_loss: 2.0363972805164483\n",
            "test_ind: 3, Epoch: 402, train_loss: 2.0941131438738037, valid_loss: 2.040865809829147\n",
            "test_ind: 3, Epoch: 403, train_loss: 2.1838132128303434, valid_loss: 2.770351427572745\n",
            "test_ind: 3, Epoch: 404, train_loss: 2.07933270489728, valid_loss: 2.105494993704337\n",
            "test_ind: 3, Epoch: 405, train_loss: 2.2225048630325883, valid_loss: 3.247709927735505\n",
            "test_ind: 3, Epoch: 406, train_loss: 2.1935101615058055, valid_loss: 1.7843954298231337\n",
            "test_ind: 3, Epoch: 407, train_loss: 1.995234954504319, valid_loss: 2.1337104020295317\n",
            "test_ind: 3, Epoch: 408, train_loss: 2.1684468940452293, valid_loss: 1.8590477484243888\n",
            "test_ind: 3, Epoch: 409, train_loss: 2.0842385939609858, valid_loss: 2.3252231809828015\n",
            "test_ind: 3, Epoch: 410, train_loss: 2.0999848047892256, valid_loss: 2.005506815733733\n",
            "test_ind: 3, Epoch: 411, train_loss: 1.9810127976499956, valid_loss: 2.5134753827695495\n",
            "test_ind: 3, Epoch: 412, train_loss: 2.0956927523200894, valid_loss: 1.7524298385337547\n",
            "test_ind: 3, Epoch: 413, train_loss: 2.323076683797954, valid_loss: 2.0773352163809315\n",
            "test_ind: 3, Epoch: 414, train_loss: 2.0433514088760187, valid_loss: 2.2166954852916576\n",
            "test_ind: 3, Epoch: 415, train_loss: 1.9131284525365002, valid_loss: 1.8473993760568124\n",
            "test_ind: 3, Epoch: 416, train_loss: 1.974243246478799, valid_loss: 1.9518565778379087\n",
            "test_ind: 3, Epoch: 417, train_loss: 1.998293058371838, valid_loss: 1.8191051129941587\n",
            "test_ind: 3, Epoch: 418, train_loss: 2.0825480826107072, valid_loss: 2.448780024493182\n",
            "test_ind: 3, Epoch: 419, train_loss: 2.124260213640001, valid_loss: 2.617385793615271\n",
            "test_ind: 3, Epoch: 420, train_loss: 2.1753591078299066, valid_loss: 2.3087540909096047\n",
            "test_ind: 3, Epoch: 421, train_loss: 2.1143557054025153, valid_loss: 1.938055232719139\n",
            "test_ind: 3, Epoch: 422, train_loss: 2.069083366864993, valid_loss: 1.9935061490094221\n",
            "test_ind: 3, Epoch: 423, train_loss: 2.1117398061870056, valid_loss: 2.378167576260037\n",
            "test_ind: 3, Epoch: 424, train_loss: 2.2086817717846534, valid_loss: 2.150452454884847\n",
            "test_ind: 3, Epoch: 425, train_loss: 2.05569017669301, valid_loss: 2.009395722989683\n",
            "test_ind: 3, Epoch: 426, train_loss: 2.092937387066123, valid_loss: 2.000800715552436\n",
            "test_ind: 3, Epoch: 427, train_loss: 2.0348150759567445, valid_loss: 1.9667033442744501\n",
            "test_ind: 3, Epoch: 428, train_loss: 2.358283902391975, valid_loss: 2.2245560575414585\n",
            "test_ind: 3, Epoch: 429, train_loss: 2.105700428103223, valid_loss: 2.794493940141466\n",
            "test_ind: 3, Epoch: 430, train_loss: 2.007780816819933, valid_loss: 2.1207654387862593\n",
            "test_ind: 3, Epoch: 431, train_loss: 2.26700097543222, valid_loss: 2.6575327272768376\n",
            "test_ind: 3, Epoch: 432, train_loss: 1.9872828707282926, valid_loss: 2.3894910635771573\n",
            "test_ind: 3, Epoch: 433, train_loss: 2.1289975025035717, valid_loss: 2.290535414660418\n",
            "test_ind: 3, Epoch: 434, train_loss: 2.0840465993057062, valid_loss: 2.1005788379245334\n",
            "test_ind: 3, Epoch: 435, train_loss: 2.190114468704035, valid_loss: 2.5650475290086536\n",
            "test_ind: 3, Epoch: 436, train_loss: 2.14260047747765, valid_loss: 2.07309412073206\n",
            "test_ind: 3, Epoch: 437, train_loss: 1.882438188717689, valid_loss: 2.1147702711599843\n",
            "test_ind: 3, Epoch: 438, train_loss: 2.099510039812253, valid_loss: 2.0592617282161005\n",
            "test_ind: 3, Epoch: 439, train_loss: 2.0460439611364296, valid_loss: 1.9358746034127692\n",
            "test_ind: 3, Epoch: 440, train_loss: 2.0332189901375477, valid_loss: 1.9563254250420465\n",
            "test_ind: 3, Epoch: 441, train_loss: 1.9561319292327504, valid_loss: 1.6871558825174968\n",
            "test_ind: 3, Epoch: 442, train_loss: 2.053610842904927, valid_loss: 2.9725947203459566\n",
            "test_ind: 3, Epoch: 443, train_loss: 2.0749123891194663, valid_loss: 1.8426309691535103\n",
            "test_ind: 3, Epoch: 444, train_loss: 2.0087463826309016, valid_loss: 2.5569201398778847\n",
            "test_ind: 3, Epoch: 445, train_loss: 2.1397004480715154, valid_loss: 1.8821854238156919\n",
            "test_ind: 3, Epoch: 446, train_loss: 2.130088605998475, valid_loss: 2.5056557125515404\n",
            "test_ind: 3, Epoch: 447, train_loss: 2.0421972392517844, valid_loss: 2.1824565640202276\n",
            "test_ind: 3, Epoch: 448, train_loss: 2.078619415377393, valid_loss: 1.993064491837113\n",
            "test_ind: 3, Epoch: 449, train_loss: 2.097298404316843, valid_loss: 1.8458462997719094\n",
            "test_ind: 3, Epoch: 450, train_loss: 2.0067538449793685, valid_loss: 2.279795734970658\n",
            "test_ind: 3, Epoch: 451, train_loss: 2.069249235553506, valid_loss: 1.9409357176886664\n",
            "test_ind: 3, Epoch: 452, train_loss: 2.095982692859791, valid_loss: 1.716065936618381\n",
            "test_ind: 3, Epoch: 453, train_loss: 1.9480158723430867, valid_loss: 2.1177414081714776\n",
            "test_ind: 3, Epoch: 454, train_loss: 2.0425380954036005, valid_loss: 1.8764677754154913\n",
            "test_ind: 3, Epoch: 455, train_loss: 2.2016820613248846, valid_loss: 2.316849567272045\n",
            "test_ind: 3, Epoch: 456, train_loss: 1.9783479431529105, valid_loss: 1.5930171012878418\n",
            "Validation loss decreased (1.6753608209115485 --> 1.5930171012878418).  Saving model ...\n",
            "test_ind: 3, Epoch: 457, train_loss: 2.0800442401273753, valid_loss: 1.8792720900641546\n",
            "test_ind: 3, Epoch: 458, train_loss: 1.8537552739367071, valid_loss: 2.0227550400627985\n",
            "test_ind: 3, Epoch: 459, train_loss: 2.006412771013048, valid_loss: 1.8451747541074401\n",
            "test_ind: 3, Epoch: 460, train_loss: 1.9337162265071162, valid_loss: 2.144614431593153\n",
            "test_ind: 3, Epoch: 461, train_loss: 2.030024269480764, valid_loss: 2.010959060103805\n",
            "test_ind: 3, Epoch: 462, train_loss: 1.9339892304973838, valid_loss: 1.8115258923283328\n",
            "test_ind: 3, Epoch: 463, train_loss: 2.0627327200807173, valid_loss: 2.0484365357293024\n",
            "test_ind: 3, Epoch: 464, train_loss: 1.9097401477672435, valid_loss: 1.9026898278130426\n",
            "test_ind: 3, Epoch: 465, train_loss: 1.9054874961758836, valid_loss: 1.8875318809791848\n",
            "test_ind: 3, Epoch: 466, train_loss: 2.052971245330057, valid_loss: 1.8566931618584526\n",
            "test_ind: 3, Epoch: 467, train_loss: 2.0057364805245106, valid_loss: 1.8712582941408509\n",
            "test_ind: 3, Epoch: 468, train_loss: 1.8902744187249076, valid_loss: 1.9848976665072973\n",
            "test_ind: 3, Epoch: 469, train_loss: 1.9566540835816184, valid_loss: 2.654959042867025\n",
            "test_ind: 3, Epoch: 470, train_loss: 2.1761721505059133, valid_loss: 1.8466991141990379\n",
            "test_ind: 3, Epoch: 471, train_loss: 2.087368659031244, valid_loss: 1.8286542715849698\n",
            "test_ind: 3, Epoch: 472, train_loss: 2.214805426420989, valid_loss: 1.8039065113774053\n",
            "test_ind: 3, Epoch: 473, train_loss: 1.8699224554462197, valid_loss: 1.5625810446562591\n",
            "Validation loss decreased (1.5930171012878418 --> 1.5625810446562591).  Saving model ...\n",
            "test_ind: 3, Epoch: 474, train_loss: 2.0661230793705694, valid_loss: 1.984049673433657\n",
            "test_ind: 3, Epoch: 475, train_loss: 1.896245609095067, valid_loss: 2.2049083886323153\n",
            "test_ind: 3, Epoch: 476, train_loss: 1.8561419675379627, valid_loss: 2.517769901840775\n",
            "test_ind: 3, Epoch: 477, train_loss: 2.056001357090326, valid_loss: 2.023965976856373\n",
            "test_ind: 3, Epoch: 478, train_loss: 2.0458502769470215, valid_loss: 1.6396547070255985\n",
            "test_ind: 3, Epoch: 479, train_loss: 2.0171121373588656, valid_loss: 1.9766883143672236\n",
            "test_ind: 3, Epoch: 480, train_loss: 1.9595334323835962, valid_loss: 1.9096515443589954\n",
            "test_ind: 3, Epoch: 481, train_loss: 2.1650209780092595, valid_loss: 2.6563734478420677\n",
            "test_ind: 3, Epoch: 482, train_loss: 1.9422792681941279, valid_loss: 1.9130142706411857\n",
            "test_ind: 3, Epoch: 483, train_loss: 2.007737695434947, valid_loss: 2.6177554660373263\n",
            "test_ind: 3, Epoch: 484, train_loss: 1.7954625023735895, valid_loss: 2.172611271893537\n",
            "test_ind: 3, Epoch: 485, train_loss: 2.0139206898065263, valid_loss: 1.8887095274748626\n",
            "test_ind: 3, Epoch: 486, train_loss: 1.8238925050806118, valid_loss: 1.6990190082126193\n",
            "test_ind: 3, Epoch: 487, train_loss: 2.176613148347831, valid_loss: 3.2291939346878618\n",
            "test_ind: 3, Epoch: 488, train_loss: 1.8965795481646497, valid_loss: 1.819231633786802\n",
            "test_ind: 3, Epoch: 489, train_loss: 1.79000120398439, valid_loss: 1.9882952548839428\n",
            "test_ind: 3, Epoch: 490, train_loss: 1.9301177307411477, valid_loss: 2.2830215383459023\n",
            "test_ind: 3, Epoch: 491, train_loss: 1.9140488247812526, valid_loss: 1.5969361729092069\n",
            "test_ind: 3, Epoch: 492, train_loss: 2.0207811343817066, valid_loss: 2.1228684495996544\n",
            "test_ind: 3, Epoch: 493, train_loss: 1.9471527146704404, valid_loss: 2.8887167859960488\n",
            "test_ind: 3, Epoch: 494, train_loss: 1.9177769790461032, valid_loss: 1.893146974069101\n",
            "test_ind: 3, Epoch: 495, train_loss: 1.934778266482883, valid_loss: 2.2358798097681114\n",
            "test_ind: 3, Epoch: 496, train_loss: 2.278693823166835, valid_loss: 1.9630014808089644\n",
            "test_ind: 3, Epoch: 497, train_loss: 1.9994587780516824, valid_loss: 1.9569191756071869\n",
            "test_ind: 3, Epoch: 498, train_loss: 1.9408536075073994, valid_loss: 2.5780988622594765\n",
            "test_ind: 3, Epoch: 499, train_loss: 2.0264502278080694, valid_loss: 2.169595577098705\n",
            "test_ind: 3, Epoch: 500, train_loss: 1.955526493213795, valid_loss: 1.7397030194600425\n",
            "test_ind: 3, Epoch: 501, train_loss: 1.7905770054569952, valid_loss: 2.127380847930908\n",
            "test_ind: 3, Epoch: 502, train_loss: 2.0247386002246244, valid_loss: 2.5008939107259116\n",
            "test_ind: 3, Epoch: 503, train_loss: 1.9713017322399002, valid_loss: 2.302001811839916\n",
            "test_ind: 3, Epoch: 504, train_loss: 1.9584588827910245, valid_loss: 2.1475979310494884\n",
            "test_ind: 3, Epoch: 505, train_loss: 1.7825296072312342, valid_loss: 1.6965472963121202\n",
            "test_ind: 3, Epoch: 506, train_loss: 1.903858679312247, valid_loss: 1.695164715802228\n",
            "test_ind: 3, Epoch: 507, train_loss: 1.8628697159849565, valid_loss: 1.8938480836373788\n",
            "test_ind: 3, Epoch: 508, train_loss: 1.7826776092435104, valid_loss: 2.1720097329881454\n",
            "test_ind: 3, Epoch: 509, train_loss: 1.9886269275053048, valid_loss: 1.9648684395684135\n",
            "test_ind: 3, Epoch: 510, train_loss: 1.9946929496011616, valid_loss: 1.7324643488283513\n",
            "test_ind: 3, Epoch: 511, train_loss: 1.893403212229411, valid_loss: 1.7750154777809426\n",
            "test_ind: 3, Epoch: 512, train_loss: 2.0396174913571206, valid_loss: 1.774334925192374\n",
            "test_ind: 3, Epoch: 513, train_loss: 1.9056967452720357, valid_loss: 1.7502997009842485\n",
            "test_ind: 3, Epoch: 514, train_loss: 1.9889856032383295, valid_loss: 1.7575004895528157\n",
            "test_ind: 3, Epoch: 515, train_loss: 2.1295238424230507, valid_loss: 1.7860403590732152\n",
            "test_ind: 3, Epoch: 516, train_loss: 2.0501259874414517, valid_loss: 1.8189514654654044\n",
            "test_ind: 3, Epoch: 517, train_loss: 2.0295068599559647, valid_loss: 1.7269746285897711\n",
            "test_ind: 3, Epoch: 518, train_loss: 1.7853234785574454, valid_loss: 1.7042469095300745\n",
            "test_ind: 3, Epoch: 519, train_loss: 2.0800428802584423, valid_loss: 3.0515965002554433\n",
            "test_ind: 3, Epoch: 520, train_loss: 1.961012716646548, valid_loss: 1.9089439180162215\n",
            "test_ind: 3, Epoch: 521, train_loss: 1.8801707161797416, valid_loss: 2.818990848682545\n",
            "test_ind: 3, Epoch: 522, train_loss: 1.8869174144886158, valid_loss: 1.8287512284738048\n",
            "test_ind: 3, Epoch: 523, train_loss: 1.9567040749538092, valid_loss: 1.639588091108534\n",
            "test_ind: 3, Epoch: 524, train_loss: 1.8276315912788297, valid_loss: 1.7009329795837402\n",
            "test_ind: 3, Epoch: 525, train_loss: 1.7924042925422574, valid_loss: 1.5850974365516945\n",
            "test_ind: 3, Epoch: 526, train_loss: 1.984722932179769, valid_loss: 2.5104846954345703\n",
            "test_ind: 3, Epoch: 527, train_loss: 1.8845383326212566, valid_loss: 1.9832217251812971\n",
            "test_ind: 3, Epoch: 528, train_loss: 1.785637154991244, valid_loss: 1.5060371822781033\n",
            "Validation loss decreased (1.5625810446562591 --> 1.5060371822781033).  Saving model ...\n",
            "test_ind: 3, Epoch: 529, train_loss: 1.821905036031464, valid_loss: 1.8457220218799733\n",
            "test_ind: 3, Epoch: 530, train_loss: 1.9302896099326052, valid_loss: 2.3264136137785734\n",
            "test_ind: 3, Epoch: 531, train_loss: 1.9236356947157118, valid_loss: 1.6655874958744756\n",
            "test_ind: 3, Epoch: 532, train_loss: 2.105784975452188, valid_loss: 2.4260515460261596\n",
            "test_ind: 3, Epoch: 533, train_loss: 1.7278508610195586, valid_loss: 1.8634637550071433\n",
            "test_ind: 3, Epoch: 534, train_loss: 1.9637661510043674, valid_loss: 1.8230636208145705\n",
            "test_ind: 3, Epoch: 535, train_loss: 1.882336057262656, valid_loss: 1.8529709886621546\n",
            "test_ind: 3, Epoch: 536, train_loss: 2.284589443677737, valid_loss: 2.824577066633436\n",
            "test_ind: 3, Epoch: 537, train_loss: 1.8506303363376193, valid_loss: 1.922586229112413\n",
            "test_ind: 3, Epoch: 538, train_loss: 1.834864463335202, valid_loss: 1.599592367808024\n",
            "test_ind: 3, Epoch: 539, train_loss: 1.945318704769935, valid_loss: 1.9889982011583114\n",
            "test_ind: 3, Epoch: 540, train_loss: 1.9494777608800817, valid_loss: 2.3926577038235135\n",
            "test_ind: 3, Epoch: 541, train_loss: 1.8708216054939928, valid_loss: 2.5836605495876737\n",
            "test_ind: 3, Epoch: 542, train_loss: 1.9086383949091406, valid_loss: 1.8094123910974573\n",
            "test_ind: 3, Epoch: 543, train_loss: 1.8733793953318654, valid_loss: 1.5505276785956488\n",
            "test_ind: 3, Epoch: 544, train_loss: 1.8274391668814198, valid_loss: 2.0461803012424045\n",
            "test_ind: 3, Epoch: 545, train_loss: 2.000697954201404, valid_loss: 2.1033778720431857\n",
            "test_ind: 3, Epoch: 546, train_loss: 1.8055568918769742, valid_loss: 1.5433810728567618\n",
            "test_ind: 3, Epoch: 547, train_loss: 1.8369966318577897, valid_loss: 1.7310596925240975\n",
            "test_ind: 3, Epoch: 548, train_loss: 1.8189353236445671, valid_loss: 1.9540299309624567\n",
            "test_ind: 3, Epoch: 549, train_loss: 1.7688170362401892, valid_loss: 1.6047092013888888\n",
            "test_ind: 3, Epoch: 550, train_loss: 1.7252988756438834, valid_loss: 1.5494054158528645\n",
            "test_ind: 3, Epoch: 551, train_loss: 1.9788480099336603, valid_loss: 2.393145543557626\n",
            "test_ind: 3, Epoch: 552, train_loss: 1.9955067575713734, valid_loss: 2.881503952874078\n",
            "test_ind: 3, Epoch: 553, train_loss: 1.911736653174883, valid_loss: 2.380286622930456\n",
            "test_ind: 3, Epoch: 554, train_loss: 2.023430582917767, valid_loss: 1.6461282482853643\n",
            "test_ind: 3, Epoch: 555, train_loss: 2.1571220115379055, valid_loss: 2.228353005868417\n",
            "test_ind: 3, Epoch: 556, train_loss: 1.890002156481331, valid_loss: 1.952203626985903\n",
            "test_ind: 3, Epoch: 557, train_loss: 2.1896397508220913, valid_loss: 1.8028625205711082\n",
            "test_ind: 3, Epoch: 558, train_loss: 1.9095375508437922, valid_loss: 1.6374774509006076\n",
            "test_ind: 3, Epoch: 559, train_loss: 2.0253833017231506, valid_loss: 1.7247650711624711\n",
            "test_ind: 3, Epoch: 560, train_loss: 1.9036628699597018, valid_loss: 2.4750983626754195\n",
            "test_ind: 3, Epoch: 561, train_loss: 1.998577747815921, valid_loss: 1.7920797665913901\n",
            "test_ind: 3, Epoch: 562, train_loss: 1.7923058815944342, valid_loss: 1.6375520847461842\n",
            "test_ind: 3, Epoch: 563, train_loss: 1.8186113392865215, valid_loss: 2.1924716278358742\n",
            "test_ind: 3, Epoch: 564, train_loss: 1.9188082777423623, valid_loss: 1.7869675424363878\n",
            "test_ind: 3, Epoch: 565, train_loss: 2.0049604604273665, valid_loss: 1.928047639352304\n",
            "test_ind: 3, Epoch: 566, train_loss: 1.7537387035511158, valid_loss: 1.7235440501460322\n",
            "test_ind: 3, Epoch: 567, train_loss: 1.8202748475251376, valid_loss: 1.6844412485758462\n",
            "test_ind: 3, Epoch: 568, train_loss: 2.0000369813707137, valid_loss: 1.8465129181190774\n",
            "test_ind: 3, Epoch: 569, train_loss: 1.9161143479523834, valid_loss: 1.6144207848442924\n",
            "test_ind: 3, Epoch: 570, train_loss: 1.8481076970512484, valid_loss: 1.506669521331787\n",
            "test_ind: 3, Epoch: 571, train_loss: 1.6995159372871305, valid_loss: 1.7023130522833931\n",
            "test_ind: 3, Epoch: 572, train_loss: 1.8673742023515114, valid_loss: 2.5753855881867587\n",
            "test_ind: 3, Epoch: 573, train_loss: 2.0373137791951494, valid_loss: 1.885366369176794\n",
            "test_ind: 3, Epoch: 574, train_loss: 1.8079175949096684, valid_loss: 1.9990714391072593\n",
            "test_ind: 3, Epoch: 575, train_loss: 2.023053910997179, valid_loss: 1.8277571819446705\n",
            "test_ind: 3, Epoch: 576, train_loss: 1.904666158888075, valid_loss: 1.6986851338987\n",
            "test_ind: 3, Epoch: 577, train_loss: 1.6889319831942333, valid_loss: 1.8945246449223272\n",
            "test_ind: 3, Epoch: 578, train_loss: 1.987863787898311, valid_loss: 1.7240263444406014\n",
            "test_ind: 3, Epoch: 579, train_loss: 1.8199442698631756, valid_loss: 1.898904076328984\n",
            "test_ind: 3, Epoch: 580, train_loss: 1.8711330566877198, valid_loss: 2.36132252657855\n",
            "test_ind: 3, Epoch: 581, train_loss: 2.095157317173334, valid_loss: 1.7974726535655834\n",
            "test_ind: 3, Epoch: 582, train_loss: 1.865504288379057, valid_loss: 2.585325629622848\n",
            "test_ind: 3, Epoch: 583, train_loss: 1.9200706305327238, valid_loss: 2.000715149773492\n",
            "test_ind: 3, Epoch: 584, train_loss: 1.7179119204297477, valid_loss: 1.7161383275632502\n",
            "test_ind: 3, Epoch: 585, train_loss: 2.0360523683053473, valid_loss: 2.079496383666992\n",
            "test_ind: 3, Epoch: 586, train_loss: 2.056287718407902, valid_loss: 1.8188241676047996\n",
            "test_ind: 3, Epoch: 587, train_loss: 1.8412618813691313, valid_loss: 1.5943876725656017\n",
            "test_ind: 3, Epoch: 588, train_loss: 1.7476783269717369, valid_loss: 1.7995056046379936\n",
            "test_ind: 3, Epoch: 589, train_loss: 1.9057988767270688, valid_loss: 1.7065665633590135\n",
            "test_ind: 3, Epoch: 590, train_loss: 1.8735859482376664, valid_loss: 2.0246602164374456\n",
            "test_ind: 3, Epoch: 591, train_loss: 2.0385259522332086, valid_loss: 2.543031851450602\n",
            "test_ind: 3, Epoch: 592, train_loss: 1.8525389506493084, valid_loss: 1.7068683129769784\n",
            "test_ind: 3, Epoch: 593, train_loss: 1.7921092363051425, valid_loss: 1.5468979411655004\n",
            "test_ind: 3, Epoch: 594, train_loss: 1.919754999655264, valid_loss: 1.7050684999536583\n",
            "test_ind: 3, Epoch: 595, train_loss: 1.9063964890845029, valid_loss: 1.7476190461052787\n",
            "test_ind: 3, Epoch: 596, train_loss: 1.912379241284029, valid_loss: 1.8297467231750488\n",
            "test_ind: 3, Epoch: 597, train_loss: 1.9871044512148257, valid_loss: 2.1571124218128346\n",
            "test_ind: 3, Epoch: 598, train_loss: 1.9218475377118147, valid_loss: 1.564323902130127\n",
            "test_ind: 3, Epoch: 599, train_loss: 1.8478575459233038, valid_loss: 1.6691377604449238\n",
            "test_ind: 3, Epoch: 600, train_loss: 1.7684412061432262, valid_loss: 1.921078575981988\n",
            "test_ind: 3, Epoch: 601, train_loss: 1.7740557929615914, valid_loss: 1.8753153306466561\n",
            "test_ind: 3, Epoch: 602, train_loss: 1.8405951747187863, valid_loss: 2.141098976135254\n",
            "test_ind: 3, Epoch: 603, train_loss: 1.8870421751045887, valid_loss: 1.8657652890240706\n",
            "test_ind: 3, Epoch: 604, train_loss: 1.8679740634965307, valid_loss: 1.584915938200774\n",
            "test_ind: 3, Epoch: 605, train_loss: 1.8848494718104234, valid_loss: 2.487372045163755\n",
            "test_ind: 3, Epoch: 606, train_loss: 1.8089098577146179, valid_loss: 1.5715771427860967\n",
            "test_ind: 3, Epoch: 607, train_loss: 1.8433176205482014, valid_loss: 1.7162677093788432\n",
            "test_ind: 3, Epoch: 608, train_loss: 2.060389601154092, valid_loss: 2.9077963652434176\n",
            "test_ind: 3, Epoch: 609, train_loss: 1.8935856230464982, valid_loss: 1.7162302158496998\n",
            "test_ind: 3, Epoch: 610, train_loss: 1.816070886305821, valid_loss: 1.8132741009747542\n",
            "test_ind: 3, Epoch: 611, train_loss: 1.9944968282440563, valid_loss: 1.9069178369310167\n",
            "test_ind: 3, Epoch: 612, train_loss: 1.765424934434302, valid_loss: 1.5361336602105033\n",
            "test_ind: 3, Epoch: 613, train_loss: 1.892767076139097, valid_loss: 1.610860135820177\n",
            "test_ind: 3, Epoch: 614, train_loss: 2.0959281273830084, valid_loss: 2.3641034762064614\n",
            "test_ind: 3, Epoch: 615, train_loss: 1.8169771947978453, valid_loss: 1.6516852202238859\n",
            "test_ind: 3, Epoch: 616, train_loss: 1.8402668870525596, valid_loss: 1.679764253121835\n",
            "test_ind: 3, Epoch: 617, train_loss: 1.8766562261699158, valid_loss: 1.6569254133436413\n",
            "test_ind: 3, Epoch: 618, train_loss: 1.7967090959902159, valid_loss: 1.8285295345165111\n",
            "test_ind: 3, Epoch: 619, train_loss: 1.8487076994813523, valid_loss: 1.8034937293441207\n",
            "test_ind: 3, Epoch: 620, train_loss: 1.8160582059695398, valid_loss: 1.9476525342022932\n",
            "test_ind: 3, Epoch: 621, train_loss: 1.7626471342863859, valid_loss: 1.5376655437328197\n",
            "test_ind: 3, Epoch: 622, train_loss: 1.932843691037025, valid_loss: 1.8065085411071777\n",
            "test_ind: 3, Epoch: 623, train_loss: 1.904980488765387, valid_loss: 2.567759955370868\n",
            "test_ind: 3, Epoch: 624, train_loss: 1.8757125301125608, valid_loss: 2.0117071116412126\n",
            "test_ind: 3, Epoch: 625, train_loss: 1.8866368458594802, valid_loss: 1.7475665763572412\n",
            "test_ind: 3, Epoch: 626, train_loss: 1.7764192157321503, valid_loss: 2.2219126842640065\n",
            "test_ind: 3, Epoch: 627, train_loss: 1.803842880107738, valid_loss: 1.638315271448206\n",
            "test_ind: 3, Epoch: 628, train_loss: 1.8944223250871821, valid_loss: 1.7191784116956923\n",
            "test_ind: 3, Epoch: 629, train_loss: 1.7260659241381986, valid_loss: 2.8512938110916703\n",
            "test_ind: 3, Epoch: 630, train_loss: 1.979442873118836, valid_loss: 2.538763311174181\n",
            "test_ind: 3, Epoch: 631, train_loss: 1.8358659508787556, valid_loss: 1.8617981628135398\n",
            "test_ind: 3, Epoch: 632, train_loss: 1.8636563618977866, valid_loss: 1.5678939289516873\n",
            "test_ind: 3, Epoch: 633, train_loss: 1.9092421708283605, valid_loss: 1.829050134729456\n",
            "test_ind: 3, Epoch: 634, train_loss: 1.8346885163107036, valid_loss: 2.0021542796382197\n",
            "test_ind: 3, Epoch: 635, train_loss: 1.8098416681642884, valid_loss: 1.8230527418631095\n",
            "test_ind: 3, Epoch: 636, train_loss: 1.8326276378867066, valid_loss: 1.496168507470025\n",
            "Validation loss decreased (1.5060371822781033 --> 1.496168507470025).  Saving model ...\n",
            "test_ind: 3, Epoch: 637, train_loss: 1.7378585073682997, valid_loss: 1.4863454147621438\n",
            "Validation loss decreased (1.496168507470025 --> 1.4863454147621438).  Saving model ...\n",
            "test_ind: 3, Epoch: 638, train_loss: 1.952478402926598, valid_loss: 1.5674627445362233\n",
            "test_ind: 3, Epoch: 639, train_loss: 1.8105270008981962, valid_loss: 1.682019074757894\n",
            "test_ind: 3, Epoch: 640, train_loss: 1.9013310538397896, valid_loss: 2.115104639971698\n",
            "test_ind: 3, Epoch: 641, train_loss: 1.7698210374808605, valid_loss: 1.731763203938802\n",
            "test_ind: 3, Epoch: 642, train_loss: 1.7938824641851732, valid_loss: 1.6187369735152632\n",
            "test_ind: 3, Epoch: 643, train_loss: 1.9346418145262163, valid_loss: 3.1043114132351346\n",
            "test_ind: 3, Epoch: 644, train_loss: 1.7588594931143302, valid_loss: 1.7865400314331055\n",
            "test_ind: 3, Epoch: 645, train_loss: 1.994376865434058, valid_loss: 1.5367357465955946\n",
            "test_ind: 3, Epoch: 646, train_loss: 1.766184618443619, valid_loss: 1.7585204972161186\n",
            "test_ind: 3, Epoch: 647, train_loss: 1.8005022708280585, valid_loss: 1.5464537232010453\n",
            "test_ind: 3, Epoch: 648, train_loss: 1.6780088801442843, valid_loss: 1.573233021630181\n",
            "test_ind: 3, Epoch: 649, train_loss: 1.9708930239265348, valid_loss: 2.2682067729808666\n",
            "test_ind: 3, Epoch: 650, train_loss: 1.765874727272693, valid_loss: 1.8947035294991952\n",
            "test_ind: 3, Epoch: 651, train_loss: 1.6927439371744792, valid_loss: 2.0104884218286587\n",
            "test_ind: 3, Epoch: 652, train_loss: 1.8431322250837163, valid_loss: 1.9689110296743886\n",
            "test_ind: 3, Epoch: 653, train_loss: 1.8282477296428916, valid_loss: 1.6435937175044306\n",
            "test_ind: 3, Epoch: 654, train_loss: 1.765343142144474, valid_loss: 1.9260921831484195\n",
            "test_ind: 3, Epoch: 655, train_loss: 1.8157707673531993, valid_loss: 1.8946525785658093\n",
            "test_ind: 3, Epoch: 656, train_loss: 1.7360996552455572, valid_loss: 2.028995390291567\n",
            "test_ind: 3, Epoch: 657, train_loss: 1.7642358791680983, valid_loss: 1.7967502805921765\n",
            "test_ind: 3, Epoch: 658, train_loss: 1.7758475998301566, valid_loss: 1.724774378317374\n",
            "test_ind: 3, Epoch: 659, train_loss: 1.9590324060416515, valid_loss: 1.7720919185214572\n",
            "test_ind: 3, Epoch: 660, train_loss: 1.8587254182792006, valid_loss: 2.860314298559118\n",
            "test_ind: 3, Epoch: 661, train_loss: 1.8431708194591383, valid_loss: 1.7988854690834328\n",
            "test_ind: 3, Epoch: 662, train_loss: 1.905642374062244, valid_loss: 1.7235863650286638\n",
            "test_ind: 3, Epoch: 663, train_loss: 1.6845763995323653, valid_loss: 1.9244149172747576\n",
            "test_ind: 3, Epoch: 664, train_loss: 1.8889561700232236, valid_loss: 1.852046401412399\n",
            "test_ind: 3, Epoch: 665, train_loss: 1.8979697698428308, valid_loss: 2.0275848883169667\n",
            "test_ind: 3, Epoch: 666, train_loss: 1.9185349852950482, valid_loss: 2.5580423143174915\n",
            "test_ind: 3, Epoch: 667, train_loss: 1.8068751876736864, valid_loss: 1.8562279807196722\n",
            "test_ind: 3, Epoch: 668, train_loss: 1.7867731989165885, valid_loss: 1.482824272579617\n",
            "Validation loss decreased (1.4863454147621438 --> 1.482824272579617).  Saving model ...\n",
            "test_ind: 3, Epoch: 669, train_loss: 1.756509492426743, valid_loss: 1.8024432747452346\n",
            "test_ind: 3, Epoch: 670, train_loss: 1.7123184439576704, valid_loss: 1.7750574041295935\n",
            "test_ind: 3, Epoch: 671, train_loss: 1.9390750637760867, valid_loss: 2.1916343900892468\n",
            "test_ind: 3, Epoch: 672, train_loss: 1.8658914389433683, valid_loss: 1.9274793907448096\n",
            "test_ind: 3, Epoch: 673, train_loss: 2.0844234360588922, valid_loss: 2.1274877477575234\n",
            "test_ind: 3, Epoch: 674, train_loss: 1.7559171900337125, valid_loss: 1.869932527895327\n",
            "test_ind: 3, Epoch: 675, train_loss: 1.7449888123406305, valid_loss: 1.6740169701752838\n",
            "test_ind: 3, Epoch: 676, train_loss: 1.825752217092632, valid_loss: 1.5193475087483723\n",
            "test_ind: 3, Epoch: 677, train_loss: 1.831837765964461, valid_loss: 1.5230503965307165\n",
            "test_ind: 3, Epoch: 678, train_loss: 1.7438895143108606, valid_loss: 1.6695034592239946\n",
            "test_ind: 3, Epoch: 679, train_loss: 1.8110980634336116, valid_loss: 1.9396418465508356\n",
            "test_ind: 3, Epoch: 680, train_loss: 1.8452508596726407, valid_loss: 1.5243870240670665\n",
            "test_ind: 3, Epoch: 681, train_loss: 1.7781776557733981, valid_loss: 1.8685074558964483\n",
            "test_ind: 3, Epoch: 682, train_loss: 1.776121528060348, valid_loss: 1.4833536148071291\n",
            "test_ind: 3, Epoch: 683, train_loss: 1.7182218881300937, valid_loss: 1.6555940839979382\n",
            "test_ind: 3, Epoch: 684, train_loss: 1.9651526580622167, valid_loss: 1.851115350370054\n",
            "test_ind: 3, Epoch: 685, train_loss: 1.7746280740808555, valid_loss: 1.9672743302804454\n",
            "test_ind: 3, Epoch: 686, train_loss: 1.8003693568853685, valid_loss: 1.6286768560056333\n",
            "test_ind: 3, Epoch: 687, train_loss: 1.8318155371112586, valid_loss: 1.4694176603246618\n",
            "Validation loss decreased (1.482824272579617 --> 1.4694176603246618).  Saving model ...\n",
            "test_ind: 3, Epoch: 688, train_loss: 1.8278738245551969, valid_loss: 1.9929374588860407\n",
            "test_ind: 3, Epoch: 689, train_loss: 1.8259822998517827, valid_loss: 1.7983245849609375\n",
            "test_ind: 3, Epoch: 690, train_loss: 1.9012692887106057, valid_loss: 1.6569615293432165\n",
            "test_ind: 3, Epoch: 691, train_loss: 1.9264443538807057, valid_loss: 1.4690238458138924\n",
            "Validation loss decreased (1.4694176603246618 --> 1.4690238458138924).  Saving model ...\n",
            "test_ind: 3, Epoch: 692, train_loss: 2.037465660660355, valid_loss: 2.0882887840270996\n",
            "test_ind: 3, Epoch: 693, train_loss: 2.1025157151398837, valid_loss: 2.3059721522861056\n",
            "test_ind: 3, Epoch: 694, train_loss: 1.7865692833323537, valid_loss: 1.759898962797942\n",
            "test_ind: 3, Epoch: 695, train_loss: 1.8811866560100037, valid_loss: 1.986391791590938\n",
            "test_ind: 3, Epoch: 696, train_loss: 1.8560210392798908, valid_loss: 1.8981369866265192\n",
            "test_ind: 3, Epoch: 697, train_loss: 1.7423917629100656, valid_loss: 2.007507977662263\n",
            "test_ind: 3, Epoch: 698, train_loss: 1.8029139895498016, valid_loss: 2.3214127399303295\n",
            "test_ind: 3, Epoch: 699, train_loss: 1.6713513503839939, valid_loss: 1.635765375914397\n",
            "test_ind: 3, Epoch: 700, train_loss: 1.739607192851879, valid_loss: 1.7120936181810167\n",
            "test_ind: 3, Epoch: 701, train_loss: 1.7909188093962491, valid_loss: 1.6542774659615975\n",
            "test_ind: 3, Epoch: 702, train_loss: 2.0987569137855813, valid_loss: 2.522853957282172\n",
            "test_ind: 3, Epoch: 703, train_loss: 1.7521495230403945, valid_loss: 1.8026775430749964\n",
            "test_ind: 3, Epoch: 704, train_loss: 1.6764987957330397, valid_loss: 1.9037517971462674\n",
            "test_ind: 3, Epoch: 705, train_loss: 1.9141817387239433, valid_loss: 1.6411969396803112\n",
            "test_ind: 3, Epoch: 706, train_loss: 1.888294072798741, valid_loss: 1.6317803771407515\n",
            "test_ind: 3, Epoch: 707, train_loss: 1.7956241619439774, valid_loss: 1.6636722352769637\n",
            "test_ind: 3, Epoch: 708, train_loss: 1.7136901101948303, valid_loss: 1.8237590083369501\n",
            "test_ind: 3, Epoch: 709, train_loss: 1.7100100517272951, valid_loss: 1.5806673367818198\n",
            "test_ind: 3, Epoch: 710, train_loss: 1.867073176819601, valid_loss: 1.9784590050026223\n",
            "test_ind: 3, Epoch: 711, train_loss: 2.066819108562705, valid_loss: 1.877263863881429\n",
            "test_ind: 3, Epoch: 712, train_loss: 1.7327567383095068, valid_loss: 1.6778643925984698\n",
            "test_ind: 3, Epoch: 713, train_loss: 1.631868580241262, valid_loss: 1.701467990875244\n",
            "test_ind: 3, Epoch: 714, train_loss: 1.7502190389750918, valid_loss: 1.5630658820823387\n",
            "test_ind: 3, Epoch: 715, train_loss: 1.8472554713119695, valid_loss: 1.6573410387392395\n",
            "test_ind: 3, Epoch: 716, train_loss: 1.8314676520265176, valid_loss: 1.506923587233932\n",
            "test_ind: 3, Epoch: 717, train_loss: 1.6965713500976565, valid_loss: 1.632992567839446\n",
            "test_ind: 3, Epoch: 718, train_loss: 1.692141674183033, valid_loss: 1.7029779575489186\n",
            "test_ind: 3, Epoch: 719, train_loss: 1.8463749826690297, valid_loss: 1.6968757488109447\n",
            "test_ind: 3, Epoch: 720, train_loss: 1.8405989482079022, valid_loss: 1.557282659742567\n",
            "test_ind: 3, Epoch: 721, train_loss: 1.7662538952297635, valid_loss: 1.938096558606183\n",
            "test_ind: 3, Epoch: 722, train_loss: 1.925594965616862, valid_loss: 2.0770919587877064\n",
            "test_ind: 3, Epoch: 723, train_loss: 1.8167535699444053, valid_loss: 1.802155512350577\n",
            "test_ind: 3, Epoch: 724, train_loss: 1.7619389251426416, valid_loss: 1.9314807079456469\n",
            "test_ind: 3, Epoch: 725, train_loss: 1.6311833593580456, valid_loss: 1.4576490543506764\n",
            "Validation loss decreased (1.4690238458138924 --> 1.4576490543506764).  Saving model ...\n",
            "test_ind: 3, Epoch: 726, train_loss: 1.7493841559798626, valid_loss: 1.7031506079214591\n",
            "test_ind: 3, Epoch: 727, train_loss: 1.703110765527796, valid_loss: 1.5999285909864638\n",
            "test_ind: 3, Epoch: 728, train_loss: 1.8344157772299685, valid_loss: 1.687463195235641\n",
            "test_ind: 3, Epoch: 729, train_loss: 2.0272621755246765, valid_loss: 2.07886067143193\n",
            "test_ind: 3, Epoch: 730, train_loss: 1.7441341023386263, valid_loss: 1.5783735557838723\n",
            "test_ind: 3, Epoch: 731, train_loss: 1.8134417239530587, valid_loss: 2.3650538656446667\n",
            "test_ind: 3, Epoch: 732, train_loss: 1.761277875782531, valid_loss: 1.614048233738652\n",
            "test_ind: 3, Epoch: 733, train_loss: 1.6460694089347936, valid_loss: 1.4965556992424858\n",
            "test_ind: 3, Epoch: 734, train_loss: 1.862122494497417, valid_loss: 1.6112459500630698\n",
            "test_ind: 3, Epoch: 735, train_loss: 1.6997890531280893, valid_loss: 1.5465490023295083\n",
            "test_ind: 3, Epoch: 736, train_loss: 1.6859601986261061, valid_loss: 1.411822283709491\n",
            "Validation loss decreased (1.4576490543506764 --> 1.411822283709491).  Saving model ...\n",
            "test_ind: 3, Epoch: 737, train_loss: 1.6355383248976718, valid_loss: 1.8741923791390878\n",
            "test_ind: 3, Epoch: 738, train_loss: 1.8175618442488306, valid_loss: 2.1759960033275463\n",
            "test_ind: 3, Epoch: 739, train_loss: 1.9113176545979065, valid_loss: 1.7435929333722149\n",
            "test_ind: 3, Epoch: 740, train_loss: 1.601711373270294, valid_loss: 1.6428524123297799\n",
            "test_ind: 3, Epoch: 741, train_loss: 1.8019487592909074, valid_loss: 1.7200204884564436\n",
            "test_ind: 3, Epoch: 742, train_loss: 1.6928037360862447, valid_loss: 2.1118193202548556\n",
            "test_ind: 3, Epoch: 743, train_loss: 1.795327074733781, valid_loss: 1.9502485063340926\n",
            "test_ind: 3, Epoch: 744, train_loss: 1.975724979683205, valid_loss: 1.888332826119882\n",
            "test_ind: 3, Epoch: 745, train_loss: 1.700530617325394, valid_loss: 1.5782579316033258\n",
            "test_ind: 3, Epoch: 746, train_loss: 1.5834802639337233, valid_loss: 2.0060375884727195\n",
            "test_ind: 3, Epoch: 747, train_loss: 1.7535203533408084, valid_loss: 1.8513529388992875\n",
            "test_ind: 3, Epoch: 748, train_loss: 1.626349761162275, valid_loss: 1.5368885110925745\n",
            "test_ind: 3, Epoch: 749, train_loss: 1.6768593729278187, valid_loss: 1.4589721715008772\n",
            "test_ind: 3, Epoch: 750, train_loss: 1.7663030330045726, valid_loss: 1.7544286692584001\n",
            "test_ind: 3, Epoch: 751, train_loss: 2.0253801110350054, valid_loss: 1.7465654302526403\n",
            "test_ind: 3, Epoch: 752, train_loss: 1.8792871545862269, valid_loss: 2.2204098348264343\n",
            "test_ind: 3, Epoch: 753, train_loss: 1.8216263158821766, valid_loss: 1.7952947263364438\n",
            "test_ind: 3, Epoch: 754, train_loss: 1.695147908764121, valid_loss: 1.4816317911501284\n",
            "test_ind: 3, Epoch: 755, train_loss: 1.6497095367054881, valid_loss: 1.7112824651930068\n",
            "test_ind: 3, Epoch: 756, train_loss: 1.727953357461058, valid_loss: 1.3346094908537687\n",
            "Validation loss decreased (1.411822283709491 --> 1.3346094908537687).  Saving model ...\n",
            "test_ind: 3, Epoch: 757, train_loss: 1.7504844312314636, valid_loss: 1.5788648746631764\n",
            "test_ind: 3, Epoch: 758, train_loss: 1.7472204161278997, valid_loss: 1.649041935249611\n",
            "test_ind: 3, Epoch: 759, train_loss: 1.596602180857717, valid_loss: 1.7131995271753382\n",
            "test_ind: 3, Epoch: 760, train_loss: 1.7223662093833638, valid_loss: 1.4676021646570276\n",
            "test_ind: 3, Epoch: 761, train_loss: 1.6956049012549133, valid_loss: 1.4971622007864491\n",
            "test_ind: 3, Epoch: 762, train_loss: 1.658603991991208, valid_loss: 1.432045406765408\n",
            "test_ind: 3, Epoch: 763, train_loss: 1.7682003916045768, valid_loss: 1.8280954360961912\n",
            "test_ind: 3, Epoch: 764, train_loss: 1.6450561947292752, valid_loss: 1.515752085932979\n",
            "test_ind: 3, Epoch: 765, train_loss: 1.8211854298909504, valid_loss: 2.0706554165592896\n",
            "test_ind: 3, Epoch: 766, train_loss: 1.6617246910377783, valid_loss: 1.6601828469170465\n",
            "test_ind: 3, Epoch: 767, train_loss: 1.7282002237108018, valid_loss: 1.6086157869409632\n",
            "test_ind: 3, Epoch: 768, train_loss: 1.7830016524703414, valid_loss: 2.5164172737686723\n",
            "test_ind: 3, Epoch: 769, train_loss: 1.8100878927442765, valid_loss: 1.5321588692841708\n",
            "test_ind: 3, Epoch: 770, train_loss: 1.8661470766420718, valid_loss: 2.0331575958817094\n",
            "test_ind: 3, Epoch: 771, train_loss: 1.7382984220245736, valid_loss: 1.8721639845106335\n",
            "test_ind: 3, Epoch: 772, train_loss: 1.7541380517276717, valid_loss: 1.6855730127405237\n",
            "test_ind: 3, Epoch: 773, train_loss: 1.7704313654958466, valid_loss: 1.8830268294722945\n",
            "test_ind: 3, Epoch: 774, train_loss: 1.6922814345654145, valid_loss: 1.5520525685063116\n",
            "test_ind: 3, Epoch: 775, train_loss: 1.9304905173219278, valid_loss: 1.977154466840956\n",
            "test_ind: 3, Epoch: 776, train_loss: 1.7099564693592209, valid_loss: 1.6291018591986763\n",
            "test_ind: 3, Epoch: 777, train_loss: 1.617499887207408, valid_loss: 1.556372501232006\n",
            "test_ind: 3, Epoch: 778, train_loss: 1.629036291145984, valid_loss: 1.6430446483470775\n",
            "test_ind: 3, Epoch: 779, train_loss: 1.5642531006424516, valid_loss: 1.8384826624834978\n",
            "test_ind: 3, Epoch: 780, train_loss: 1.6958804836979617, valid_loss: 1.5627099496346932\n",
            "test_ind: 3, Epoch: 781, train_loss: 1.601165889221945, valid_loss: 1.7141823238796658\n",
            "test_ind: 3, Epoch: 782, train_loss: 1.682121335724254, valid_loss: 1.646614357277199\n",
            "test_ind: 3, Epoch: 783, train_loss: 1.7214365064361952, valid_loss: 1.8138216336568198\n",
            "test_ind: 3, Epoch: 784, train_loss: 1.6316717760062511, valid_loss: 1.7587635782029891\n",
            "test_ind: 3, Epoch: 785, train_loss: 1.7240385126184534, valid_loss: 1.5293079482184515\n",
            "test_ind: 3, Epoch: 786, train_loss: 1.6988392641514907, valid_loss: 1.6101924048529732\n",
            "test_ind: 3, Epoch: 787, train_loss: 1.715815355748306, valid_loss: 1.570151929502134\n",
            "test_ind: 3, Epoch: 788, train_loss: 1.915558085029508, valid_loss: 1.5355141427781849\n",
            "test_ind: 3, Epoch: 789, train_loss: 1.8414313175060129, valid_loss: 2.287428696950277\n",
            "test_ind: 3, Epoch: 790, train_loss: 1.6697370446758506, valid_loss: 1.864846494462755\n",
            "test_ind: 3, Epoch: 791, train_loss: 1.6309075885348852, valid_loss: 1.4461247302867748\n",
            "test_ind: 3, Epoch: 792, train_loss: 1.5581963503802263, valid_loss: 1.4391667048136392\n",
            "test_ind: 3, Epoch: 793, train_loss: 1.9423933853337796, valid_loss: 2.6715134691309044\n",
            "test_ind: 3, Epoch: 794, train_loss: 1.611806192515809, valid_loss: 1.5501308970981174\n",
            "test_ind: 3, Epoch: 795, train_loss: 1.7031234635247126, valid_loss: 1.9369417296515572\n",
            "test_ind: 3, Epoch: 796, train_loss: 1.7444158071353113, valid_loss: 1.6689297888014052\n",
            "test_ind: 3, Epoch: 797, train_loss: 1.8116528840712554, valid_loss: 1.8768105153684265\n",
            "test_ind: 3, Epoch: 798, train_loss: 1.8882802680686668, valid_loss: 1.8251899436668113\n",
            "test_ind: 3, Epoch: 799, train_loss: 1.934976777912658, valid_loss: 1.6284157435099285\n",
            "test_ind: 3, Epoch: 800, train_loss: 1.6405179294539085, valid_loss: 1.4637428919474285\n",
            "test_ind: 3, Epoch: 801, train_loss: 1.7320777045355902, valid_loss: 1.5069184479890048\n",
            "test_ind: 3, Epoch: 802, train_loss: 1.7527831630942263, valid_loss: 1.5575498298362451\n",
            "test_ind: 3, Epoch: 803, train_loss: 1.6394717958238387, valid_loss: 1.7317815709997104\n",
            "test_ind: 3, Epoch: 804, train_loss: 1.560435919114101, valid_loss: 1.6472288767496743\n",
            "test_ind: 3, Epoch: 805, train_loss: 1.6293395890129936, valid_loss: 1.6370564566718206\n",
            "test_ind: 3, Epoch: 806, train_loss: 1.7066537303689084, valid_loss: 1.5283489757113986\n",
            "test_ind: 3, Epoch: 807, train_loss: 1.6549416177066758, valid_loss: 1.5604220849496346\n",
            "test_ind: 3, Epoch: 808, train_loss: 1.7752430115217042, valid_loss: 1.56366056866116\n",
            "test_ind: 3, Epoch: 809, train_loss: 1.785786163659743, valid_loss: 2.170090428105107\n",
            "test_ind: 3, Epoch: 810, train_loss: 1.6216750498171206, valid_loss: 1.6663572876541703\n",
            "test_ind: 3, Epoch: 811, train_loss: 1.6336901452806258, valid_loss: 1.635021827839039\n",
            "test_ind: 3, Epoch: 812, train_loss: 1.6240412570812086, valid_loss: 1.5249805097226743\n",
            "test_ind: 3, Epoch: 813, train_loss: 1.882886392098886, valid_loss: 1.6447169692428023\n",
            "test_ind: 3, Epoch: 814, train_loss: 1.690565244651135, valid_loss: 1.8275039107711226\n",
            "test_ind: 3, Epoch: 815, train_loss: 1.732210600817645, valid_loss: 1.5718782742818196\n",
            "test_ind: 3, Epoch: 816, train_loss: 1.6245799064636228, valid_loss: 1.7876575611255787\n",
            "test_ind: 3, Epoch: 817, train_loss: 1.63021116492189, valid_loss: 1.4150314507661041\n",
            "test_ind: 3, Epoch: 818, train_loss: 1.630883440559293, valid_loss: 1.6007326620596425\n",
            "test_ind: 3, Epoch: 819, train_loss: 1.6936274928811157, valid_loss: 1.6252298178496185\n",
            "test_ind: 3, Epoch: 820, train_loss: 1.6644777192009816, valid_loss: 1.4173614713880751\n",
            "test_ind: 3, Epoch: 821, train_loss: 1.6680856457463016, valid_loss: 1.6959369624102556\n",
            "test_ind: 3, Epoch: 822, train_loss: 1.6615498566333158, valid_loss: 1.4546371036105685\n",
            "test_ind: 3, Epoch: 823, train_loss: 1.7044648888670366, valid_loss: 2.286693184464066\n",
            "test_ind: 3, Epoch: 824, train_loss: 1.5629271989987221, valid_loss: 1.4182197429515697\n",
            "test_ind: 3, Epoch: 825, train_loss: 1.6662633154127333, valid_loss: 1.696272956000434\n",
            "test_ind: 3, Epoch: 826, train_loss: 1.7953716207433632, valid_loss: 1.4010086589389377\n",
            "test_ind: 3, Epoch: 827, train_loss: 1.58191869288315, valid_loss: 1.6368957272282354\n",
            "test_ind: 3, Epoch: 828, train_loss: 1.7081757357091076, valid_loss: 2.0129541291130915\n",
            "test_ind: 3, Epoch: 829, train_loss: 1.5957606044816381, valid_loss: 1.85176220646611\n",
            "test_ind: 3, Epoch: 830, train_loss: 1.7684534685111342, valid_loss: 2.2579535378350153\n",
            "test_ind: 3, Epoch: 831, train_loss: 1.8979013819753385, valid_loss: 1.653730374795419\n",
            "test_ind: 3, Epoch: 832, train_loss: 1.7084330864894537, valid_loss: 1.4739425447252061\n",
            "test_ind: 3, Epoch: 833, train_loss: 1.6444460138862516, valid_loss: 1.8383194958722153\n",
            "test_ind: 3, Epoch: 834, train_loss: 1.6960309699729637, valid_loss: 1.7321003454702872\n",
            "test_ind: 3, Epoch: 835, train_loss: 1.617925626260263, valid_loss: 1.4393197695414224\n",
            "test_ind: 3, Epoch: 836, train_loss: 1.6264879438612196, valid_loss: 1.4406504101223416\n",
            "test_ind: 3, Epoch: 837, train_loss: 1.7346536436198672, valid_loss: 1.6315747190404821\n",
            "test_ind: 3, Epoch: 838, train_loss: 1.6078440878126354, valid_loss: 1.568670467094139\n",
            "test_ind: 3, Epoch: 839, train_loss: 1.7382437682446141, valid_loss: 1.6433946468211986\n",
            "test_ind: 3, Epoch: 840, train_loss: 1.6933249956295813, valid_loss: 1.4215029257315177\n",
            "test_ind: 3, Epoch: 841, train_loss: 1.6552699465810516, valid_loss: 1.7235499664589211\n",
            "test_ind: 3, Epoch: 842, train_loss: 1.6258656595960075, valid_loss: 1.4979404166892722\n",
            "test_ind: 3, Epoch: 843, train_loss: 1.678564660343123, valid_loss: 1.7910695605807834\n",
            "test_ind: 3, Epoch: 844, train_loss: 1.7098031220612702, valid_loss: 1.7962013173986364\n",
            "test_ind: 3, Epoch: 845, train_loss: 1.5469230016072595, valid_loss: 1.409709171012596\n",
            "test_ind: 3, Epoch: 846, train_loss: 1.6706873575846353, valid_loss: 1.4367303848266602\n",
            "test_ind: 3, Epoch: 847, train_loss: 1.589539663291272, valid_loss: 1.4829130879154913\n",
            "test_ind: 3, Epoch: 848, train_loss: 1.6178106260888367, valid_loss: 1.6102410069218387\n",
            "test_ind: 3, Epoch: 849, train_loss: 1.8885372244281533, valid_loss: 1.417287861859357\n",
            "test_ind: 3, Epoch: 850, train_loss: 1.4995689274352273, valid_loss: 1.5430000623067222\n",
            "test_ind: 3, Epoch: 851, train_loss: 1.6060102074234577, valid_loss: 1.5467323550471554\n",
            "test_ind: 3, Epoch: 852, train_loss: 1.8026083369313937, valid_loss: 1.8094174420392073\n",
            "test_ind: 3, Epoch: 853, train_loss: 1.5982469805964719, valid_loss: 1.6340495215521917\n",
            "test_ind: 3, Epoch: 854, train_loss: 1.6821348461104029, valid_loss: 2.0329956478542752\n",
            "test_ind: 3, Epoch: 855, train_loss: 1.7018400415962125, valid_loss: 1.64940576200132\n",
            "test_ind: 3, Epoch: 856, train_loss: 1.6021166966285234, valid_loss: 1.6883386152761952\n",
            "test_ind: 3, Epoch: 857, train_loss: 1.6118219987845714, valid_loss: 1.6152334389863192\n",
            "test_ind: 3, Epoch: 858, train_loss: 1.669606038081793, valid_loss: 1.5270423889160154\n",
            "test_ind: 3, Epoch: 859, train_loss: 1.7244657822597174, valid_loss: 1.5335916766413935\n",
            "test_ind: 3, Epoch: 860, train_loss: 1.6179980289788896, valid_loss: 1.5738829330161765\n",
            "test_ind: 3, Epoch: 861, train_loss: 1.6966460486988966, valid_loss: 1.4277560269391096\n",
            "test_ind: 3, Epoch: 862, train_loss: 1.5991184682021906, valid_loss: 1.3797933260599773\n",
            "test_ind: 3, Epoch: 863, train_loss: 1.838748237233103, valid_loss: 2.571220486252396\n",
            "test_ind: 3, Epoch: 864, train_loss: 1.5999110304279092, valid_loss: 1.553816141905608\n",
            "test_ind: 3, Epoch: 865, train_loss: 1.60194206237793, valid_loss: 1.6569529639350042\n",
            "test_ind: 3, Epoch: 866, train_loss: 1.6928837740862812, valid_loss: 1.8950006520306621\n",
            "test_ind: 3, Epoch: 867, train_loss: 1.597694467615198, valid_loss: 1.572152473308422\n",
            "test_ind: 3, Epoch: 868, train_loss: 1.532751707383144, valid_loss: 1.7497412010475442\n",
            "test_ind: 3, Epoch: 869, train_loss: 1.5856728553771973, valid_loss: 1.8262198059647172\n",
            "test_ind: 3, Epoch: 870, train_loss: 1.5371523668736586, valid_loss: 1.3946748662877966\n",
            "test_ind: 3, Epoch: 871, train_loss: 1.5087426209155423, valid_loss: 1.5679571716873733\n",
            "test_ind: 3, Epoch: 872, train_loss: 1.5484200995645405, valid_loss: 1.4725817574395075\n",
            "test_ind: 3, Epoch: 873, train_loss: 1.6026315806824485, valid_loss: 1.5099238642939816\n",
            "test_ind: 3, Epoch: 874, train_loss: 1.678488902103754, valid_loss: 1.439317738568341\n",
            "test_ind: 3, Epoch: 875, train_loss: 1.6992185675067666, valid_loss: 1.6561052181102611\n",
            "test_ind: 3, Epoch: 876, train_loss: 1.6285072668099108, valid_loss: 1.7363192946822554\n",
            "test_ind: 3, Epoch: 877, train_loss: 1.675578800248511, valid_loss: 1.8570581718727395\n",
            "test_ind: 3, Epoch: 878, train_loss: 1.7126204467114108, valid_loss: 1.7608145431235984\n",
            "test_ind: 3, Epoch: 879, train_loss: 1.7056253633381406, valid_loss: 1.8977446026272242\n",
            "test_ind: 3, Epoch: 880, train_loss: 1.5384288363986542, valid_loss: 1.3921939885174786\n",
            "test_ind: 3, Epoch: 881, train_loss: 1.5348397831857943, valid_loss: 1.5577147625110768\n",
            "test_ind: 3, Epoch: 882, train_loss: 1.72027149318177, valid_loss: 1.4274063993383337\n",
            "test_ind: 3, Epoch: 883, train_loss: 1.5835392740037704, valid_loss: 1.4537209934658475\n",
            "test_ind: 3, Epoch: 884, train_loss: 1.6392995339852796, valid_loss: 1.7170171031245478\n",
            "test_ind: 3, Epoch: 885, train_loss: 1.4847381144394107, valid_loss: 1.5193097679703325\n",
            "test_ind: 3, Epoch: 886, train_loss: 1.580324367240623, valid_loss: 1.4871854428891782\n",
            "test_ind: 3, Epoch: 887, train_loss: 1.6663599014282224, valid_loss: 1.7464748488532171\n",
            "test_ind: 3, Epoch: 888, train_loss: 1.5158741856798714, valid_loss: 1.9368206659952798\n",
            "test_ind: 3, Epoch: 889, train_loss: 1.6379403008355031, valid_loss: 1.6051473617553709\n",
            "test_ind: 3, Epoch: 890, train_loss: 1.6006832476015445, valid_loss: 1.59298163873178\n",
            "test_ind: 3, Epoch: 891, train_loss: 1.5844843063825444, valid_loss: 1.4798740987424497\n",
            "test_ind: 3, Epoch: 892, train_loss: 1.5978051644784435, valid_loss: 1.5384028929251212\n",
            "test_ind: 3, Epoch: 893, train_loss: 1.8097786667906204, valid_loss: 1.586125091270164\n",
            "test_ind: 3, Epoch: 894, train_loss: 1.5986963554664892, valid_loss: 1.5522207330774378\n",
            "test_ind: 3, Epoch: 895, train_loss: 1.7727608680725098, valid_loss: 1.8884166434959129\n",
            "test_ind: 3, Epoch: 896, train_loss: 1.749292891702534, valid_loss: 2.4950516665423357\n",
            "test_ind: 3, Epoch: 897, train_loss: 1.5737297623245805, valid_loss: 1.6141180462307398\n",
            "test_ind: 3, Epoch: 898, train_loss: 1.527477670598913, valid_loss: 1.7092430150067366\n",
            "test_ind: 3, Epoch: 899, train_loss: 1.5523571438259547, valid_loss: 1.9734373622470431\n",
            "test_ind: 3, Epoch: 900, train_loss: 1.5936448250287847, valid_loss: 1.5914332071940103\n",
            "test_ind: 3, Epoch: 901, train_loss: 1.641733346162019, valid_loss: 1.749607527697528\n",
            "test_ind: 3, Epoch: 902, train_loss: 1.7022645267439478, valid_loss: 2.055350374292444\n",
            "test_ind: 3, Epoch: 903, train_loss: 1.6244466805163726, valid_loss: 1.5337494214375813\n",
            "test_ind: 3, Epoch: 904, train_loss: 1.5290897687276201, valid_loss: 1.5992538487469707\n",
            "test_ind: 3, Epoch: 905, train_loss: 1.5641117390291188, valid_loss: 1.5625600814819336\n",
            "test_ind: 3, Epoch: 906, train_loss: 1.560495982935399, valid_loss: 1.6725034183926053\n",
            "test_ind: 3, Epoch: 907, train_loss: 1.6141484990532013, valid_loss: 1.4448609352111816\n",
            "test_ind: 3, Epoch: 908, train_loss: 1.5915450284510484, valid_loss: 1.5584367469505027\n",
            "test_ind: 3, Epoch: 909, train_loss: 1.561515201756984, valid_loss: 1.4088049994574652\n",
            "test_ind: 3, Epoch: 910, train_loss: 1.5353222481998399, valid_loss: 1.7678451891298648\n",
            "test_ind: 3, Epoch: 911, train_loss: 1.6021316198655116, valid_loss: 1.449960955867061\n",
            "test_ind: 3, Epoch: 912, train_loss: 1.7116639172589339, valid_loss: 2.0404379456131547\n",
            "test_ind: 3, Epoch: 913, train_loss: 1.6040305620358313, valid_loss: 1.3722462300901062\n",
            "test_ind: 3, Epoch: 914, train_loss: 1.5803413508850852, valid_loss: 1.4371543637028448\n",
            "test_ind: 3, Epoch: 915, train_loss: 1.6686132748921711, valid_loss: 1.7861082818773057\n",
            "test_ind: 3, Epoch: 916, train_loss: 1.5667777120331188, valid_loss: 1.4596821113868996\n",
            "test_ind: 3, Epoch: 917, train_loss: 1.6414995016875091, valid_loss: 1.768860728652389\n",
            "test_ind: 3, Epoch: 918, train_loss: 1.6451800723134733, valid_loss: 1.470733501293041\n",
            "test_ind: 3, Epoch: 919, train_loss: 1.6456360875824352, valid_loss: 1.7008805098357025\n",
            "test_ind: 3, Epoch: 920, train_loss: 1.6280208281528799, valid_loss: 1.5042797371193215\n",
            "test_ind: 3, Epoch: 921, train_loss: 1.6966349460460521, valid_loss: 1.8223609924316406\n",
            "test_ind: 3, Epoch: 922, train_loss: 1.5540254380967884, valid_loss: 1.4325196654708297\n",
            "test_ind: 3, Epoch: 923, train_loss: 1.485540254616443, valid_loss: 1.4545171525743275\n",
            "test_ind: 3, Epoch: 924, train_loss: 1.6581770166938687, valid_loss: 1.9588650244253654\n",
            "test_ind: 3, Epoch: 925, train_loss: 1.685392627009639, valid_loss: 1.55589375672517\n",
            "test_ind: 3, Epoch: 926, train_loss: 1.6007747473540128, valid_loss: 1.5258095352737993\n",
            "test_ind: 3, Epoch: 927, train_loss: 1.7066013665846835, valid_loss: 1.660767767164442\n",
            "test_ind: 3, Epoch: 928, train_loss: 1.544716782040066, valid_loss: 1.5754562130680791\n",
            "test_ind: 3, Epoch: 929, train_loss: 1.5077819883087533, valid_loss: 1.52422715999462\n",
            "test_ind: 3, Epoch: 930, train_loss: 1.7051908351756908, valid_loss: 1.742260314800121\n",
            "test_ind: 3, Epoch: 931, train_loss: 1.7020018954335905, valid_loss: 2.1303336355421276\n",
            "test_ind: 3, Epoch: 932, train_loss: 1.5722894315366394, valid_loss: 1.54700356942636\n",
            "test_ind: 3, Epoch: 933, train_loss: 1.6498234419175135, valid_loss: 1.4941197677894875\n",
            "test_ind: 3, Epoch: 934, train_loss: 1.524931866445659, valid_loss: 1.7944404284159343\n",
            "test_ind: 3, Epoch: 935, train_loss: 1.7016712000340593, valid_loss: 1.7963925820809823\n",
            "test_ind: 3, Epoch: 936, train_loss: 1.6231836683956193, valid_loss: 1.529835577364321\n",
            "test_ind: 3, Epoch: 937, train_loss: 1.4677402178446453, valid_loss: 1.6096124825654208\n",
            "test_ind: 3, Epoch: 938, train_loss: 1.53749418258667, valid_loss: 1.6681531976770472\n",
            "test_ind: 3, Epoch: 939, train_loss: 1.5468864676393108, valid_loss: 1.4485829318011247\n",
            "test_ind: 3, Epoch: 940, train_loss: 1.7435713461887692, valid_loss: 1.6381789666635018\n",
            "test_ind: 3, Epoch: 941, train_loss: 1.6372512004993582, valid_loss: 1.477122324484366\n",
            "test_ind: 3, Epoch: 942, train_loss: 1.5808104173636734, valid_loss: 1.5936513830114294\n",
            "test_ind: 3, Epoch: 943, train_loss: 1.501773687056553, valid_loss: 1.3965763515896268\n",
            "test_ind: 3, Epoch: 944, train_loss: 1.5078054534064402, valid_loss: 1.4862637519836426\n",
            "test_ind: 3, Epoch: 945, train_loss: 1.5839620519567421, valid_loss: 1.6013304922315812\n",
            "test_ind: 3, Epoch: 946, train_loss: 1.497869356178943, valid_loss: 1.6156291255244501\n",
            "test_ind: 3, Epoch: 947, train_loss: 1.6417894304534535, valid_loss: 1.6098444020306624\n",
            "test_ind: 3, Epoch: 948, train_loss: 1.5665406356623144, valid_loss: 1.3710234783313893\n",
            "test_ind: 3, Epoch: 949, train_loss: 1.658176757671215, valid_loss: 1.3856048583984375\n",
            "test_ind: 3, Epoch: 950, train_loss: 1.566197000903848, valid_loss: 1.4107188648647733\n",
            "test_ind: 3, Epoch: 951, train_loss: 1.5006057421366372, valid_loss: 1.6410621183889884\n",
            "test_ind: 3, Epoch: 952, train_loss: 1.5220354751304346, valid_loss: 1.4694366101865415\n",
            "test_ind: 3, Epoch: 953, train_loss: 1.425964961817235, valid_loss: 1.3681143124898274\n",
            "test_ind: 3, Epoch: 954, train_loss: 1.592271692958879, valid_loss: 1.5544976305078577\n",
            "test_ind: 3, Epoch: 955, train_loss: 1.696176540704421, valid_loss: 1.4204523298475478\n",
            "test_ind: 3, Epoch: 956, train_loss: 1.512119887787619, valid_loss: 1.4050145502443667\n",
            "test_ind: 3, Epoch: 957, train_loss: 1.530477417839898, valid_loss: 1.4421748408564814\n",
            "test_ind: 3, Epoch: 958, train_loss: 1.5137924677059973, valid_loss: 1.6188985506693523\n",
            "test_ind: 3, Epoch: 959, train_loss: 1.7443655331929524, valid_loss: 1.6623813841078015\n",
            "test_ind: 3, Epoch: 960, train_loss: 1.703846831380585, valid_loss: 1.3753634558783636\n",
            "test_ind: 3, Epoch: 961, train_loss: 1.491650958120087, valid_loss: 1.4889884524875217\n",
            "test_ind: 3, Epoch: 962, train_loss: 1.569141270201883, valid_loss: 2.064832069255688\n",
            "test_ind: 3, Epoch: 963, train_loss: 1.570887447875223, valid_loss: 1.4503262484515154\n",
            "test_ind: 3, Epoch: 964, train_loss: 1.6950816872679153, valid_loss: 1.3812921312120225\n",
            "test_ind: 3, Epoch: 965, train_loss: 1.6544362174140084, valid_loss: 1.4561284559744374\n",
            "test_ind: 3, Epoch: 966, train_loss: 1.57656584256961, valid_loss: 1.7625873177139848\n",
            "test_ind: 3, Epoch: 967, train_loss: 1.6373000086089713, valid_loss: 1.5294481383429632\n",
            "test_ind: 3, Epoch: 968, train_loss: 1.457423404411033, valid_loss: 1.4850086282800745\n",
            "test_ind: 3, Epoch: 969, train_loss: 1.6346624633412303, valid_loss: 1.5749090159380879\n",
            "test_ind: 3, Epoch: 970, train_loss: 1.5911127843974548, valid_loss: 1.4814257798371493\n",
            "test_ind: 3, Epoch: 971, train_loss: 1.550651956487585, valid_loss: 1.4174746760615597\n",
            "test_ind: 3, Epoch: 972, train_loss: 1.4761898488174252, valid_loss: 1.546999207249394\n",
            "test_ind: 3, Epoch: 973, train_loss: 1.50869541403688, valid_loss: 1.524207486046685\n",
            "test_ind: 3, Epoch: 974, train_loss: 1.5671577865694772, valid_loss: 1.4076763612252696\n",
            "test_ind: 3, Epoch: 975, train_loss: 1.69800822528792, valid_loss: 2.200535738909686\n",
            "test_ind: 3, Epoch: 976, train_loss: 1.5542996253496335, valid_loss: 1.5422353038081418\n",
            "test_ind: 3, Epoch: 977, train_loss: 1.5783302048106251, valid_loss: 1.4416874249776206\n",
            "test_ind: 3, Epoch: 978, train_loss: 1.6537845340775854, valid_loss: 1.5004191751833316\n",
            "test_ind: 3, Epoch: 979, train_loss: 1.5217819331604758, valid_loss: 1.4588747907567907\n",
            "test_ind: 3, Epoch: 980, train_loss: 1.5608555593608338, valid_loss: 1.4979512249981917\n",
            "test_ind: 3, Epoch: 981, train_loss: 1.6926969539971999, valid_loss: 1.6700515393857605\n",
            "test_ind: 3, Epoch: 982, train_loss: 1.6779411869284546, valid_loss: 1.4601789050632055\n",
            "test_ind: 3, Epoch: 983, train_loss: 1.5245998464984658, valid_loss: 1.3602641423543296\n",
            "test_ind: 3, Epoch: 984, train_loss: 1.689736195552496, valid_loss: 1.7683269536053692\n",
            "test_ind: 3, Epoch: 985, train_loss: 1.6846460118705846, valid_loss: 1.42473985530712\n",
            "test_ind: 3, Epoch: 986, train_loss: 1.5677149325241275, valid_loss: 1.5359927989818432\n",
            "test_ind: 3, Epoch: 987, train_loss: 1.579664530577483, valid_loss: 1.640909194946289\n",
            "test_ind: 3, Epoch: 988, train_loss: 1.4449925304930886, valid_loss: 1.2976745676111292\n",
            "Validation loss decreased (1.3346094908537687 --> 1.2976745676111292).  Saving model ...\n",
            "test_ind: 3, Epoch: 989, train_loss: 1.6626113255818684, valid_loss: 2.164318331965694\n",
            "test_ind: 3, Epoch: 990, train_loss: 1.4630716641743977, valid_loss: 1.3383808842411749\n",
            "test_ind: 3, Epoch: 991, train_loss: 1.4507364579188968, valid_loss: 1.4140558772616916\n",
            "test_ind: 3, Epoch: 992, train_loss: 1.5686094966935522, valid_loss: 1.613997282805266\n",
            "test_ind: 3, Epoch: 993, train_loss: 1.5447763219291781, valid_loss: 1.4175566567314994\n",
            "test_ind: 3, Epoch: 994, train_loss: 1.6517713040481379, valid_loss: 1.4332984818352594\n",
            "test_ind: 3, Epoch: 995, train_loss: 1.5414151380091536, valid_loss: 1.3828635392365634\n",
            "test_ind: 3, Epoch: 996, train_loss: 1.515614539016912, valid_loss: 1.6454775245101365\n",
            "test_ind: 3, Epoch: 997, train_loss: 1.5197389096389582, valid_loss: 1.3535723156399198\n",
            "test_ind: 3, Epoch: 998, train_loss: 1.4756392196372705, valid_loss: 1.5043552716573079\n",
            "test_ind: 3, Epoch: 999, train_loss: 1.5298469390398188, valid_loss: 1.3373339617693865\n",
            "test_ind: 3, Epoch: 1000, train_loss: 1.4958402256906769, valid_loss: 1.4523836418434426\n",
            "test_ind: 3, Epoch: 1001, train_loss: 1.55603375846957, valid_loss: 1.5057469297338415\n",
            "test_ind: 3, Epoch: 1002, train_loss: 1.5797713774221915, valid_loss: 1.6007606894881639\n",
            "test_ind: 3, Epoch: 1003, train_loss: 1.5797015884776173, valid_loss: 1.6631461249457467\n",
            "test_ind: 3, Epoch: 1004, train_loss: 1.5277533943270458, valid_loss: 1.8891724833735712\n",
            "test_ind: 3, Epoch: 1005, train_loss: 1.524218276694969, valid_loss: 1.6067370132163719\n",
            "test_ind: 3, Epoch: 1006, train_loss: 1.4326162573731978, valid_loss: 1.5595751161928528\n",
            "test_ind: 3, Epoch: 1007, train_loss: 1.504801296893461, valid_loss: 1.3091807188811124\n",
            "test_ind: 3, Epoch: 1008, train_loss: 1.5507627416540073, valid_loss: 1.5345197960182473\n",
            "test_ind: 3, Epoch: 1009, train_loss: 1.4844685248386713, valid_loss: 1.460022802706118\n",
            "test_ind: 3, Epoch: 1010, train_loss: 1.520770808796824, valid_loss: 1.333222406881827\n",
            "test_ind: 3, Epoch: 1011, train_loss: 1.478571915332182, valid_loss: 1.359493926719383\n",
            "test_ind: 3, Epoch: 1012, train_loss: 1.5498709560912334, valid_loss: 1.555714412971779\n",
            "test_ind: 3, Epoch: 1013, train_loss: 1.596331578713876, valid_loss: 1.803305396327266\n",
            "test_ind: 3, Epoch: 1014, train_loss: 1.5147309774233972, valid_loss: 1.8914713506345395\n",
            "test_ind: 3, Epoch: 1015, train_loss: 1.5257496303982203, valid_loss: 1.470066300144902\n",
            "test_ind: 3, Epoch: 1016, train_loss: 1.5111338768476321, valid_loss: 1.546232541402181\n",
            "test_ind: 3, Epoch: 1017, train_loss: 1.568511344768383, valid_loss: 1.5702590942382812\n",
            "test_ind: 3, Epoch: 1018, train_loss: 1.585741820158782, valid_loss: 1.4853376105979637\n",
            "test_ind: 3, Epoch: 1019, train_loss: 1.5904433168010945, valid_loss: 1.5441726048787436\n",
            "test_ind: 3, Epoch: 1020, train_loss: 1.3855548846868821, valid_loss: 1.399820327758789\n",
            "test_ind: 3, Epoch: 1021, train_loss: 1.5132083892822268, valid_loss: 1.3835791835078486\n",
            "test_ind: 3, Epoch: 1022, train_loss: 1.4177022568973494, valid_loss: 1.4097353617350261\n",
            "test_ind: 3, Epoch: 1023, train_loss: 1.6154592184372891, valid_loss: 1.4511597244827836\n",
            "test_ind: 3, Epoch: 1024, train_loss: 1.5084543287018197, valid_loss: 1.3508624853911222\n",
            "test_ind: 3, Epoch: 1025, train_loss: 1.5101467532876096, valid_loss: 1.6681860817803278\n",
            "test_ind: 3, Epoch: 1026, train_loss: 1.517111448594082, valid_loss: 1.9511540024368852\n",
            "test_ind: 3, Epoch: 1027, train_loss: 1.528066158294678, valid_loss: 1.4864171699241355\n",
            "test_ind: 3, Epoch: 1028, train_loss: 1.514145762832077, valid_loss: 1.4960986066747595\n",
            "test_ind: 3, Epoch: 1029, train_loss: 1.4372974619453336, valid_loss: 1.515028335430004\n",
            "test_ind: 3, Epoch: 1030, train_loss: 1.4910708651130584, valid_loss: 1.600795622225161\n",
            "test_ind: 3, Epoch: 1031, train_loss: 1.454970760109984, valid_loss: 1.6233397766395852\n",
            "test_ind: 3, Epoch: 1032, train_loss: 1.551013864116904, valid_loss: 1.6587375358298972\n",
            "test_ind: 3, Epoch: 1033, train_loss: 1.5171205555951155, valid_loss: 1.4182504547966852\n",
            "test_ind: 3, Epoch: 1034, train_loss: 1.4676682860762984, valid_loss: 1.5939893369321467\n",
            "test_ind: 3, Epoch: 1035, train_loss: 1.5811162465884363, valid_loss: 1.7817073927985296\n",
            "test_ind: 3, Epoch: 1036, train_loss: 1.4403984458358199, valid_loss: 1.4539837307400174\n",
            "test_ind: 3, Epoch: 1037, train_loss: 1.6661454483314797, valid_loss: 1.7912576110274703\n",
            "test_ind: 3, Epoch: 1038, train_loss: 1.7554145153657892, valid_loss: 1.9449786610073514\n",
            "test_ind: 3, Epoch: 1039, train_loss: 1.5904070300820434, valid_loss: 1.4735463283680104\n",
            "test_ind: 3, Epoch: 1040, train_loss: 1.4800396024444955, valid_loss: 1.3720977747881853\n",
            "test_ind: 3, Epoch: 1041, train_loss: 1.6144942177666557, valid_loss: 1.4728935736197013\n",
            "test_ind: 3, Epoch: 1042, train_loss: 1.563145213656955, valid_loss: 1.3236540335196034\n",
            "test_ind: 3, Epoch: 1043, train_loss: 1.5818981771115903, valid_loss: 1.8977588370994285\n",
            "test_ind: 3, Epoch: 1044, train_loss: 1.4192337283381706, valid_loss: 1.3654492872732658\n",
            "test_ind: 3, Epoch: 1045, train_loss: 1.5512188864342962, valid_loss: 1.4152242519237377\n",
            "test_ind: 3, Epoch: 1046, train_loss: 1.4785061059174716, valid_loss: 1.5097093935366028\n",
            "test_ind: 3, Epoch: 1047, train_loss: 1.565173319828363, valid_loss: 1.635777544092249\n",
            "test_ind: 3, Epoch: 1048, train_loss: 1.473894672629274, valid_loss: 1.296744011066578\n",
            "Validation loss decreased (1.2976745676111292 --> 1.296744011066578).  Saving model ...\n",
            "test_ind: 3, Epoch: 1049, train_loss: 1.5667927706683127, valid_loss: 1.293182690938314\n",
            "Validation loss decreased (1.296744011066578 --> 1.293182690938314).  Saving model ...\n",
            "test_ind: 3, Epoch: 1050, train_loss: 1.590119308895535, valid_loss: 2.022238219225848\n",
            "test_ind: 3, Epoch: 1051, train_loss: 1.585435955612748, valid_loss: 1.841653135087755\n",
            "test_ind: 3, Epoch: 1052, train_loss: 1.5414687910197695, valid_loss: 1.4583183924357097\n",
            "test_ind: 3, Epoch: 1053, train_loss: 1.4973031503182872, valid_loss: 1.3647598690456815\n",
            "test_ind: 3, Epoch: 1054, train_loss: 1.5291857660552604, valid_loss: 1.8100229016056766\n",
            "test_ind: 3, Epoch: 1055, train_loss: 1.585580113493366, valid_loss: 1.6148386354799624\n",
            "test_ind: 3, Epoch: 1056, train_loss: 1.4800025327706041, valid_loss: 1.5094620210153087\n",
            "test_ind: 3, Epoch: 1057, train_loss: 1.4692848935539342, valid_loss: 1.3745086634600605\n",
            "test_ind: 3, Epoch: 1058, train_loss: 1.517588191562229, valid_loss: 1.5001696833857783\n",
            "test_ind: 3, Epoch: 1059, train_loss: 1.7210226824254165, valid_loss: 1.516934765709771\n",
            "test_ind: 3, Epoch: 1060, train_loss: 1.5300358960657945, valid_loss: 1.4432817741676613\n",
            "test_ind: 3, Epoch: 1061, train_loss: 1.5307562086317277, valid_loss: 1.559788368366383\n",
            "test_ind: 3, Epoch: 1062, train_loss: 1.4867479300793307, valid_loss: 1.5543397620872215\n",
            "test_ind: 3, Epoch: 1063, train_loss: 1.503698142958276, valid_loss: 1.3423386149936252\n",
            "test_ind: 3, Epoch: 1064, train_loss: 1.4970800081888835, valid_loss: 1.585285822550456\n",
            "test_ind: 3, Epoch: 1065, train_loss: 1.5323584344651964, valid_loss: 1.3851603402031794\n",
            "test_ind: 3, Epoch: 1066, train_loss: 1.4316869311862523, valid_loss: 1.4003711806403265\n",
            "test_ind: 3, Epoch: 1067, train_loss: 1.4601682203787345, valid_loss: 1.3051134921886303\n",
            "test_ind: 3, Epoch: 1068, train_loss: 1.445990538891451, valid_loss: 1.4128079767580386\n",
            "test_ind: 3, Epoch: 1069, train_loss: 1.4849633993925873, valid_loss: 1.3600394284283674\n",
            "test_ind: 3, Epoch: 1070, train_loss: 1.4430839809370628, valid_loss: 1.263974013151946\n",
            "Validation loss decreased (1.293182690938314 --> 1.263974013151946).  Saving model ...\n",
            "test_ind: 3, Epoch: 1071, train_loss: 1.6187267244598014, valid_loss: 1.4476200033117224\n",
            "test_ind: 3, Epoch: 1072, train_loss: 1.6071627228348344, valid_loss: 1.714673466152615\n",
            "test_ind: 3, Epoch: 1073, train_loss: 1.4995594848821192, valid_loss: 1.517033576965332\n",
            "test_ind: 3, Epoch: 1074, train_loss: 1.512100390446039, valid_loss: 1.4908000628153484\n",
            "test_ind: 3, Epoch: 1075, train_loss: 1.4300649372147924, valid_loss: 1.7186009618971085\n",
            "test_ind: 3, Epoch: 1076, train_loss: 1.4727522944226676, valid_loss: 1.4036308217931674\n",
            "test_ind: 3, Epoch: 1077, train_loss: 1.4543211254072776, valid_loss: 1.3613002035352917\n",
            "test_ind: 3, Epoch: 1078, train_loss: 1.4445472705511397, valid_loss: 1.5997327521995262\n",
            "test_ind: 3, Epoch: 1079, train_loss: 1.7168111624541107, valid_loss: 1.6529186213458027\n",
            "test_ind: 3, Epoch: 1080, train_loss: 1.5309909302511335, valid_loss: 1.382822213349519\n",
            "test_ind: 3, Epoch: 1081, train_loss: 1.5086422672978155, valid_loss: 2.0876390669080944\n",
            "test_ind: 3, Epoch: 1082, train_loss: 1.7002460102976107, valid_loss: 1.555414976897063\n",
            "test_ind: 3, Epoch: 1083, train_loss: 1.5436104786248854, valid_loss: 1.4063253226103607\n",
            "test_ind: 3, Epoch: 1084, train_loss: 1.4565223999965338, valid_loss: 1.2851436403062608\n",
            "test_ind: 3, Epoch: 1085, train_loss: 1.4710683940369402, valid_loss: 1.5919290825172707\n",
            "test_ind: 3, Epoch: 1086, train_loss: 1.4852210445168577, valid_loss: 1.4643619855244956\n",
            "test_ind: 3, Epoch: 1087, train_loss: 1.5989467891646019, valid_loss: 1.5833708975050185\n",
            "test_ind: 3, Epoch: 1088, train_loss: 1.4213226753988382, valid_loss: 1.2862062277617277\n",
            "test_ind: 3, Epoch: 1089, train_loss: 1.4808678803620516, valid_loss: 1.339617128725405\n",
            "test_ind: 3, Epoch: 1090, train_loss: 1.6089339197417833, valid_loss: 1.3446495974505388\n",
            "test_ind: 3, Epoch: 1091, train_loss: 1.4855239303023724, valid_loss: 1.4016040166219077\n",
            "test_ind: 3, Epoch: 1092, train_loss: 1.510110484229194, valid_loss: 1.4950606734664351\n",
            "test_ind: 3, Epoch: 1093, train_loss: 1.4501797240457417, valid_loss: 1.2735684006302446\n",
            "test_ind: 3, Epoch: 1094, train_loss: 1.4396525783303344, valid_loss: 1.45215673799868\n",
            "test_ind: 3, Epoch: 1095, train_loss: 1.4907230330102237, valid_loss: 1.3796475021927443\n",
            "test_ind: 3, Epoch: 1096, train_loss: 1.6692884704213085, valid_loss: 1.50133392545912\n",
            "test_ind: 3, Epoch: 1097, train_loss: 1.51281448646828, valid_loss: 1.6696831385294595\n",
            "test_ind: 3, Epoch: 1098, train_loss: 1.5060986883846328, valid_loss: 1.38023277565285\n",
            "test_ind: 3, Epoch: 1099, train_loss: 1.5391028898733634, valid_loss: 1.2785988207216616\n",
            "test_ind: 3, Epoch: 1100, train_loss: 1.5456571578979492, valid_loss: 1.7110485324153195\n",
            "test_ind: 3, Epoch: 1101, train_loss: 1.5419930293236253, valid_loss: 1.7903743320041234\n",
            "test_ind: 3, Epoch: 1102, train_loss: 1.470275107725167, valid_loss: 1.363503685703984\n",
            "test_ind: 3, Epoch: 1103, train_loss: 1.5359769927130806, valid_loss: 1.4143870671590169\n",
            "test_ind: 3, Epoch: 1104, train_loss: 1.4075424288525993, valid_loss: 1.385361159289325\n",
            "test_ind: 3, Epoch: 1105, train_loss: 1.4639276457421575, valid_loss: 1.4575149394847728\n",
            "test_ind: 3, Epoch: 1106, train_loss: 1.5415410171320407, valid_loss: 1.5501361952887642\n",
            "test_ind: 3, Epoch: 1107, train_loss: 1.472567758442443, valid_loss: 1.6451501846313477\n",
            "test_ind: 3, Epoch: 1108, train_loss: 1.6350996464858822, valid_loss: 1.4077502886454265\n",
            "test_ind: 3, Epoch: 1109, train_loss: 1.545095184702932, valid_loss: 1.6478991861696597\n",
            "test_ind: 3, Epoch: 1110, train_loss: 1.4315216923937388, valid_loss: 1.4854119265521015\n",
            "test_ind: 3, Epoch: 1111, train_loss: 1.4658462383128978, valid_loss: 1.4169254302978516\n",
            "test_ind: 3, Epoch: 1112, train_loss: 1.6410402309747392, valid_loss: 1.6350385348002114\n",
            "test_ind: 3, Epoch: 1113, train_loss: 1.425599345454463, valid_loss: 1.362254125100595\n",
            "test_ind: 3, Epoch: 1114, train_loss: 1.480111457683422, valid_loss: 1.533539436481617\n",
            "test_ind: 3, Epoch: 1115, train_loss: 1.5431075449343081, valid_loss: 1.445681713245533\n",
            "test_ind: 3, Epoch: 1116, train_loss: 1.5284131485738874, valid_loss: 1.6271331398575395\n",
            "test_ind: 3, Epoch: 1117, train_loss: 1.470549241996106, valid_loss: 1.6068787574768066\n",
            "test_ind: 3, Epoch: 1118, train_loss: 1.6078699900780193, valid_loss: 1.933002966421622\n",
            "test_ind: 3, Epoch: 1119, train_loss: 1.5724696171136547, valid_loss: 1.9969983983922888\n",
            "test_ind: 3, Epoch: 1120, train_loss: 1.5493233351059905, valid_loss: 1.4532341957092285\n",
            "test_ind: 3, Epoch: 1121, train_loss: 1.5833693551428525, valid_loss: 1.4228979746500652\n",
            "test_ind: 3, Epoch: 1122, train_loss: 1.5377059924749683, valid_loss: 1.3527193952489782\n",
            "test_ind: 3, Epoch: 1123, train_loss: 1.5099633240405423, valid_loss: 1.5910118420918782\n",
            "test_ind: 3, Epoch: 1124, train_loss: 1.447672920462526, valid_loss: 1.3145625326368544\n",
            "test_ind: 3, Epoch: 1125, train_loss: 1.52565837200777, valid_loss: 1.3264815189220287\n",
            "test_ind: 3, Epoch: 1126, train_loss: 1.5114762282665866, valid_loss: 1.4214544826083713\n",
            "test_ind: 3, Epoch: 1127, train_loss: 1.6146718366646473, valid_loss: 1.4716908666822648\n",
            "test_ind: 3, Epoch: 1128, train_loss: 1.6217228571573892, valid_loss: 1.3967084884643555\n",
            "test_ind: 3, Epoch: 1129, train_loss: 1.5202406600669576, valid_loss: 1.5336028381630227\n",
            "test_ind: 3, Epoch: 1130, train_loss: 1.468707119977033, valid_loss: 1.2785260059215402\n",
            "test_ind: 3, Epoch: 1131, train_loss: 1.5507774706240052, valid_loss: 1.3615892551563404\n",
            "test_ind: 3, Epoch: 1132, train_loss: 1.4427406405225212, valid_loss: 1.4761396337438513\n",
            "test_ind: 3, Epoch: 1133, train_loss: 1.518500045493797, valid_loss: 1.4286304226628055\n",
            "test_ind: 3, Epoch: 1134, train_loss: 1.442751242790693, valid_loss: 1.4124990922433358\n",
            "test_ind: 3, Epoch: 1135, train_loss: 1.5753151575724285, valid_loss: 1.5235902468363443\n",
            "test_ind: 3, Epoch: 1136, train_loss: 1.5524770595409254, valid_loss: 1.49660062789917\n",
            "test_ind: 3, Epoch: 1137, train_loss: 1.529399624577275, valid_loss: 1.4062087094342268\n",
            "test_ind: 3, Epoch: 1138, train_loss: 1.4386001457402733, valid_loss: 1.502434324335169\n",
            "test_ind: 3, Epoch: 1139, train_loss: 1.4980046719680598, valid_loss: 1.480500927677861\n",
            "test_ind: 3, Epoch: 1140, train_loss: 1.464718359488028, valid_loss: 1.6051821178860135\n",
            "test_ind: 3, Epoch: 1141, train_loss: 1.4732906082530082, valid_loss: 1.416602929433187\n",
            "test_ind: 3, Epoch: 1142, train_loss: 1.4794520095542625, valid_loss: 1.4805874471311216\n",
            "test_ind: 3, Epoch: 1143, train_loss: 1.4890055362089178, valid_loss: 1.8547826872931588\n",
            "test_ind: 3, Epoch: 1144, train_loss: 1.5123594837424195, valid_loss: 1.3535193690547236\n",
            "test_ind: 3, Epoch: 1145, train_loss: 1.5094277888168524, valid_loss: 1.3308550516764324\n",
            "test_ind: 3, Epoch: 1146, train_loss: 1.5423110090656045, valid_loss: 1.641319751739502\n",
            "test_ind: 3, Epoch: 1147, train_loss: 1.5522600574257934, valid_loss: 1.5533694691128201\n",
            "test_ind: 3, Epoch: 1148, train_loss: 1.594058707908348, valid_loss: 1.6812910503811307\n",
            "test_ind: 3, Epoch: 1149, train_loss: 1.4262452714237166, valid_loss: 1.498042866035744\n",
            "test_ind: 3, Epoch: 1150, train_loss: 1.4432143105400936, valid_loss: 1.3732326472247087\n",
            "test_ind: 3, Epoch: 1151, train_loss: 1.5279669408445005, valid_loss: 1.4294147844667788\n",
            "test_ind: 3, Epoch: 1152, train_loss: 1.5190992590821817, valid_loss: 1.587840274528221\n",
            "test_ind: 3, Epoch: 1153, train_loss: 1.6216946648962702, valid_loss: 1.6983891946298106\n",
            "test_ind: 3, Epoch: 1154, train_loss: 1.423154889801402, valid_loss: 1.3617009763364438\n",
            "test_ind: 3, Epoch: 1155, train_loss: 1.5498972410037193, valid_loss: 1.9238020049201119\n",
            "test_ind: 3, Epoch: 1156, train_loss: 1.6233521802925768, valid_loss: 1.740616339224356\n",
            "test_ind: 3, Epoch: 1157, train_loss: 1.5414642698970842, valid_loss: 1.4759635395473905\n",
            "test_ind: 3, Epoch: 1158, train_loss: 1.4527296313533076, valid_loss: 1.4804601316098813\n",
            "test_ind: 3, Epoch: 1159, train_loss: 1.4326681678677784, valid_loss: 1.4948270762408222\n",
            "test_ind: 3, Epoch: 1160, train_loss: 1.4616148030316387, valid_loss: 1.4347130280953868\n",
            "test_ind: 3, Epoch: 1161, train_loss: 1.5847952630784774, valid_loss: 1.3901385554560908\n",
            "test_ind: 3, Epoch: 1162, train_loss: 1.510524296466215, valid_loss: 1.4666309356689453\n",
            "test_ind: 3, Epoch: 1163, train_loss: 1.4778716887956784, valid_loss: 1.349814079425953\n",
            "test_ind: 3, Epoch: 1164, train_loss: 1.5301510669566964, valid_loss: 1.8967545474017107\n",
            "test_ind: 3, Epoch: 1165, train_loss: 1.4278912485381703, valid_loss: 1.4999181076332373\n",
            "test_ind: 3, Epoch: 1166, train_loss: 1.5303451808882347, valid_loss: 1.5836744485078036\n",
            "test_ind: 3, Epoch: 1167, train_loss: 1.4889408747355142, valid_loss: 1.4382151850947626\n",
            "test_ind: 3, Epoch: 1168, train_loss: 1.492789633480119, valid_loss: 1.4309989081488714\n",
            "test_ind: 3, Epoch: 1169, train_loss: 1.4872804747687445, valid_loss: 1.4642926145482946\n",
            "test_ind: 3, Epoch: 1170, train_loss: 1.5633733066511744, valid_loss: 1.5399164270471644\n",
            "test_ind: 3, Epoch: 1171, train_loss: 1.4541874049622334, valid_loss: 1.5877240145647966\n",
            "test_ind: 3, Epoch: 1172, train_loss: 1.4773868042745708, valid_loss: 1.2441308056866682\n",
            "Validation loss decreased (1.263974013151946 --> 1.2441308056866682).  Saving model ...\n",
            "test_ind: 3, Epoch: 1173, train_loss: 1.4480142475646218, valid_loss: 1.354209511368363\n",
            "test_ind: 3, Epoch: 1174, train_loss: 1.4409156905280218, valid_loss: 1.4824280738830566\n",
            "test_ind: 3, Epoch: 1175, train_loss: 1.6059526396386414, valid_loss: 1.4535092954282407\n",
            "test_ind: 3, Epoch: 1176, train_loss: 1.5035238089384855, valid_loss: 1.4490781006989655\n",
            "test_ind: 3, Epoch: 1177, train_loss: 1.5481844949133605, valid_loss: 1.713484940705476\n",
            "test_ind: 3, Epoch: 1178, train_loss: 1.4721524803726762, valid_loss: 1.4560459984673395\n",
            "test_ind: 3, Epoch: 1179, train_loss: 1.4404230412141776, valid_loss: 1.3712812176457159\n",
            "test_ind: 3, Epoch: 1180, train_loss: 1.542047912691846, valid_loss: 1.4204046461317275\n",
            "test_ind: 3, Epoch: 1181, train_loss: 1.5400967362486284, valid_loss: 1.3863522564923323\n",
            "test_ind: 3, Epoch: 1182, train_loss: 1.496138190045769, valid_loss: 1.5193372302585177\n",
            "test_ind: 3, Epoch: 1183, train_loss: 1.4460793189060541, valid_loss: 1.434886967694318\n",
            "test_ind: 3, Epoch: 1184, train_loss: 1.5274423493279354, valid_loss: 1.4817502940142595\n",
            "test_ind: 3, Epoch: 1185, train_loss: 1.6390039479290999, valid_loss: 1.5600383723223652\n",
            "test_ind: 3, Epoch: 1186, train_loss: 1.590019196639826, valid_loss: 1.7290874940377692\n",
            "test_ind: 3, Epoch: 1187, train_loss: 1.4872769779629178, valid_loss: 1.71850225660536\n",
            "test_ind: 3, Epoch: 1188, train_loss: 1.50129833927861, valid_loss: 1.389249289477313\n",
            "test_ind: 3, Epoch: 1189, train_loss: 1.6040305443751957, valid_loss: 2.3480034051118075\n",
            "test_ind: 3, Epoch: 1190, train_loss: 1.4601817719730332, valid_loss: 1.4066109657287598\n",
            "test_ind: 3, Epoch: 1191, train_loss: 1.5292266386526603, valid_loss: 1.4076882468329537\n",
            "test_ind: 3, Epoch: 1192, train_loss: 1.403951262250359, valid_loss: 1.3971851490162037\n",
            "test_ind: 3, Epoch: 1193, train_loss: 1.4353052539589966, valid_loss: 1.271236949496799\n",
            "test_ind: 3, Epoch: 1194, train_loss: 1.3777173301320018, valid_loss: 1.3247886940285012\n",
            "test_ind: 3, Epoch: 1195, train_loss: 1.5901564668726038, valid_loss: 1.8430868431373881\n",
            "test_ind: 3, Epoch: 1196, train_loss: 1.4811561195938674, valid_loss: 1.5475796770166468\n",
            "test_ind: 3, Epoch: 1197, train_loss: 1.5007804352560161, valid_loss: 1.824262195163303\n",
            "test_ind: 3, Epoch: 1198, train_loss: 1.4370949768725734, valid_loss: 1.737702828866464\n",
            "test_ind: 3, Epoch: 1199, train_loss: 1.4439827071295843, valid_loss: 1.71000807373612\n",
            "test_ind: 3, Epoch: 1200, train_loss: 1.436797383390827, valid_loss: 1.426170896600794\n",
            "test_ind: 3, Epoch: 1201, train_loss: 1.4543310212500302, valid_loss: 1.4473241523460105\n",
            "test_ind: 3, Epoch: 1202, train_loss: 1.4388028486275377, valid_loss: 1.342629450338858\n",
            "test_ind: 3, Epoch: 1203, train_loss: 1.5846249027016723, valid_loss: 1.6184591893796567\n",
            "test_ind: 3, Epoch: 1204, train_loss: 1.4614213543173706, valid_loss: 1.3210891794275355\n",
            "test_ind: 3, Epoch: 1205, train_loss: 1.4986844651493023, valid_loss: 1.7815804304899994\n",
            "test_ind: 3, Epoch: 1206, train_loss: 1.7101808889412586, valid_loss: 1.392024358113607\n",
            "test_ind: 3, Epoch: 1207, train_loss: 1.4580006717163843, valid_loss: 1.3523226314120822\n",
            "test_ind: 3, Epoch: 1208, train_loss: 1.5365862081080306, valid_loss: 1.5604555871751573\n",
            "test_ind: 3, Epoch: 1209, train_loss: 1.421926427770544, valid_loss: 1.3920569066648132\n",
            "test_ind: 3, Epoch: 1210, train_loss: 1.398527357313368, valid_loss: 1.3597522135134097\n",
            "test_ind: 3, Epoch: 1211, train_loss: 1.5661726586612654, valid_loss: 1.50903140174018\n",
            "test_ind: 3, Epoch: 1212, train_loss: 1.4600220550725491, valid_loss: 1.353987905714247\n",
            "test_ind: 3, Epoch: 1213, train_loss: 1.4895509672753604, valid_loss: 1.482528916111699\n",
            "test_ind: 3, Epoch: 1214, train_loss: 1.5030176021434642, valid_loss: 1.411765663712113\n",
            "test_ind: 3, Epoch: 1215, train_loss: 1.462495203371401, valid_loss: 1.7157612376742892\n",
            "test_ind: 3, Epoch: 1216, train_loss: 1.391338860547101, valid_loss: 1.362066357224076\n",
            "test_ind: 3, Epoch: 1217, train_loss: 1.368590090009901, valid_loss: 1.470416157333939\n",
            "test_ind: 3, Epoch: 1218, train_loss: 1.4760344057907295, valid_loss: 1.4555834664238825\n",
            "test_ind: 3, Epoch: 1219, train_loss: 1.5309469905900366, valid_loss: 1.6709053604691118\n",
            "test_ind: 3, Epoch: 1220, train_loss: 1.4969840756169073, valid_loss: 1.5471228316978172\n",
            "test_ind: 3, Epoch: 1221, train_loss: 1.4675729892871998, valid_loss: 1.418041052641692\n",
            "test_ind: 3, Epoch: 1222, train_loss: 1.5273567011326918, valid_loss: 1.4356441144590026\n",
            "test_ind: 3, Epoch: 1223, train_loss: 1.4678228107499485, valid_loss: 1.2926144423308197\n",
            "test_ind: 3, Epoch: 1224, train_loss: 1.4678439328699935, valid_loss: 1.4453600954126429\n",
            "test_ind: 3, Epoch: 1225, train_loss: 1.389210718649405, valid_loss: 1.583491607948586\n",
            "test_ind: 3, Epoch: 1226, train_loss: 1.4221210126523618, valid_loss: 1.4691504195884422\n",
            "test_ind: 3, Epoch: 1227, train_loss: 1.4328054498743128, valid_loss: 1.4285469231782137\n",
            "test_ind: 3, Epoch: 1228, train_loss: 1.5311674895109955, valid_loss: 1.6239296595255532\n",
            "test_ind: 3, Epoch: 1229, train_loss: 1.5646522957601663, valid_loss: 1.3864477475484214\n",
            "test_ind: 3, Epoch: 1230, train_loss: 1.375385873111678, valid_loss: 1.3452877645139343\n",
            "test_ind: 3, Epoch: 1231, train_loss: 1.4683423160034932, valid_loss: 1.3758828375074597\n",
            "test_ind: 3, Epoch: 1232, train_loss: 1.4472779933317208, valid_loss: 1.4026281745345504\n",
            "test_ind: 3, Epoch: 1233, train_loss: 1.4980638056625553, valid_loss: 1.3449156902454518\n",
            "test_ind: 3, Epoch: 1234, train_loss: 1.567593403804449, valid_loss: 2.0882969609013307\n",
            "test_ind: 3, Epoch: 1235, train_loss: 1.381227970123291, valid_loss: 1.3106703405027036\n",
            "test_ind: 3, Epoch: 1236, train_loss: 1.5015966097513833, valid_loss: 2.160153283013238\n",
            "test_ind: 3, Epoch: 1237, train_loss: 1.4434128455173822, valid_loss: 1.5403253767225478\n",
            "test_ind: 3, Epoch: 1238, train_loss: 1.4944199102896232, valid_loss: 1.3614428484881367\n",
            "test_ind: 3, Epoch: 1239, train_loss: 1.5005868746910567, valid_loss: 1.4227083700674554\n",
            "test_ind: 3, Epoch: 1240, train_loss: 1.5747432237789956, valid_loss: 1.384166805832474\n",
            "test_ind: 3, Epoch: 1241, train_loss: 1.5170704818066256, valid_loss: 1.2908411909032749\n",
            "test_ind: 3, Epoch: 1242, train_loss: 1.4956777360704208, valid_loss: 1.5671885455096208\n",
            "test_ind: 3, Epoch: 1243, train_loss: 1.5013957141358176, valid_loss: 1.280789092735008\n",
            "test_ind: 3, Epoch: 1244, train_loss: 1.481338830641758, valid_loss: 1.4827829113713016\n",
            "test_ind: 3, Epoch: 1245, train_loss: 1.4822334831143602, valid_loss: 1.5464833400867604\n",
            "test_ind: 3, Epoch: 1246, train_loss: 1.6068051302874529, valid_loss: 1.4654595410382307\n",
            "test_ind: 3, Epoch: 1247, train_loss: 1.3789620870425376, valid_loss: 1.438753816816542\n",
            "test_ind: 3, Epoch: 1248, train_loss: 1.404084223288077, valid_loss: 1.3176726235283747\n",
            "test_ind: 3, Epoch: 1249, train_loss: 1.3878129441061138, valid_loss: 1.3175612732216164\n",
            "test_ind: 3, Epoch: 1250, train_loss: 1.4190173384584024, valid_loss: 1.4785502751668291\n",
            "test_ind: 3, Epoch: 1251, train_loss: 1.536850799748927, valid_loss: 1.7910021499351219\n",
            "test_ind: 3, Epoch: 1252, train_loss: 1.3438314273033614, valid_loss: 1.3886778796160664\n",
            "test_ind: 3, Epoch: 1253, train_loss: 1.4702112350934817, valid_loss: 1.4155311761079012\n",
            "test_ind: 3, Epoch: 1254, train_loss: 1.4508731335769465, valid_loss: 1.2704525876928259\n",
            "test_ind: 3, Epoch: 1255, train_loss: 1.3751902639130014, valid_loss: 1.3382356961568196\n",
            "test_ind: 3, Epoch: 1256, train_loss: 1.384551583984752, valid_loss: 1.730900552537706\n",
            "test_ind: 3, Epoch: 1257, train_loss: 1.4668557320112066, valid_loss: 1.3429932417692962\n",
            "test_ind: 3, Epoch: 1258, train_loss: 1.4323904367140783, valid_loss: 1.3120466161657263\n",
            "test_ind: 3, Epoch: 1259, train_loss: 1.4685948159959583, valid_loss: 1.5744372473822699\n",
            "test_ind: 3, Epoch: 1260, train_loss: 1.4529954651255665, valid_loss: 1.4837302808408384\n",
            "test_ind: 3, Epoch: 1261, train_loss: 1.4581886809549214, valid_loss: 1.777055687374539\n",
            "test_ind: 3, Epoch: 1262, train_loss: 1.5207116162335428, valid_loss: 1.3269960791976363\n",
            "test_ind: 3, Epoch: 1263, train_loss: 1.407853008788309, valid_loss: 1.529986611119023\n",
            "test_ind: 3, Epoch: 1264, train_loss: 1.6400347756750793, valid_loss: 1.993629420245135\n",
            "test_ind: 3, Epoch: 1265, train_loss: 1.4368738186212233, valid_loss: 1.423862510257297\n",
            "test_ind: 3, Epoch: 1266, train_loss: 1.408859582594883, valid_loss: 1.3676661208823875\n",
            "test_ind: 3, Epoch: 1267, train_loss: 1.3753754651104961, valid_loss: 1.1833788907086409\n",
            "Validation loss decreased (1.2441308056866682 --> 1.1833788907086409).  Saving model ...\n",
            "test_ind: 3, Epoch: 1268, train_loss: 1.3821610815731096, valid_loss: 1.3065398180926286\n",
            "test_ind: 3, Epoch: 1269, train_loss: 1.359398035355556, valid_loss: 1.2891240473146792\n",
            "test_ind: 3, Epoch: 1270, train_loss: 1.429303958092207, valid_loss: 1.4445072986461498\n",
            "test_ind: 3, Epoch: 1271, train_loss: 1.369988771132481, valid_loss: 1.651415259749801\n",
            "test_ind: 3, Epoch: 1272, train_loss: 1.3990883768340687, valid_loss: 1.3181942127369068\n",
            "test_ind: 3, Epoch: 1273, train_loss: 1.3382466987327295, valid_loss: 1.4545444735774287\n",
            "test_ind: 3, Epoch: 1274, train_loss: 1.4657573170132108, valid_loss: 1.332507610321045\n",
            "test_ind: 3, Epoch: 1275, train_loss: 1.4253566706622087, valid_loss: 1.312919192843967\n",
            "test_ind: 3, Epoch: 1276, train_loss: 1.4880258006814087, valid_loss: 1.4358240763346355\n",
            "test_ind: 3, Epoch: 1277, train_loss: 1.3897935313942993, valid_loss: 1.3487393944351762\n",
            "test_ind: 3, Epoch: 1278, train_loss: 1.4963376374892248, valid_loss: 1.3700184292263455\n",
            "test_ind: 3, Epoch: 1279, train_loss: 1.3292935336077654, valid_loss: 1.21888181898329\n",
            "test_ind: 3, Epoch: 1280, train_loss: 1.402183250144676, valid_loss: 1.3332817642777053\n",
            "test_ind: 3, Epoch: 1281, train_loss: 1.5614289531001337, valid_loss: 2.4656222131517196\n",
            "test_ind: 3, Epoch: 1282, train_loss: 1.4452988895369163, valid_loss: 1.3890309157194913\n",
            "test_ind: 3, Epoch: 1283, train_loss: 1.4271077226709434, valid_loss: 1.3724777433607316\n",
            "test_ind: 3, Epoch: 1284, train_loss: 1.6564548751454296, valid_loss: 1.7505651579962835\n",
            "test_ind: 3, Epoch: 1285, train_loss: 1.4035386156152796, valid_loss: 1.3362573164480704\n",
            "test_ind: 3, Epoch: 1286, train_loss: 1.503193837625009, valid_loss: 1.4295440779791937\n",
            "test_ind: 3, Epoch: 1287, train_loss: 1.4455424885690948, valid_loss: 1.4794236289130316\n",
            "test_ind: 3, Epoch: 1288, train_loss: 1.4238021933002234, valid_loss: 1.1866722989965368\n",
            "test_ind: 3, Epoch: 1289, train_loss: 1.3558130558626154, valid_loss: 1.4240199548226817\n",
            "test_ind: 3, Epoch: 1290, train_loss: 1.4245704309439953, valid_loss: 1.2541058151810256\n",
            "test_ind: 3, Epoch: 1291, train_loss: 1.3431028848812903, valid_loss: 1.2125926724186649\n",
            "test_ind: 3, Epoch: 1292, train_loss: 1.35095810007166, valid_loss: 1.404444305985062\n",
            "test_ind: 3, Epoch: 1293, train_loss: 1.4438993489300764, valid_loss: 1.9560716417100696\n",
            "test_ind: 3, Epoch: 1294, train_loss: 1.4124068213097842, valid_loss: 1.6181138533133048\n",
            "test_ind: 3, Epoch: 1295, train_loss: 1.460089400962547, valid_loss: 1.616219167356138\n",
            "test_ind: 3, Epoch: 1296, train_loss: 1.41245283903899, valid_loss: 1.4235002199808757\n",
            "test_ind: 3, Epoch: 1297, train_loss: 1.434249159730511, valid_loss: 1.4816430586355702\n",
            "test_ind: 3, Epoch: 1298, train_loss: 1.3405451244778104, valid_loss: 1.4827796618143718\n",
            "test_ind: 3, Epoch: 1299, train_loss: 1.342659808971264, valid_loss: 1.4293967882792153\n",
            "test_ind: 3, Epoch: 1300, train_loss: 1.3869352222960674, valid_loss: 1.252140274754277\n",
            "test_ind: 3, Epoch: 1301, train_loss: 1.3538936214682495, valid_loss: 1.3927934964497886\n",
            "test_ind: 3, Epoch: 1302, train_loss: 1.4238137492427119, valid_loss: 1.228270830931487\n",
            "test_ind: 3, Epoch: 1303, train_loss: 1.4395962350162457, valid_loss: 1.3091435785646792\n",
            "test_ind: 3, Epoch: 1304, train_loss: 1.3990088215580692, valid_loss: 1.3141266681529857\n",
            "test_ind: 3, Epoch: 1305, train_loss: 1.4027332376550747, valid_loss: 1.4801125173215515\n",
            "test_ind: 3, Epoch: 1306, train_loss: 1.4054607697475103, valid_loss: 1.3973159260219998\n",
            "test_ind: 3, Epoch: 1307, train_loss: 1.4282783225730613, valid_loss: 1.2384675873650446\n",
            "test_ind: 3, Epoch: 1308, train_loss: 1.3490370055775585, valid_loss: 1.1989251772562661\n",
            "test_ind: 3, Epoch: 1309, train_loss: 1.5901415554093727, valid_loss: 1.4848769505818686\n",
            "test_ind: 3, Epoch: 1310, train_loss: 1.4249074253035179, valid_loss: 1.5574854214986167\n",
            "test_ind: 3, Epoch: 1311, train_loss: 1.344491234532109, valid_loss: 1.3459233884458188\n",
            "test_ind: 3, Epoch: 1312, train_loss: 1.4885864257812502, valid_loss: 1.476508688043665\n",
            "test_ind: 3, Epoch: 1313, train_loss: 1.3274682068530426, valid_loss: 1.4746152736522533\n",
            "test_ind: 3, Epoch: 1314, train_loss: 1.4198887495347012, valid_loss: 1.694698616310402\n",
            "test_ind: 3, Epoch: 1315, train_loss: 1.4187053456718537, valid_loss: 1.5595805733292192\n",
            "test_ind: 3, Epoch: 1316, train_loss: 1.3241203331652982, valid_loss: 1.3149329644662362\n",
            "test_ind: 3, Epoch: 1317, train_loss: 1.3592076242705922, valid_loss: 1.3864276497452348\n",
            "test_ind: 3, Epoch: 1318, train_loss: 1.4483880349147469, valid_loss: 1.3436904306764954\n",
            "test_ind: 3, Epoch: 1319, train_loss: 1.3049913394598314, valid_loss: 1.370485164501049\n",
            "test_ind: 3, Epoch: 1320, train_loss: 1.3661197791864843, valid_loss: 1.4447235884489835\n",
            "test_ind: 3, Epoch: 1321, train_loss: 1.5097171759899752, valid_loss: 1.4214917995311596\n",
            "test_ind: 3, Epoch: 1322, train_loss: 1.3920225508419084, valid_loss: 1.2307410063566986\n",
            "test_ind: 3, Epoch: 1323, train_loss: 1.3545530578236522, valid_loss: 1.5254164271884494\n",
            "test_ind: 3, Epoch: 1324, train_loss: 1.4558059551097728, valid_loss: 1.358830663892958\n",
            "test_ind: 3, Epoch: 1325, train_loss: 1.378399784182325, valid_loss: 1.4962637689378526\n",
            "test_ind: 3, Epoch: 1326, train_loss: 1.4334198104010687, valid_loss: 1.4036625579551414\n",
            "test_ind: 3, Epoch: 1327, train_loss: 1.407858142146358, valid_loss: 1.4029489446569372\n",
            "test_ind: 3, Epoch: 1328, train_loss: 1.5056318118248457, valid_loss: 1.468671516135887\n",
            "test_ind: 3, Epoch: 1329, train_loss: 1.454420784373342, valid_loss: 1.501123657932988\n",
            "test_ind: 3, Epoch: 1330, train_loss: 1.310664865705702, valid_loss: 1.3030187288920085\n",
            "test_ind: 3, Epoch: 1331, train_loss: 1.401863698606138, valid_loss: 1.3513017583776403\n",
            "test_ind: 3, Epoch: 1332, train_loss: 1.434235749421296, valid_loss: 1.1859016241850675\n",
            "test_ind: 3, Epoch: 1333, train_loss: 1.3044425293251318, valid_loss: 1.3658788645709001\n",
            "test_ind: 3, Epoch: 1334, train_loss: 1.3627619978822307, valid_loss: 1.4950468275282118\n",
            "test_ind: 3, Epoch: 1335, train_loss: 1.3752004894209495, valid_loss: 1.566975787833885\n",
            "test_ind: 3, Epoch: 1336, train_loss: 1.461735690081561, valid_loss: 1.349422083960639\n",
            "test_ind: 3, Epoch: 1337, train_loss: 1.4462460647394626, valid_loss: 1.4696841769748266\n",
            "test_ind: 3, Epoch: 1338, train_loss: 1.3677212456126269, valid_loss: 1.2581608207137496\n",
            "test_ind: 3, Epoch: 1339, train_loss: 1.5493123384169591, valid_loss: 1.3855236724570945\n",
            "test_ind: 3, Epoch: 1340, train_loss: 1.3204180517314392, valid_loss: 1.4813811337506329\n",
            "test_ind: 3, Epoch: 1341, train_loss: 1.4349799273926536, valid_loss: 1.3704462581210668\n",
            "test_ind: 3, Epoch: 1342, train_loss: 1.5531129895904918, valid_loss: 1.5158305874577276\n",
            "test_ind: 3, Epoch: 1343, train_loss: 1.4492199097150635, valid_loss: 1.3032938815929271\n",
            "test_ind: 3, Epoch: 1344, train_loss: 1.3423427652429654, valid_loss: 1.3964832800405995\n",
            "test_ind: 3, Epoch: 1345, train_loss: 1.3641156090630426, valid_loss: 1.2387168142530653\n",
            "test_ind: 3, Epoch: 1346, train_loss: 1.3036637953770012, valid_loss: 1.6726452686168531\n",
            "test_ind: 3, Epoch: 1347, train_loss: 1.4331195560502417, valid_loss: 1.4051216973198786\n",
            "test_ind: 3, Epoch: 1348, train_loss: 1.463613686738191, valid_loss: 1.5392369517573603\n",
            "test_ind: 3, Epoch: 1349, train_loss: 1.4001744882560072, valid_loss: 1.3003270007945873\n",
            "test_ind: 3, Epoch: 1350, train_loss: 1.5177110036214192, valid_loss: 1.416638692220052\n",
            "test_ind: 3, Epoch: 1351, train_loss: 1.386948114559974, valid_loss: 1.392577471556487\n",
            "test_ind: 3, Epoch: 1352, train_loss: 1.3961478103826075, valid_loss: 1.2053355817441587\n",
            "test_ind: 3, Epoch: 1353, train_loss: 1.3776143156451943, valid_loss: 1.417695239738182\n",
            "test_ind: 3, Epoch: 1354, train_loss: 1.4299659729003906, valid_loss: 1.350093400036847\n",
            "test_ind: 3, Epoch: 1355, train_loss: 1.3842407097051175, valid_loss: 1.2247395162229184\n",
            "test_ind: 3, Epoch: 1356, train_loss: 1.401622301266517, valid_loss: 1.4180698571381747\n",
            "test_ind: 3, Epoch: 1357, train_loss: 1.360350944377758, valid_loss: 1.3380705868756329\n",
            "test_ind: 3, Epoch: 1358, train_loss: 1.39024512561751, valid_loss: 1.5268212247777868\n",
            "test_ind: 3, Epoch: 1359, train_loss: 1.4136628515926408, valid_loss: 1.3319070957325123\n",
            "test_ind: 3, Epoch: 1360, train_loss: 1.4112794487564655, valid_loss: 1.5298569820545338\n",
            "test_ind: 3, Epoch: 1361, train_loss: 1.3683807879318426, valid_loss: 1.764622935542354\n",
            "test_ind: 3, Epoch: 1362, train_loss: 1.4681386829894267, valid_loss: 1.4737407189828375\n",
            "test_ind: 3, Epoch: 1363, train_loss: 1.4503236170168274, valid_loss: 1.5513915485805936\n",
            "test_ind: 3, Epoch: 1364, train_loss: 1.412040604485406, valid_loss: 1.5236956455089428\n",
            "test_ind: 3, Epoch: 1365, train_loss: 1.5121498402254083, valid_loss: 1.233671930101183\n",
            "test_ind: 3, Epoch: 1366, train_loss: 1.4722378518846302, valid_loss: 1.483947506657353\n",
            "test_ind: 3, Epoch: 1367, train_loss: 1.4747160923333815, valid_loss: 1.228659329590974\n",
            "test_ind: 3, Epoch: 1368, train_loss: 1.4748077863528402, valid_loss: 1.5141700285452382\n",
            "test_ind: 3, Epoch: 1369, train_loss: 1.3709462954674236, valid_loss: 1.390799186847828\n",
            "test_ind: 3, Epoch: 1370, train_loss: 1.461294733447793, valid_loss: 1.3459066108421043\n",
            "test_ind: 3, Epoch: 1371, train_loss: 1.381999009921227, valid_loss: 1.3536976178487141\n",
            "test_ind: 3, Epoch: 1372, train_loss: 1.416449929461067, valid_loss: 1.351377010345459\n",
            "test_ind: 3, Epoch: 1373, train_loss: 1.4117498986515, valid_loss: 1.3788714408874512\n",
            "test_ind: 3, Epoch: 1374, train_loss: 1.4174044632617337, valid_loss: 1.3312815560234919\n",
            "test_ind: 3, Epoch: 1375, train_loss: 1.3616871539457345, valid_loss: 1.302284523292824\n",
            "test_ind: 3, Epoch: 1376, train_loss: 1.3310623581026808, valid_loss: 1.4178878466288247\n",
            "test_ind: 3, Epoch: 1377, train_loss: 1.3095912050317835, valid_loss: 1.427902098055239\n",
            "test_ind: 3, Epoch: 1378, train_loss: 1.3600221151187097, valid_loss: 1.262602276272244\n",
            "test_ind: 3, Epoch: 1379, train_loss: 1.336752497119668, valid_loss: 1.2918929347285517\n",
            "test_ind: 3, Epoch: 1380, train_loss: 1.2793345627961334, valid_loss: 1.2641785939534507\n",
            "test_ind: 3, Epoch: 1381, train_loss: 1.3724899468598541, valid_loss: 1.4113442279674389\n",
            "test_ind: 3, Epoch: 1382, train_loss: 1.5191595524917414, valid_loss: 1.5247719376175493\n",
            "test_ind: 3, Epoch: 1383, train_loss: 1.3345652803962613, valid_loss: 1.50566777476558\n",
            "test_ind: 3, Epoch: 1384, train_loss: 1.3688150335241247, valid_loss: 1.4528384915104617\n",
            "test_ind: 3, Epoch: 1385, train_loss: 1.3476768010928306, valid_loss: 1.4260370289837874\n",
            "test_ind: 3, Epoch: 1386, train_loss: 1.3770756015071162, valid_loss: 1.1942898079201028\n",
            "test_ind: 3, Epoch: 1387, train_loss: 1.4434839001408328, valid_loss: 1.7155544492933483\n",
            "test_ind: 3, Epoch: 1388, train_loss: 1.4281520431424366, valid_loss: 1.3087524484705044\n",
            "test_ind: 3, Epoch: 1389, train_loss: 1.3774405938607674, valid_loss: 1.4399346422266077\n",
            "test_ind: 3, Epoch: 1390, train_loss: 1.377835750579834, valid_loss: 1.293432253378409\n",
            "test_ind: 3, Epoch: 1391, train_loss: 1.4151674906412761, valid_loss: 1.2479466685542353\n",
            "test_ind: 3, Epoch: 1392, train_loss: 1.3179037306043837, valid_loss: 1.3541895371896249\n",
            "test_ind: 3, Epoch: 1393, train_loss: 1.3441952893763414, valid_loss: 1.3280945354037814\n",
            "test_ind: 3, Epoch: 1394, train_loss: 1.4833896660510404, valid_loss: 1.672053089848271\n",
            "test_ind: 3, Epoch: 1395, train_loss: 1.482611985854161, valid_loss: 1.4426856217560946\n",
            "test_ind: 3, Epoch: 1396, train_loss: 1.3540885713365343, valid_loss: 1.181216875712077\n",
            "Validation loss decreased (1.1833788907086409 --> 1.181216875712077).  Saving model ...\n",
            "test_ind: 3, Epoch: 1397, train_loss: 1.3182627536632399, valid_loss: 1.2508882593225548\n",
            "test_ind: 3, Epoch: 1398, train_loss: 1.3766713201263805, valid_loss: 1.7333186114275896\n",
            "test_ind: 3, Epoch: 1399, train_loss: 1.4698535248085305, valid_loss: 1.407662550608317\n",
            "test_ind: 3, Epoch: 1400, train_loss: 1.374789544093756, valid_loss: 1.263302520469383\n",
            "test_ind: 3, Epoch: 1401, train_loss: 1.5350916356216244, valid_loss: 1.9494258386117438\n",
            "test_ind: 3, Epoch: 1402, train_loss: 1.3651803216816467, valid_loss: 1.1950293470312048\n",
            "test_ind: 3, Epoch: 1403, train_loss: 1.389788097805447, valid_loss: 1.819872185035988\n",
            "test_ind: 3, Epoch: 1404, train_loss: 1.363098274042577, valid_loss: 1.4211308338023998\n",
            "test_ind: 3, Epoch: 1405, train_loss: 1.3360884866596743, valid_loss: 1.2880660163031683\n",
            "test_ind: 3, Epoch: 1406, train_loss: 1.4056061403250988, valid_loss: 1.3045936690436468\n",
            "test_ind: 3, Epoch: 1407, train_loss: 1.3691117616347324, valid_loss: 1.30324829949273\n",
            "test_ind: 3, Epoch: 1408, train_loss: 1.4499471217025943, valid_loss: 1.4288306766086154\n",
            "test_ind: 3, Epoch: 1409, train_loss: 1.3697356412440167, valid_loss: 1.234193660594799\n",
            "test_ind: 3, Epoch: 1410, train_loss: 1.220507992638482, valid_loss: 1.527463400805438\n",
            "test_ind: 3, Epoch: 1411, train_loss: 1.3797678770842376, valid_loss: 1.3253217449894659\n",
            "test_ind: 3, Epoch: 1412, train_loss: 1.3867548247914254, valid_loss: 1.2623687143679019\n",
            "test_ind: 3, Epoch: 1413, train_loss: 1.3105565530282481, valid_loss: 1.2163349434181496\n",
            "test_ind: 3, Epoch: 1414, train_loss: 1.3162867287058888, valid_loss: 1.1600217466001157\n",
            "Validation loss decreased (1.181216875712077 --> 1.1600217466001157).  Saving model ...\n",
            "test_ind: 3, Epoch: 1415, train_loss: 1.2811796105938196, valid_loss: 1.255061732398139\n",
            "test_ind: 3, Epoch: 1416, train_loss: 1.2725020514594183, valid_loss: 1.3800061720388905\n",
            "test_ind: 3, Epoch: 1417, train_loss: 1.5632736888932595, valid_loss: 1.549098880202682\n",
            "test_ind: 3, Epoch: 1418, train_loss: 1.3268009291754825, valid_loss: 1.3806098655418113\n",
            "test_ind: 3, Epoch: 1419, train_loss: 1.4656655288036957, valid_loss: 1.284765402475993\n",
            "test_ind: 3, Epoch: 1420, train_loss: 1.3637843956182032, valid_loss: 1.478102613378454\n",
            "test_ind: 3, Epoch: 1421, train_loss: 1.3745274013943143, valid_loss: 1.5009026174192073\n",
            "test_ind: 3, Epoch: 1422, train_loss: 1.307914698565448, valid_loss: 1.3024337026807995\n",
            "test_ind: 3, Epoch: 1423, train_loss: 1.3890761858151282, valid_loss: 1.331248159761782\n",
            "test_ind: 3, Epoch: 1424, train_loss: 1.2819368692091955, valid_loss: 1.3108930234555847\n",
            "test_ind: 3, Epoch: 1425, train_loss: 1.3149247404969768, valid_loss: 1.511927057195593\n",
            "test_ind: 3, Epoch: 1426, train_loss: 1.3957490862151725, valid_loss: 1.3525128894382052\n",
            "test_ind: 3, Epoch: 1427, train_loss: 1.353609314671269, valid_loss: 1.4134515479758933\n",
            "test_ind: 3, Epoch: 1428, train_loss: 1.3513584666781955, valid_loss: 1.2129598723517523\n",
            "test_ind: 3, Epoch: 1429, train_loss: 1.217867456836465, valid_loss: 1.2527017240171079\n",
            "test_ind: 3, Epoch: 1430, train_loss: 1.3205891185336642, valid_loss: 1.4330725140041776\n",
            "test_ind: 3, Epoch: 1431, train_loss: 1.3951776292588975, valid_loss: 1.2423679033915203\n",
            "test_ind: 3, Epoch: 1432, train_loss: 1.2941208003479758, valid_loss: 1.232073148091634\n",
            "test_ind: 3, Epoch: 1433, train_loss: 1.284412348711932, valid_loss: 1.150402757856581\n",
            "Validation loss decreased (1.1600217466001157 --> 1.150402757856581).  Saving model ...\n",
            "test_ind: 3, Epoch: 1434, train_loss: 1.3554969540348756, valid_loss: 1.468207253350152\n",
            "test_ind: 3, Epoch: 1435, train_loss: 1.2820141815844879, valid_loss: 1.216615429630986\n",
            "test_ind: 3, Epoch: 1436, train_loss: 1.3693466304261008, valid_loss: 1.2670361200968425\n",
            "test_ind: 3, Epoch: 1437, train_loss: 1.3965441503642517, valid_loss: 1.2964043440642181\n",
            "test_ind: 3, Epoch: 1438, train_loss: 1.3591438752633556, valid_loss: 1.2463766910411693\n",
            "test_ind: 3, Epoch: 1439, train_loss: 1.3161021691781503, valid_loss: 1.4381500173498083\n",
            "test_ind: 3, Epoch: 1440, train_loss: 1.3595125115947957, valid_loss: 1.311119662390815\n",
            "test_ind: 3, Epoch: 1441, train_loss: 1.3787226205990637, valid_loss: 1.2777391716285988\n",
            "test_ind: 3, Epoch: 1442, train_loss: 1.4110874541011857, valid_loss: 1.4846589653580278\n",
            "test_ind: 3, Epoch: 1443, train_loss: 1.3011426101496189, valid_loss: 1.2294918519479257\n",
            "test_ind: 3, Epoch: 1444, train_loss: 1.3216171617861145, valid_loss: 1.3926981820000544\n",
            "test_ind: 3, Epoch: 1445, train_loss: 1.285539338618149, valid_loss: 1.4017725697270147\n",
            "test_ind: 3, Epoch: 1446, train_loss: 1.2538012222007469, valid_loss: 1.5964503288269043\n",
            "test_ind: 3, Epoch: 1447, train_loss: 1.3348884464781963, valid_loss: 1.4195618629455566\n",
            "test_ind: 3, Epoch: 1448, train_loss: 1.447374909012406, valid_loss: 1.5733335812886557\n",
            "test_ind: 3, Epoch: 1449, train_loss: 1.2361665066377618, valid_loss: 1.3305235262270327\n",
            "test_ind: 3, Epoch: 1450, train_loss: 1.3835734143669223, valid_loss: 1.3504399723476834\n",
            "test_ind: 3, Epoch: 1451, train_loss: 1.3098137643602157, valid_loss: 1.1749653463010437\n",
            "test_ind: 3, Epoch: 1452, train_loss: 1.2804362744460869, valid_loss: 1.268914434644911\n",
            "test_ind: 3, Epoch: 1453, train_loss: 1.3169196505605438, valid_loss: 1.2981713259661638\n",
            "test_ind: 3, Epoch: 1454, train_loss: 1.3384349022382571, valid_loss: 1.3425443790577076\n",
            "test_ind: 3, Epoch: 1455, train_loss: 1.331693007622236, valid_loss: 1.602494646001745\n",
            "test_ind: 3, Epoch: 1456, train_loss: 1.2377123302883573, valid_loss: 1.2553488236886483\n",
            "test_ind: 3, Epoch: 1457, train_loss: 1.4327957659591863, valid_loss: 1.4671419991387262\n",
            "test_ind: 3, Epoch: 1458, train_loss: 1.3563824936195656, valid_loss: 1.327049590923168\n",
            "test_ind: 3, Epoch: 1459, train_loss: 1.368256362867944, valid_loss: 1.8762148751152887\n",
            "test_ind: 3, Epoch: 1460, train_loss: 1.3316259089811349, valid_loss: 1.3270296697263364\n",
            "test_ind: 3, Epoch: 1461, train_loss: 1.3103687203960654, valid_loss: 1.2222287213360823\n",
            "test_ind: 3, Epoch: 1462, train_loss: 1.3229430634298442, valid_loss: 1.2496759273387767\n",
            "test_ind: 3, Epoch: 1463, train_loss: 1.3323553815300082, valid_loss: 1.42255018375538\n",
            "test_ind: 3, Epoch: 1464, train_loss: 1.3946745189619651, valid_loss: 1.4921751198945221\n",
            "test_ind: 3, Epoch: 1465, train_loss: 1.4089955341668776, valid_loss: 1.4922728891725894\n",
            "test_ind: 3, Epoch: 1466, train_loss: 1.3679509633853113, valid_loss: 1.209412769035057\n",
            "test_ind: 3, Epoch: 1467, train_loss: 1.3130845729215646, valid_loss: 1.2662198631851762\n",
            "test_ind: 3, Epoch: 1468, train_loss: 1.3951548058309675, valid_loss: 1.4212918104948822\n",
            "test_ind: 3, Epoch: 1469, train_loss: 1.2913393444485137, valid_loss: 1.1754477818806968\n",
            "test_ind: 3, Epoch: 1470, train_loss: 1.3973712744536222, valid_loss: 1.6936215118125633\n",
            "test_ind: 3, Epoch: 1471, train_loss: 1.3984666518223137, valid_loss: 1.7844771455835415\n",
            "test_ind: 3, Epoch: 1472, train_loss: 1.3933189533374928, valid_loss: 1.9263422577469436\n",
            "test_ind: 3, Epoch: 1473, train_loss: 1.3460241600319194, valid_loss: 1.3431168838783547\n",
            "test_ind: 3, Epoch: 1474, train_loss: 1.278413230990186, valid_loss: 1.179412983081959\n",
            "test_ind: 3, Epoch: 1475, train_loss: 1.4052568835976684, valid_loss: 1.5900645785861545\n",
            "test_ind: 3, Epoch: 1476, train_loss: 1.414408571926164, valid_loss: 1.2410087585449219\n",
            "test_ind: 3, Epoch: 1477, train_loss: 1.3465749775921858, valid_loss: 1.2905434679102015\n",
            "test_ind: 3, Epoch: 1478, train_loss: 1.3667968467429832, valid_loss: 1.3290392557779949\n",
            "test_ind: 3, Epoch: 1479, train_loss: 1.3228411144680445, valid_loss: 1.364620526631673\n",
            "test_ind: 3, Epoch: 1480, train_loss: 1.2898390381424516, valid_loss: 1.3383569540800873\n",
            "test_ind: 3, Epoch: 1481, train_loss: 1.3558235580538527, valid_loss: 1.2276835264983001\n",
            "test_ind: 3, Epoch: 1482, train_loss: 1.384290995421233, valid_loss: 1.1955750430071796\n",
            "test_ind: 3, Epoch: 1483, train_loss: 1.2993501674981767, valid_loss: 1.2048638308489765\n",
            "test_ind: 3, Epoch: 1484, train_loss: 1.455757682706103, valid_loss: 1.9967653839676465\n",
            "test_ind: 3, Epoch: 1485, train_loss: 1.2602083064891674, valid_loss: 1.221545449009648\n",
            "test_ind: 3, Epoch: 1486, train_loss: 1.2802718715903199, valid_loss: 1.2391986846923828\n",
            "test_ind: 3, Epoch: 1487, train_loss: 1.3117779272573966, valid_loss: 1.3671784400939944\n",
            "test_ind: 3, Epoch: 1488, train_loss: 1.2529321835364824, valid_loss: 1.3019075746889466\n",
            "test_ind: 3, Epoch: 1489, train_loss: 1.2840771439634724, valid_loss: 1.2872310037966126\n",
            "test_ind: 3, Epoch: 1490, train_loss: 1.3156667403232905, valid_loss: 1.2607312379059967\n",
            "test_ind: 3, Epoch: 1491, train_loss: 1.3378548033443496, valid_loss: 1.1844703533031322\n",
            "test_ind: 3, Epoch: 1492, train_loss: 1.315847620551969, valid_loss: 1.6516320970323348\n",
            "test_ind: 3, Epoch: 1493, train_loss: 1.2370646735768258, valid_loss: 1.2778349099335846\n",
            "test_ind: 3, Epoch: 1494, train_loss: 1.3484098057688019, valid_loss: 1.4377061349374278\n",
            "test_ind: 3, Epoch: 1495, train_loss: 1.3502266613053688, valid_loss: 1.4191532841435186\n",
            "test_ind: 3, Epoch: 1496, train_loss: 1.338085769135275, valid_loss: 1.3331436757688169\n",
            "test_ind: 3, Epoch: 1497, train_loss: 1.305747679722162, valid_loss: 1.3455201784769695\n",
            "test_ind: 3, Epoch: 1498, train_loss: 1.3154415201257774, valid_loss: 1.2100309619197138\n",
            "test_ind: 3, Epoch: 1499, train_loss: 1.355290424676589, valid_loss: 1.4586990497730397\n",
            "test_ind: 3, Epoch: 1500, train_loss: 1.2781994489975914, valid_loss: 1.2019619411892362\n",
            "test_ind: 3, Epoch: 1501, train_loss: 1.349293585176821, valid_loss: 1.3127950915583857\n",
            "test_ind: 3, Epoch: 1502, train_loss: 1.3427560064527724, valid_loss: 1.404083322595667\n",
            "test_ind: 3, Epoch: 1503, train_loss: 1.3908936535870584, valid_loss: 1.4424662943239566\n",
            "test_ind: 3, Epoch: 1504, train_loss: 1.4166459213068456, valid_loss: 1.4100504627934207\n",
            "test_ind: 3, Epoch: 1505, train_loss: 1.366119702657064, valid_loss: 1.416244489175302\n",
            "test_ind: 3, Epoch: 1506, train_loss: 1.3295747144722645, valid_loss: 1.2849848005506728\n",
            "test_ind: 3, Epoch: 1507, train_loss: 1.263633934068091, valid_loss: 1.2334704045896177\n",
            "test_ind: 3, Epoch: 1508, train_loss: 1.303162298084777, valid_loss: 1.3672339651319714\n",
            "test_ind: 3, Epoch: 1509, train_loss: 1.2446245028648848, valid_loss: 1.389134989844428\n",
            "test_ind: 3, Epoch: 1510, train_loss: 1.212760736912857, valid_loss: 1.2611515963519062\n",
            "test_ind: 3, Epoch: 1511, train_loss: 1.3089289017665535, valid_loss: 1.269119245034677\n",
            "test_ind: 3, Epoch: 1512, train_loss: 1.3357707188453203, valid_loss: 1.614692970558449\n",
            "test_ind: 3, Epoch: 1513, train_loss: 1.3310345543755424, valid_loss: 1.755626254611545\n",
            "test_ind: 3, Epoch: 1514, train_loss: 1.37272388552442, valid_loss: 1.3748053444756403\n",
            "test_ind: 3, Epoch: 1515, train_loss: 1.3044287422556935, valid_loss: 1.2147720124986436\n",
            "test_ind: 3, Epoch: 1516, train_loss: 1.3752621132650495, valid_loss: 1.23571217501605\n",
            "test_ind: 3, Epoch: 1517, train_loss: 1.2816788178903087, valid_loss: 1.2554421954684787\n",
            "test_ind: 3, Epoch: 1518, train_loss: 1.2745579613579645, valid_loss: 1.2738944866039135\n",
            "test_ind: 3, Epoch: 1519, train_loss: 1.3545236587524414, valid_loss: 1.7909492916531033\n",
            "test_ind: 3, Epoch: 1520, train_loss: 1.2530529351881994, valid_loss: 1.249740141409415\n",
            "test_ind: 3, Epoch: 1521, train_loss: 1.265102845651132, valid_loss: 1.4960498279995387\n",
            "test_ind: 3, Epoch: 1522, train_loss: 1.3814099158769775, valid_loss: 1.3327656322055392\n",
            "test_ind: 3, Epoch: 1523, train_loss: 1.4389245362929357, valid_loss: 1.4391893634089719\n",
            "test_ind: 3, Epoch: 1524, train_loss: 1.2503448062472873, valid_loss: 1.2923721737331815\n",
            "test_ind: 3, Epoch: 1525, train_loss: 1.3533072471618655, valid_loss: 1.1670285330878363\n",
            "test_ind: 3, Epoch: 1526, train_loss: 1.4419045919253501, valid_loss: 1.5195717281765408\n",
            "test_ind: 3, Epoch: 1527, train_loss: 1.2954258271205572, valid_loss: 1.1535866525438097\n",
            "test_ind: 3, Epoch: 1528, train_loss: 1.3275180098451214, valid_loss: 1.3555087336787472\n",
            "test_ind: 3, Epoch: 1529, train_loss: 1.2687856120827756, valid_loss: 1.273799984543412\n",
            "test_ind: 3, Epoch: 1530, train_loss: 1.3452192647957508, valid_loss: 1.1979384598908602\n",
            "test_ind: 3, Epoch: 1531, train_loss: 1.336929386044726, valid_loss: 1.347307805661802\n",
            "test_ind: 3, Epoch: 1532, train_loss: 1.369210166695677, valid_loss: 1.5274278852674696\n",
            "test_ind: 3, Epoch: 1533, train_loss: 1.242150330249174, valid_loss: 1.1492572183962222\n",
            "Validation loss decreased (1.150402757856581 --> 1.1492572183962222).  Saving model ...\n",
            "test_ind: 3, Epoch: 1534, train_loss: 1.3452726175755634, valid_loss: 1.357118571246112\n",
            "test_ind: 3, Epoch: 1535, train_loss: 1.3288138766347628, valid_loss: 1.5122711216961897\n",
            "test_ind: 3, Epoch: 1536, train_loss: 1.3306951522827148, valid_loss: 1.4033781864024975\n",
            "test_ind: 3, Epoch: 1537, train_loss: 1.4053398002812894, valid_loss: 1.2947421603732638\n",
            "test_ind: 3, Epoch: 1538, train_loss: 1.2986925148669586, valid_loss: 1.1196668412950306\n",
            "Validation loss decreased (1.1492572183962222 --> 1.1196668412950306).  Saving model ...\n",
            "test_ind: 3, Epoch: 1539, train_loss: 1.3248712574994121, valid_loss: 1.324982484181722\n",
            "test_ind: 3, Epoch: 1540, train_loss: 1.3205852744020061, valid_loss: 1.4875511063469782\n",
            "test_ind: 3, Epoch: 1541, train_loss: 1.2834380526601532, valid_loss: 1.2937477429707844\n",
            "test_ind: 3, Epoch: 1542, train_loss: 1.312727262944351, valid_loss: 1.3677796787685819\n",
            "test_ind: 3, Epoch: 1543, train_loss: 1.361256829014531, valid_loss: 1.2020410078543204\n",
            "test_ind: 3, Epoch: 1544, train_loss: 1.2306346775572976, valid_loss: 1.2994563844468858\n",
            "test_ind: 3, Epoch: 1545, train_loss: 1.2309858004252117, valid_loss: 1.2307429137053312\n",
            "test_ind: 3, Epoch: 1546, train_loss: 1.3151282793209877, valid_loss: 1.3568609555562336\n",
            "test_ind: 3, Epoch: 1547, train_loss: 1.3227655210612732, valid_loss: 1.4948598367196542\n",
            "test_ind: 3, Epoch: 1548, train_loss: 1.2895049106927567, valid_loss: 1.1591401806584112\n",
            "test_ind: 3, Epoch: 1549, train_loss: 1.2474380834603014, valid_loss: 1.3668486100656017\n",
            "test_ind: 3, Epoch: 1550, train_loss: 1.295734629218961, valid_loss: 1.1543283285918058\n",
            "test_ind: 3, Epoch: 1551, train_loss: 1.2471243069495686, valid_loss: 1.1068939103020563\n",
            "Validation loss decreased (1.1196668412950306 --> 1.1068939103020563).  Saving model ...\n",
            "test_ind: 3, Epoch: 1552, train_loss: 1.2272592061831629, valid_loss: 1.3103575000056515\n",
            "test_ind: 3, Epoch: 1553, train_loss: 1.3427853231076838, valid_loss: 1.3614573655305084\n",
            "test_ind: 3, Epoch: 1554, train_loss: 1.3603075051013334, valid_loss: 1.480738922401711\n",
            "test_ind: 3, Epoch: 1555, train_loss: 1.3578664344034077, valid_loss: 1.272550106048584\n",
            "test_ind: 3, Epoch: 1556, train_loss: 1.287417405917321, valid_loss: 1.472206468935366\n",
            "test_ind: 3, Epoch: 1557, train_loss: 1.2313133169103554, valid_loss: 1.3481360364843298\n",
            "test_ind: 3, Epoch: 1558, train_loss: 1.3637657106658558, valid_loss: 1.2950011359320746\n",
            "test_ind: 3, Epoch: 1559, train_loss: 1.337742546458303, valid_loss: 1.3311500372710052\n",
            "test_ind: 3, Epoch: 1560, train_loss: 1.37765876746472, valid_loss: 1.283829441776982\n",
            "test_ind: 3, Epoch: 1561, train_loss: 1.29010589034469, valid_loss: 1.271372883408158\n",
            "test_ind: 3, Epoch: 1562, train_loss: 1.2238497734069824, valid_loss: 1.255685400079798\n",
            "test_ind: 3, Epoch: 1563, train_loss: 1.3127854194170163, valid_loss: 1.238722112443712\n",
            "test_ind: 3, Epoch: 1564, train_loss: 1.3288075482403792, valid_loss: 1.317057556576199\n",
            "test_ind: 3, Epoch: 1565, train_loss: 1.2686352729797366, valid_loss: 1.2419417875784415\n",
            "test_ind: 3, Epoch: 1566, train_loss: 1.2449700390851055, valid_loss: 1.1111955642700195\n",
            "test_ind: 3, Epoch: 1567, train_loss: 1.372492207421197, valid_loss: 1.5666538344489205\n",
            "test_ind: 3, Epoch: 1568, train_loss: 1.3748087176570187, valid_loss: 1.3887365305865251\n",
            "test_ind: 3, Epoch: 1569, train_loss: 1.3554517545817812, valid_loss: 1.5962683712994612\n",
            "test_ind: 3, Epoch: 1570, train_loss: 1.3121455687063712, valid_loss: 1.3826137295475713\n",
            "test_ind: 3, Epoch: 1571, train_loss: 1.288095745039575, valid_loss: 1.2216647642630118\n",
            "test_ind: 3, Epoch: 1572, train_loss: 1.3440660017508048, valid_loss: 1.1935539422211823\n",
            "test_ind: 3, Epoch: 1573, train_loss: 1.314009536931544, valid_loss: 1.4976263046264648\n",
            "test_ind: 3, Epoch: 1574, train_loss: 1.337806566261951, valid_loss: 1.1547457023903176\n",
            "test_ind: 3, Epoch: 1575, train_loss: 1.3640479158472134, valid_loss: 1.4973612361484105\n",
            "test_ind: 3, Epoch: 1576, train_loss: 1.2549747007864491, valid_loss: 1.2856505711873374\n",
            "test_ind: 3, Epoch: 1577, train_loss: 1.2552774923819083, valid_loss: 1.1261260244581435\n",
            "test_ind: 3, Epoch: 1578, train_loss: 1.2333766560495636, valid_loss: 1.2359169677451805\n",
            "test_ind: 3, Epoch: 1579, train_loss: 1.2234596676296658, valid_loss: 1.2559487907974807\n",
            "test_ind: 3, Epoch: 1580, train_loss: 1.341021732047752, valid_loss: 1.4301098894189905\n",
            "test_ind: 3, Epoch: 1581, train_loss: 1.3042452305923273, valid_loss: 1.1941179522761594\n",
            "test_ind: 3, Epoch: 1582, train_loss: 1.2329249028806333, valid_loss: 1.198734371750443\n",
            "test_ind: 3, Epoch: 1583, train_loss: 1.3928579342218095, valid_loss: 1.568398811199047\n",
            "test_ind: 3, Epoch: 1584, train_loss: 1.2426607343885634, valid_loss: 1.1592042534439653\n",
            "test_ind: 3, Epoch: 1585, train_loss: 1.2004714365358706, valid_loss: 1.262525540811044\n",
            "test_ind: 3, Epoch: 1586, train_loss: 1.2808978116070782, valid_loss: 1.6950317488776312\n",
            "test_ind: 3, Epoch: 1587, train_loss: 1.3166450983212319, valid_loss: 1.3112331496344674\n",
            "test_ind: 3, Epoch: 1588, train_loss: 1.2315799042030617, valid_loss: 1.269455591837565\n",
            "test_ind: 3, Epoch: 1589, train_loss: 1.2473906882015275, valid_loss: 1.081905064759431\n",
            "Validation loss decreased (1.1068939103020563 --> 1.081905064759431).  Saving model ...\n",
            "test_ind: 3, Epoch: 1590, train_loss: 1.2553865880142023, valid_loss: 1.4244226702937375\n",
            "test_ind: 3, Epoch: 1591, train_loss: 1.2317532374535078, valid_loss: 1.220665490185773\n",
            "test_ind: 3, Epoch: 1592, train_loss: 1.2640094286129797, valid_loss: 1.2334913147820368\n",
            "test_ind: 3, Epoch: 1593, train_loss: 1.3465175098843045, valid_loss: 1.1764050589667425\n",
            "test_ind: 3, Epoch: 1594, train_loss: 1.2847844347541713, valid_loss: 1.363207428543656\n",
            "test_ind: 3, Epoch: 1595, train_loss: 1.2452551406106833, valid_loss: 1.1069420002124928\n",
            "test_ind: 3, Epoch: 1596, train_loss: 1.2467202610439723, valid_loss: 1.132921854654948\n",
            "test_ind: 3, Epoch: 1597, train_loss: 1.1888643076390395, valid_loss: 1.2468627647117332\n",
            "test_ind: 3, Epoch: 1598, train_loss: 1.3004244757287293, valid_loss: 1.5350215346724898\n",
            "test_ind: 3, Epoch: 1599, train_loss: 1.2679616727946714, valid_loss: 1.3736222938255027\n",
            "test_ind: 3, Epoch: 1600, train_loss: 1.2634228659264835, valid_loss: 1.6181897763852722\n",
            "test_ind: 3, Epoch: 1601, train_loss: 1.2677125989654918, valid_loss: 1.212022481141267\n",
            "test_ind: 3, Epoch: 1602, train_loss: 1.3361763895293814, valid_loss: 1.416212929619683\n",
            "test_ind: 3, Epoch: 1603, train_loss: 1.363889670666353, valid_loss: 1.3007031016879613\n",
            "test_ind: 3, Epoch: 1604, train_loss: 1.3003713407634216, valid_loss: 1.190891901652018\n",
            "test_ind: 3, Epoch: 1605, train_loss: 1.3700047422338417, valid_loss: 1.267769425003617\n",
            "test_ind: 3, Epoch: 1606, train_loss: 1.2554526034696605, valid_loss: 1.1076217050905584\n",
            "test_ind: 3, Epoch: 1607, train_loss: 1.1900805779445318, valid_loss: 1.5133352809482152\n",
            "test_ind: 3, Epoch: 1608, train_loss: 1.2276503245035808, valid_loss: 1.0733269762109827\n",
            "Validation loss decreased (1.081905064759431 --> 1.0733269762109827).  Saving model ...\n",
            "test_ind: 3, Epoch: 1609, train_loss: 1.2911579991564337, valid_loss: 1.1862807980290166\n",
            "test_ind: 3, Epoch: 1610, train_loss: 1.371976934833291, valid_loss: 1.2562883694966633\n",
            "test_ind: 3, Epoch: 1611, train_loss: 1.3320926442558385, valid_loss: 1.3291702270507812\n",
            "test_ind: 3, Epoch: 1612, train_loss: 1.2706440289815266, valid_loss: 1.4453548148826316\n",
            "test_ind: 3, Epoch: 1613, train_loss: 1.221710658367769, valid_loss: 1.4235806288542572\n",
            "test_ind: 3, Epoch: 1614, train_loss: 1.225479773533197, valid_loss: 1.2358200285169814\n",
            "test_ind: 3, Epoch: 1615, train_loss: 1.3041761833944439, valid_loss: 1.3376411155418113\n",
            "test_ind: 3, Epoch: 1616, train_loss: 1.2647659454816653, valid_loss: 1.232585041611283\n",
            "test_ind: 3, Epoch: 1617, train_loss: 1.232853459723202, valid_loss: 1.35340565222281\n",
            "test_ind: 3, Epoch: 1618, train_loss: 1.3112765594764992, valid_loss: 1.3048926282812048\n",
            "test_ind: 3, Epoch: 1619, train_loss: 1.3983222172584064, valid_loss: 1.4389612939622667\n",
            "test_ind: 3, Epoch: 1620, train_loss: 1.2825190285105765, valid_loss: 1.1676734994958948\n",
            "test_ind: 3, Epoch: 1621, train_loss: 1.2665399857509283, valid_loss: 1.2796073136506259\n",
            "test_ind: 3, Epoch: 1622, train_loss: 1.2914276123046875, valid_loss: 1.1175839636060927\n",
            "test_ind: 3, Epoch: 1623, train_loss: 1.2890667267787606, valid_loss: 1.2759108720002352\n",
            "test_ind: 3, Epoch: 1624, train_loss: 1.1822380018822942, valid_loss: 1.2255027559068468\n",
            "test_ind: 3, Epoch: 1625, train_loss: 1.1995941974498607, valid_loss: 1.3847586137277108\n",
            "test_ind: 3, Epoch: 1626, train_loss: 1.30725947132817, valid_loss: 1.2090689871046278\n",
            "test_ind: 3, Epoch: 1627, train_loss: 1.312053798157492, valid_loss: 1.1941284780149106\n",
            "test_ind: 3, Epoch: 1628, train_loss: 1.3033059261463307, valid_loss: 1.238087565810592\n",
            "test_ind: 3, Epoch: 1629, train_loss: 1.2255432340833876, valid_loss: 1.2030126253763835\n",
            "test_ind: 3, Epoch: 1630, train_loss: 1.2010961874031727, valid_loss: 1.1268377657289859\n",
            "test_ind: 3, Epoch: 1631, train_loss: 1.4199281445255985, valid_loss: 1.6953955756293404\n",
            "test_ind: 3, Epoch: 1632, train_loss: 1.2857882358409742, valid_loss: 1.3043047233864113\n",
            "test_ind: 3, Epoch: 1633, train_loss: 1.2986485222239554, valid_loss: 1.241857193134449\n",
            "test_ind: 3, Epoch: 1634, train_loss: 1.2503909181665493, valid_loss: 1.2305718351293493\n",
            "test_ind: 3, Epoch: 1635, train_loss: 1.2798875114064159, valid_loss: 1.298775143093533\n",
            "test_ind: 3, Epoch: 1636, train_loss: 1.2724214012240187, valid_loss: 1.3297636597244828\n",
            "test_ind: 3, Epoch: 1637, train_loss: 1.3050715835006148, valid_loss: 1.1442932729367858\n",
            "test_ind: 3, Epoch: 1638, train_loss: 1.2250270490293147, valid_loss: 1.334086435812491\n",
            "test_ind: 3, Epoch: 1639, train_loss: 1.2380866827788177, valid_loss: 1.3655225435892742\n",
            "test_ind: 3, Epoch: 1640, train_loss: 1.1836498166307992, valid_loss: 1.160036316624394\n",
            "test_ind: 3, Epoch: 1641, train_loss: 1.1675767839690787, valid_loss: 1.1126408047146268\n",
            "test_ind: 3, Epoch: 1642, train_loss: 1.1735060421037085, valid_loss: 1.125175193504051\n",
            "test_ind: 3, Epoch: 1643, train_loss: 1.2139641267281993, valid_loss: 1.1545740763346355\n",
            "test_ind: 3, Epoch: 1644, train_loss: 1.2946011166513702, valid_loss: 1.3396145679332592\n",
            "test_ind: 3, Epoch: 1645, train_loss: 1.2468165880368078, valid_loss: 1.2406504065902144\n",
            "test_ind: 3, Epoch: 1646, train_loss: 1.3083592756294908, valid_loss: 1.2786288967838995\n",
            "test_ind: 3, Epoch: 1647, train_loss: 1.3033964369032118, valid_loss: 1.4405613828588415\n",
            "test_ind: 3, Epoch: 1648, train_loss: 1.3505562617455003, valid_loss: 1.3876714353208188\n",
            "test_ind: 3, Epoch: 1649, train_loss: 1.2691115214500899, valid_loss: 1.1752628220452204\n",
            "test_ind: 3, Epoch: 1650, train_loss: 1.2237722726515783, valid_loss: 1.1302936165421098\n",
            "test_ind: 3, Epoch: 1651, train_loss: 1.2305456090856481, valid_loss: 1.2622669361255787\n",
            "test_ind: 3, Epoch: 1652, train_loss: 1.2359545907856504, valid_loss: 1.258143107096354\n",
            "test_ind: 3, Epoch: 1653, train_loss: 1.2090894322336456, valid_loss: 1.1794884646380388\n",
            "test_ind: 3, Epoch: 1654, train_loss: 1.3279622690177255, valid_loss: 1.2527781769081399\n",
            "test_ind: 3, Epoch: 1655, train_loss: 1.2813863165584614, valid_loss: 1.7158118530556008\n",
            "test_ind: 3, Epoch: 1656, train_loss: 1.349413224208502, valid_loss: 1.530658969172725\n",
            "test_ind: 3, Epoch: 1657, train_loss: 1.2861818148766035, valid_loss: 1.2842781102215803\n",
            "test_ind: 3, Epoch: 1658, train_loss: 1.3513818140383123, valid_loss: 1.737916663840965\n",
            "test_ind: 3, Epoch: 1659, train_loss: 1.2547100090686185, valid_loss: 1.3766605059305828\n",
            "test_ind: 3, Epoch: 1660, train_loss: 1.332952752525424, valid_loss: 1.3674602508544922\n",
            "test_ind: 3, Epoch: 1661, train_loss: 1.1974297570593566, valid_loss: 1.4860098273665816\n",
            "test_ind: 3, Epoch: 1662, train_loss: 1.2349238277953347, valid_loss: 1.1610939237806532\n",
            "test_ind: 3, Epoch: 1663, train_loss: 1.2359041755582076, valid_loss: 1.2191810254697446\n",
            "test_ind: 3, Epoch: 1664, train_loss: 1.2618566795631692, valid_loss: 1.591915801719383\n",
            "test_ind: 3, Epoch: 1665, train_loss: 1.2314647156515242, valid_loss: 1.170910022876881\n",
            "test_ind: 3, Epoch: 1666, train_loss: 1.171112566818426, valid_loss: 1.1459904776679144\n",
            "test_ind: 3, Epoch: 1667, train_loss: 1.2955089910530748, valid_loss: 1.2521822364241988\n",
            "test_ind: 3, Epoch: 1668, train_loss: 1.2484091888239355, valid_loss: 1.1232366738496005\n",
            "test_ind: 3, Epoch: 1669, train_loss: 1.2638928154368458, valid_loss: 1.3263526139435946\n",
            "test_ind: 3, Epoch: 1670, train_loss: 1.2367541407361442, valid_loss: 1.1559742291768391\n",
            "test_ind: 3, Epoch: 1671, train_loss: 1.174893308568884, valid_loss: 1.2377993442394115\n",
            "test_ind: 3, Epoch: 1672, train_loss: 1.2128363833015345, valid_loss: 1.2522122418438948\n",
            "test_ind: 3, Epoch: 1673, train_loss: 1.1783368793534643, valid_loss: 1.0872062930354367\n",
            "test_ind: 3, Epoch: 1674, train_loss: 1.2418674716243039, valid_loss: 1.1263925940902146\n",
            "test_ind: 3, Epoch: 1675, train_loss: 1.3582857744193368, valid_loss: 1.2414563850120262\n",
            "test_ind: 3, Epoch: 1676, train_loss: 1.3237148332007138, valid_loss: 1.7689145229480885\n",
            "test_ind: 3, Epoch: 1677, train_loss: 1.219889399446087, valid_loss: 1.4471087102536802\n",
            "test_ind: 3, Epoch: 1678, train_loss: 1.3431820280757951, valid_loss: 1.1470586282235604\n",
            "test_ind: 3, Epoch: 1679, train_loss: 1.3223277668894073, valid_loss: 1.3616845166241682\n",
            "test_ind: 3, Epoch: 1680, train_loss: 1.1818464420459887, valid_loss: 1.2523402991118255\n",
            "test_ind: 3, Epoch: 1681, train_loss: 1.2338742738888588, valid_loss: 1.109018431769477\n",
            "test_ind: 3, Epoch: 1682, train_loss: 1.3092815728835119, valid_loss: 1.2404461083588778\n",
            "test_ind: 3, Epoch: 1683, train_loss: 1.1800509735389992, valid_loss: 1.0912266837226021\n",
            "test_ind: 3, Epoch: 1684, train_loss: 1.2247326580094702, valid_loss: 1.2659080646656178\n",
            "test_ind: 3, Epoch: 1685, train_loss: 1.1789869202507868, valid_loss: 1.1780967359189634\n",
            "test_ind: 3, Epoch: 1686, train_loss: 1.2621694965127073, valid_loss: 1.1861777835422092\n",
            "test_ind: 3, Epoch: 1687, train_loss: 1.2571603103920266, valid_loss: 1.217084602073387\n",
            "test_ind: 3, Epoch: 1688, train_loss: 1.2012630038791232, valid_loss: 1.18523935035423\n",
            "test_ind: 3, Epoch: 1689, train_loss: 1.2020008240217044, valid_loss: 1.3823101962054216\n",
            "test_ind: 3, Epoch: 1690, train_loss: 1.1261324823638539, valid_loss: 1.193462195219817\n",
            "test_ind: 3, Epoch: 1691, train_loss: 1.2567441669511206, valid_loss: 1.1033257378472223\n",
            "test_ind: 3, Epoch: 1692, train_loss: 1.255357907142168, valid_loss: 1.0393311006051522\n",
            "Validation loss decreased (1.0733269762109827 --> 1.0393311006051522).  Saving model ...\n",
            "test_ind: 3, Epoch: 1693, train_loss: 1.228966365625829, valid_loss: 1.3556253291942455\n",
            "test_ind: 3, Epoch: 1694, train_loss: 1.266354454888238, valid_loss: 1.0664818198592574\n",
            "test_ind: 3, Epoch: 1695, train_loss: 1.2457773538283359, valid_loss: 1.1918408075968423\n",
            "test_ind: 3, Epoch: 1696, train_loss: 1.2694772378897965, valid_loss: 1.4275600645277238\n",
            "test_ind: 3, Epoch: 1697, train_loss: 1.2901960007938338, valid_loss: 1.1426416149845828\n",
            "test_ind: 3, Epoch: 1698, train_loss: 1.1741865888053988, valid_loss: 1.2444808218214247\n",
            "test_ind: 3, Epoch: 1699, train_loss: 1.2914932980949498, valid_loss: 1.3973964938411008\n",
            "test_ind: 3, Epoch: 1700, train_loss: 1.3066526636665252, valid_loss: 1.3652794979236744\n",
            "test_ind: 3, Epoch: 1701, train_loss: 1.327582989209964, valid_loss: 1.3149661664609555\n",
            "test_ind: 3, Epoch: 1702, train_loss: 1.3195425080664365, valid_loss: 1.1670460524382416\n",
            "test_ind: 3, Epoch: 1703, train_loss: 1.2388219126948605, valid_loss: 1.2529086890044037\n",
            "test_ind: 3, Epoch: 1704, train_loss: 1.3041997838903356, valid_loss: 1.3789360258314347\n",
            "test_ind: 3, Epoch: 1705, train_loss: 1.1698545114493666, valid_loss: 1.2653305089032207\n",
            "test_ind: 3, Epoch: 1706, train_loss: 1.2256381423385054, valid_loss: 1.3225159645080566\n",
            "test_ind: 3, Epoch: 1707, train_loss: 1.1653227040797105, valid_loss: 1.3072010852672435\n",
            "test_ind: 3, Epoch: 1708, train_loss: 1.1977256668938532, valid_loss: 1.1993975815949618\n",
            "test_ind: 3, Epoch: 1709, train_loss: 1.1940338464430822, valid_loss: 1.130092532546432\n",
            "test_ind: 3, Epoch: 1710, train_loss: 1.2038556675852083, valid_loss: 1.1331129250703036\n",
            "test_ind: 3, Epoch: 1711, train_loss: 1.273343468889778, valid_loss: 1.6822021978872792\n",
            "test_ind: 3, Epoch: 1712, train_loss: 1.2272549499700098, valid_loss: 1.1942567825317383\n",
            "test_ind: 3, Epoch: 1713, train_loss: 1.2666019981290086, valid_loss: 1.1651055018107097\n",
            "test_ind: 3, Epoch: 1714, train_loss: 1.163688865708716, valid_loss: 1.1830524338616266\n",
            "test_ind: 3, Epoch: 1715, train_loss: 1.2287598774756914, valid_loss: 1.133898169906051\n",
            "test_ind: 3, Epoch: 1716, train_loss: 1.1897962299394018, valid_loss: 1.1602829297383626\n",
            "test_ind: 3, Epoch: 1717, train_loss: 1.1556270151962469, valid_loss: 1.2998634444342718\n",
            "test_ind: 3, Epoch: 1718, train_loss: 1.1622949176364474, valid_loss: 1.0471181869506836\n",
            "test_ind: 3, Epoch: 1719, train_loss: 1.200119142179136, valid_loss: 1.2596844213980216\n",
            "test_ind: 3, Epoch: 1720, train_loss: 1.207749914239954, valid_loss: 1.2220755153232152\n",
            "test_ind: 3, Epoch: 1721, train_loss: 1.268307809476499, valid_loss: 1.1741724367494937\n",
            "test_ind: 3, Epoch: 1722, train_loss: 1.2659903632269967, valid_loss: 1.3009774773209184\n",
            "test_ind: 3, Epoch: 1723, train_loss: 1.2385662396748858, valid_loss: 1.2223322126600478\n",
            "test_ind: 3, Epoch: 1724, train_loss: 1.1895603191705397, valid_loss: 1.1437566368668168\n",
            "test_ind: 3, Epoch: 1725, train_loss: 1.2235301159046315, valid_loss: 1.1511815388997395\n",
            "test_ind: 3, Epoch: 1726, train_loss: 1.1507835270446023, valid_loss: 1.47454833984375\n",
            "test_ind: 3, Epoch: 1727, train_loss: 1.261347617632077, valid_loss: 1.3157184742115162\n",
            "test_ind: 3, Epoch: 1728, train_loss: 1.1803334730642812, valid_loss: 1.3348477328265154\n",
            "test_ind: 3, Epoch: 1729, train_loss: 1.1597085882116247, valid_loss: 1.2025793570059315\n",
            "test_ind: 3, Epoch: 1730, train_loss: 1.2247824551146709, valid_loss: 1.1812805776242856\n",
            "test_ind: 3, Epoch: 1731, train_loss: 1.2735629905889063, valid_loss: 1.2436585602936923\n",
            "test_ind: 3, Epoch: 1732, train_loss: 1.2494062729823736, valid_loss: 1.4760843736154063\n",
            "test_ind: 3, Epoch: 1733, train_loss: 1.1875018838011189, valid_loss: 1.0690469035395869\n",
            "test_ind: 3, Epoch: 1734, train_loss: 1.2644832222550002, valid_loss: 1.1318249349240903\n",
            "test_ind: 3, Epoch: 1735, train_loss: 1.3204138426133145, valid_loss: 1.219692548116048\n",
            "test_ind: 3, Epoch: 1736, train_loss: 1.3521406797715176, valid_loss: 1.3995330068800185\n",
            "test_ind: 3, Epoch: 1737, train_loss: 1.1638681093851726, valid_loss: 1.1807633859139903\n",
            "test_ind: 3, Epoch: 1738, train_loss: 1.1968405805988074, valid_loss: 1.2755137902719005\n",
            "test_ind: 3, Epoch: 1739, train_loss: 1.2233152860476646, valid_loss: 1.1621109467965585\n",
            "test_ind: 3, Epoch: 1740, train_loss: 1.158865734382912, valid_loss: 1.272503623255977\n",
            "test_ind: 3, Epoch: 1741, train_loss: 1.147227069477976, valid_loss: 1.210539111384639\n",
            "test_ind: 3, Epoch: 1742, train_loss: 1.1735468087372958, valid_loss: 1.145173267081932\n",
            "test_ind: 3, Epoch: 1743, train_loss: 1.2522339820861816, valid_loss: 1.2153537185103804\n",
            "test_ind: 3, Epoch: 1744, train_loss: 1.1895995316682038, valid_loss: 1.1357339223225913\n",
            "test_ind: 3, Epoch: 1745, train_loss: 1.1847754172336908, valid_loss: 1.0233047803243\n",
            "Validation loss decreased (1.0393311006051522 --> 1.0233047803243).  Saving model ...\n",
            "test_ind: 3, Epoch: 1746, train_loss: 1.210676493468108, valid_loss: 1.321567005581326\n",
            "test_ind: 3, Epoch: 1747, train_loss: 1.2996681531270347, valid_loss: 1.3400590508072463\n",
            "test_ind: 3, Epoch: 1748, train_loss: 1.1681803773950648, valid_loss: 1.065506405300564\n",
            "test_ind: 3, Epoch: 1749, train_loss: 1.2636612315236788, valid_loss: 1.210007702862775\n",
            "test_ind: 3, Epoch: 1750, train_loss: 1.2721074834282016, valid_loss: 1.2483633006060564\n",
            "test_ind: 3, Epoch: 1751, train_loss: 1.3086170208306962, valid_loss: 1.1495294924135562\n",
            "test_ind: 3, Epoch: 1752, train_loss: 1.2562660817746765, valid_loss: 1.2869747126543964\n",
            "test_ind: 3, Epoch: 1753, train_loss: 1.1601560321854958, valid_loss: 1.17804785127993\n",
            "test_ind: 3, Epoch: 1754, train_loss: 1.2348608323085455, valid_loss: 1.0013518686647769\n",
            "Validation loss decreased (1.0233047803243 --> 1.0013518686647769).  Saving model ...\n",
            "test_ind: 3, Epoch: 1755, train_loss: 1.2097954514585894, valid_loss: 1.1898359369348597\n",
            "test_ind: 3, Epoch: 1756, train_loss: 1.2873549637971102, valid_loss: 1.0143458401715315\n",
            "test_ind: 3, Epoch: 1757, train_loss: 1.2458759943644206, valid_loss: 1.0923672428837528\n",
            "test_ind: 3, Epoch: 1758, train_loss: 1.1954886707258814, valid_loss: 1.0214290972109192\n",
            "test_ind: 3, Epoch: 1759, train_loss: 1.2066115744319963, valid_loss: 1.3506391843159995\n",
            "test_ind: 3, Epoch: 1760, train_loss: 1.1875735800943257, valid_loss: 1.3505245491310403\n",
            "test_ind: 3, Epoch: 1761, train_loss: 1.1553016356479977, valid_loss: 1.131383189448604\n",
            "test_ind: 3, Epoch: 1762, train_loss: 1.2072826197117934, valid_loss: 1.094039069281684\n",
            "test_ind: 3, Epoch: 1763, train_loss: 1.152447188342059, valid_loss: 1.214317003885905\n",
            "test_ind: 3, Epoch: 1764, train_loss: 1.203614988444764, valid_loss: 1.0853348308139377\n",
            "test_ind: 3, Epoch: 1765, train_loss: 1.186832563376721, valid_loss: 1.203276175039786\n",
            "test_ind: 3, Epoch: 1766, train_loss: 1.1781332051312483, valid_loss: 1.2466471636736833\n",
            "test_ind: 3, Epoch: 1767, train_loss: 1.309886508517795, valid_loss: 1.3918000150609897\n",
            "test_ind: 3, Epoch: 1768, train_loss: 1.194821075156883, valid_loss: 1.2409872302302607\n",
            "test_ind: 3, Epoch: 1769, train_loss: 1.1718460424446766, valid_loss: 1.058950353551794\n",
            "test_ind: 3, Epoch: 1770, train_loss: 1.2636242383792076, valid_loss: 1.3151555767765752\n",
            "test_ind: 3, Epoch: 1771, train_loss: 1.1987608038348914, valid_loss: 1.1169103693079065\n",
            "test_ind: 3, Epoch: 1772, train_loss: 1.3241251839531794, valid_loss: 1.2151118914286296\n",
            "test_ind: 3, Epoch: 1773, train_loss: 1.216212984956341, valid_loss: 1.2268509688200775\n",
            "test_ind: 3, Epoch: 1774, train_loss: 1.194281460326395, valid_loss: 1.1022752479270652\n",
            "test_ind: 3, Epoch: 1775, train_loss: 1.1910684844593944, valid_loss: 1.4453486689814814\n",
            "test_ind: 3, Epoch: 1776, train_loss: 1.1221918176721641, valid_loss: 1.0740775532192655\n",
            "test_ind: 3, Epoch: 1777, train_loss: 1.1809152732660742, valid_loss: 1.257459216647678\n",
            "test_ind: 3, Epoch: 1778, train_loss: 1.1664542975249113, valid_loss: 1.10193286118684\n",
            "test_ind: 3, Epoch: 1779, train_loss: 1.1996205942130385, valid_loss: 1.3101793924967446\n",
            "test_ind: 3, Epoch: 1780, train_loss: 1.1574788505648388, valid_loss: 1.168650503511782\n",
            "test_ind: 3, Epoch: 1781, train_loss: 1.1256139190108685, valid_loss: 1.148770632567229\n",
            "test_ind: 3, Epoch: 1782, train_loss: 1.1994773429117085, valid_loss: 1.2942022570857294\n",
            "test_ind: 3, Epoch: 1783, train_loss: 1.1409230055632416, valid_loss: 1.146461009979248\n",
            "test_ind: 3, Epoch: 1784, train_loss: 1.2116511545063537, valid_loss: 1.1160202026367188\n",
            "test_ind: 3, Epoch: 1785, train_loss: 1.1411419503482771, valid_loss: 1.1705336570739746\n",
            "test_ind: 3, Epoch: 1786, train_loss: 1.188486999935574, valid_loss: 1.0806962649027507\n",
            "test_ind: 3, Epoch: 1787, train_loss: 1.2614364270810727, valid_loss: 1.1408636305067275\n",
            "test_ind: 3, Epoch: 1788, train_loss: 1.2747527640542864, valid_loss: 1.1119615413524486\n",
            "test_ind: 3, Epoch: 1789, train_loss: 1.1819803096629955, valid_loss: 1.2024055233708135\n",
            "test_ind: 3, Epoch: 1790, train_loss: 1.188144513118414, valid_loss: 1.0966294076707628\n",
            "test_ind: 3, Epoch: 1791, train_loss: 1.1698677157178334, valid_loss: 1.3872299194335938\n",
            "test_ind: 3, Epoch: 1792, train_loss: 1.1873823566201294, valid_loss: 1.0791913668314617\n",
            "test_ind: 3, Epoch: 1793, train_loss: 1.1543061350598747, valid_loss: 1.134938257711905\n",
            "test_ind: 3, Epoch: 1794, train_loss: 1.3359598936858, valid_loss: 1.4134746480871132\n",
            "test_ind: 3, Epoch: 1795, train_loss: 1.23863313227524, valid_loss: 1.119232866499159\n",
            "test_ind: 3, Epoch: 1796, train_loss: 1.2213204584003967, valid_loss: 1.7448841024328163\n",
            "test_ind: 3, Epoch: 1797, train_loss: 1.1339197099944693, valid_loss: 1.1598190908078794\n",
            "test_ind: 3, Epoch: 1798, train_loss: 1.175921834545371, valid_loss: 1.2709753954852068\n",
            "test_ind: 3, Epoch: 1799, train_loss: 1.2389468852384593, valid_loss: 1.3088920381334093\n",
            "test_ind: 3, Epoch: 1800, train_loss: 1.3088208127904821, valid_loss: 1.5907805937307853\n",
            "test_ind: 3, Epoch: 1801, train_loss: 1.1661060062455542, valid_loss: 1.1496176013240107\n",
            "test_ind: 3, Epoch: 1802, train_loss: 1.2105779647827148, valid_loss: 1.0530583063761394\n",
            "test_ind: 3, Epoch: 1803, train_loss: 1.1215749199007763, valid_loss: 0.9863372202272769\n",
            "Validation loss decreased (1.0013518686647769 --> 0.9863372202272769).  Saving model ...\n",
            "test_ind: 3, Epoch: 1804, train_loss: 1.209745701448417, valid_loss: 1.110550880432129\n",
            "test_ind: 3, Epoch: 1805, train_loss: 1.2385564203615544, valid_loss: 1.162215992256447\n",
            "test_ind: 3, Epoch: 1806, train_loss: 1.2207497255301771, valid_loss: 1.0155854754977756\n",
            "test_ind: 3, Epoch: 1807, train_loss: 1.2659812974341123, valid_loss: 1.3433358051158764\n",
            "test_ind: 3, Epoch: 1808, train_loss: 1.1790326554098247, valid_loss: 1.3515661204302754\n",
            "test_ind: 3, Epoch: 1809, train_loss: 1.2512231756139687, valid_loss: 1.219653641736066\n",
            "test_ind: 3, Epoch: 1810, train_loss: 1.114999211864707, valid_loss: 1.2343863734492548\n",
            "test_ind: 3, Epoch: 1811, train_loss: 1.2316220248187029, valid_loss: 1.140015478487368\n",
            "test_ind: 3, Epoch: 1812, train_loss: 1.213987350463867, valid_loss: 1.4936149208633989\n",
            "test_ind: 3, Epoch: 1813, train_loss: 1.1913895194913136, valid_loss: 1.3146263934947826\n",
            "test_ind: 3, Epoch: 1814, train_loss: 1.1601073418134524, valid_loss: 0.9933929443359375\n",
            "test_ind: 3, Epoch: 1815, train_loss: 1.1645602885587716, valid_loss: 1.1700297814828378\n",
            "test_ind: 3, Epoch: 1816, train_loss: 1.2505933208230102, valid_loss: 1.2483332422044542\n",
            "test_ind: 3, Epoch: 1817, train_loss: 1.1911076439751518, valid_loss: 1.2016274664137099\n",
            "test_ind: 3, Epoch: 1818, train_loss: 1.2197110917833116, valid_loss: 1.0792738243385598\n",
            "test_ind: 3, Epoch: 1819, train_loss: 1.2699175822882005, valid_loss: 1.1952865212051957\n",
            "test_ind: 3, Epoch: 1820, train_loss: 1.1359771563683028, valid_loss: 1.0474955417491771\n",
            "test_ind: 3, Epoch: 1821, train_loss: 1.1751939396799347, valid_loss: 1.3999692422372323\n",
            "test_ind: 3, Epoch: 1822, train_loss: 1.2181119977692025, valid_loss: 1.082176155514187\n",
            "test_ind: 3, Epoch: 1823, train_loss: 1.1478848339598855, valid_loss: 1.3700704751191317\n",
            "test_ind: 3, Epoch: 1824, train_loss: 1.2569060855441625, valid_loss: 1.1910470326741536\n",
            "test_ind: 3, Epoch: 1825, train_loss: 1.1163236829969616, valid_loss: 1.2018461757236056\n",
            "test_ind: 3, Epoch: 1826, train_loss: 1.2862282034791546, valid_loss: 1.362976144861292\n",
            "test_ind: 3, Epoch: 1827, train_loss: 1.2589841359927332, valid_loss: 1.7753479922259294\n",
            "test_ind: 3, Epoch: 1828, train_loss: 1.1972324936478227, valid_loss: 1.1832899340876826\n",
            "test_ind: 3, Epoch: 1829, train_loss: 1.2349249404153706, valid_loss: 1.2619431107132522\n",
            "test_ind: 3, Epoch: 1830, train_loss: 1.1653562239658686, valid_loss: 1.207403377250389\n",
            "test_ind: 3, Epoch: 1831, train_loss: 1.2881029917870037, valid_loss: 1.5117946377506963\n",
            "test_ind: 3, Epoch: 1832, train_loss: 1.135046923602069, valid_loss: 1.0400116531937211\n",
            "test_ind: 3, Epoch: 1833, train_loss: 1.2249115779076092, valid_loss: 1.1205481953091092\n",
            "test_ind: 3, Epoch: 1834, train_loss: 1.2251865834365656, valid_loss: 1.2208011238663286\n",
            "test_ind: 3, Epoch: 1835, train_loss: 1.1337118325410067, valid_loss: 1.0284841855367026\n",
            "test_ind: 3, Epoch: 1836, train_loss: 1.1729499499003093, valid_loss: 1.1896890181082267\n",
            "test_ind: 3, Epoch: 1837, train_loss: 1.1879561801015595, valid_loss: 1.1191665154916268\n",
            "test_ind: 3, Epoch: 1838, train_loss: 1.2385416442965285, valid_loss: 1.2675116327073839\n",
            "test_ind: 3, Epoch: 1839, train_loss: 1.1739189359876845, valid_loss: 1.0722339064986617\n",
            "test_ind: 3, Epoch: 1840, train_loss: 1.1398979999400949, valid_loss: 1.1478779934070729\n",
            "test_ind: 3, Epoch: 1841, train_loss: 1.0950325859917533, valid_loss: 1.207493128599944\n",
            "test_ind: 3, Epoch: 1842, train_loss: 1.0829139992042822, valid_loss: 1.0578963491651747\n",
            "test_ind: 3, Epoch: 1843, train_loss: 1.225281521126076, valid_loss: 1.5409106501826533\n",
            "test_ind: 3, Epoch: 1844, train_loss: 1.2113859506300937, valid_loss: 1.0977754592895508\n",
            "test_ind: 3, Epoch: 1845, train_loss: 1.232655354488043, valid_loss: 1.214230113559299\n",
            "test_ind: 3, Epoch: 1846, train_loss: 1.1519577591507522, valid_loss: 1.1815400653415256\n",
            "test_ind: 3, Epoch: 1847, train_loss: 1.252858073623092, valid_loss: 1.133786642992938\n",
            "test_ind: 3, Epoch: 1848, train_loss: 1.164122763975167, valid_loss: 1.2067385956093115\n",
            "test_ind: 3, Epoch: 1849, train_loss: 1.1911907372651276, valid_loss: 1.4635275204976401\n",
            "test_ind: 3, Epoch: 1850, train_loss: 1.2437712057137196, valid_loss: 1.1479584905836318\n",
            "test_ind: 3, Epoch: 1851, train_loss: 1.336482501324312, valid_loss: 1.57808256149292\n",
            "test_ind: 3, Epoch: 1852, train_loss: 1.195130542472557, valid_loss: 1.0233582920498319\n",
            "test_ind: 3, Epoch: 1853, train_loss: 1.111223856608073, valid_loss: 1.092966326960811\n",
            "test_ind: 3, Epoch: 1854, train_loss: 1.103919529620512, valid_loss: 1.010290711014359\n",
            "test_ind: 3, Epoch: 1855, train_loss: 1.2093948905850633, valid_loss: 1.210703037403248\n",
            "test_ind: 3, Epoch: 1856, train_loss: 1.1934724795965501, valid_loss: 1.0784040380407263\n",
            "test_ind: 3, Epoch: 1857, train_loss: 1.2216919263203938, valid_loss: 1.0715030564202201\n",
            "test_ind: 3, Epoch: 1858, train_loss: 1.1467363275127647, valid_loss: 1.2124927662037037\n",
            "test_ind: 3, Epoch: 1859, train_loss: 1.2005247952025613, valid_loss: 1.453647013063784\n",
            "test_ind: 3, Epoch: 1860, train_loss: 1.1390594670801986, valid_loss: 1.0668999000831887\n",
            "test_ind: 3, Epoch: 1861, train_loss: 1.1606169747717585, valid_loss: 1.2039459546407065\n",
            "test_ind: 3, Epoch: 1862, train_loss: 1.1872240937786338, valid_loss: 1.2622088326348198\n",
            "test_ind: 3, Epoch: 1863, train_loss: 1.1527843298735443, valid_loss: 1.1731369053875957\n",
            "test_ind: 3, Epoch: 1864, train_loss: 1.1131053853918005, valid_loss: 1.21698088116116\n",
            "test_ind: 3, Epoch: 1865, train_loss: 1.103870650868357, valid_loss: 1.093304951985677\n",
            "test_ind: 3, Epoch: 1866, train_loss: 1.1478355902212636, valid_loss: 1.0607571955080384\n",
            "test_ind: 3, Epoch: 1867, train_loss: 1.1200332994814275, valid_loss: 1.2052995540477611\n",
            "test_ind: 3, Epoch: 1868, train_loss: 1.0915255428832253, valid_loss: 1.1252925484268754\n",
            "test_ind: 3, Epoch: 1869, train_loss: 1.1093176853509596, valid_loss: 1.1528451884234392\n",
            "test_ind: 3, Epoch: 1870, train_loss: 1.1748687073036477, valid_loss: 1.1447822959334761\n",
            "test_ind: 3, Epoch: 1871, train_loss: 1.1193328786779333, valid_loss: 1.0296533196060746\n",
            "test_ind: 3, Epoch: 1872, train_loss: 1.1145533514611514, valid_loss: 1.2504576400474265\n",
            "test_ind: 3, Epoch: 1873, train_loss: 1.1657808798330802, valid_loss: 1.2680262459648979\n",
            "test_ind: 3, Epoch: 1874, train_loss: 1.1782704812509042, valid_loss: 1.0543036107663755\n",
            "test_ind: 3, Epoch: 1875, train_loss: 1.108142010959578, valid_loss: 1.0918062351368092\n",
            "test_ind: 3, Epoch: 1876, train_loss: 1.145445517551752, valid_loss: 1.3063196076287162\n",
            "test_ind: 3, Epoch: 1877, train_loss: 1.1119934894420485, valid_loss: 1.0505862412629303\n",
            "test_ind: 3, Epoch: 1878, train_loss: 1.0520379455001267, valid_loss: 1.0991538012469255\n",
            "test_ind: 3, Epoch: 1879, train_loss: 1.1619780564013824, valid_loss: 2.0451516222070762\n",
            "test_ind: 3, Epoch: 1880, train_loss: 1.102923193095643, valid_loss: 1.2175078921847875\n",
            "test_ind: 3, Epoch: 1881, train_loss: 1.2048734029134114, valid_loss: 1.2466356842606154\n",
            "test_ind: 3, Epoch: 1882, train_loss: 1.1992333730061848, valid_loss: 1.156415550797074\n",
            "test_ind: 3, Epoch: 1883, train_loss: 1.1741609396757902, valid_loss: 1.201175089235659\n",
            "test_ind: 3, Epoch: 1884, train_loss: 1.179537949738679, valid_loss: 1.2214043052108199\n",
            "test_ind: 3, Epoch: 1885, train_loss: 1.0827889265837494, valid_loss: 1.2223252720303004\n",
            "test_ind: 3, Epoch: 1886, train_loss: 1.1053707040386436, valid_loss: 1.0724283500953957\n",
            "test_ind: 3, Epoch: 1887, train_loss: 1.1930943595038521, valid_loss: 1.0506305341367368\n",
            "test_ind: 3, Epoch: 1888, train_loss: 1.0933793621298709, valid_loss: 1.5125721119068287\n",
            "test_ind: 3, Epoch: 1889, train_loss: 1.1940010094348297, valid_loss: 1.1721059128090188\n",
            "test_ind: 3, Epoch: 1890, train_loss: 1.1550067089222096, valid_loss: 1.1623779932657878\n",
            "test_ind: 3, Epoch: 1891, train_loss: 1.1214266470920893, valid_loss: 1.247872758794714\n",
            "test_ind: 3, Epoch: 1892, train_loss: 1.1636687796792866, valid_loss: 1.1450861648277\n",
            "test_ind: 3, Epoch: 1893, train_loss: 1.12764342037248, valid_loss: 1.0395474433898926\n",
            "test_ind: 3, Epoch: 1894, train_loss: 1.132360358297089, valid_loss: 1.2531307538350422\n",
            "test_ind: 3, Epoch: 1895, train_loss: 1.128210621115602, valid_loss: 1.0491888258192275\n",
            "test_ind: 3, Epoch: 1896, train_loss: 1.1565796534220376, valid_loss: 1.2632778662222404\n",
            "test_ind: 3, Epoch: 1897, train_loss: 1.161486660992658, valid_loss: 1.36653940765946\n",
            "test_ind: 3, Epoch: 1898, train_loss: 1.2026212127120406, valid_loss: 1.2907414259733978\n",
            "test_ind: 3, Epoch: 1899, train_loss: 1.1712221628353918, valid_loss: 1.0723774168226454\n",
            "test_ind: 3, Epoch: 1900, train_loss: 1.1245025234457888, valid_loss: 0.9902502519113046\n",
            "test_ind: 3, Epoch: 1901, train_loss: 1.1538942772665142, valid_loss: 1.196033795674642\n",
            "test_ind: 3, Epoch: 1902, train_loss: 1.2147111245143558, valid_loss: 1.080445201308639\n",
            "test_ind: 3, Epoch: 1903, train_loss: 1.1394017066484616, valid_loss: 1.1457515292697482\n",
            "test_ind: 3, Epoch: 1904, train_loss: 1.2449612852967817, valid_loss: 1.1137524357548467\n",
            "test_ind: 3, Epoch: 1905, train_loss: 1.1603366945996696, valid_loss: 1.247854091503002\n",
            "test_ind: 3, Epoch: 1906, train_loss: 1.1922463723170902, valid_loss: 1.2543024663571958\n",
            "test_ind: 3, Epoch: 1907, train_loss: 1.093403751467481, valid_loss: 1.0232729028772425\n",
            "test_ind: 3, Epoch: 1908, train_loss: 1.123973004611922, valid_loss: 1.0404277201052066\n",
            "test_ind: 3, Epoch: 1909, train_loss: 1.1119942488493741, valid_loss: 1.1036018795437283\n",
            "test_ind: 3, Epoch: 1910, train_loss: 1.210630864272883, valid_loss: 1.3172714798538774\n",
            "test_ind: 3, Epoch: 1911, train_loss: 1.0725462878191911, valid_loss: 1.3001165919833713\n",
            "test_ind: 3, Epoch: 1912, train_loss: 1.1702401196515118, valid_loss: 1.0857557897214536\n",
            "test_ind: 3, Epoch: 1913, train_loss: 1.168321462325108, valid_loss: 1.237581041124132\n",
            "test_ind: 3, Epoch: 1914, train_loss: 1.1350687286000192, valid_loss: 1.2595669075294775\n",
            "test_ind: 3, Epoch: 1915, train_loss: 1.2371991593160747, valid_loss: 1.3695825294212058\n",
            "test_ind: 3, Epoch: 1916, train_loss: 1.1469065760388788, valid_loss: 1.068609043403908\n",
            "test_ind: 3, Epoch: 1917, train_loss: 1.1172928044825423, valid_loss: 1.225135538313124\n",
            "test_ind: 3, Epoch: 1918, train_loss: 1.0730139355600616, valid_loss: 1.0741943959836606\n",
            "test_ind: 3, Epoch: 1919, train_loss: 1.0781666025703336, valid_loss: 1.0759118221424244\n",
            "test_ind: 3, Epoch: 1920, train_loss: 1.143096246837098, valid_loss: 1.0558002083389848\n",
            "test_ind: 3, Epoch: 1921, train_loss: 1.1066265400545097, valid_loss: 1.0250866148206923\n",
            "test_ind: 3, Epoch: 1922, train_loss: 1.1203271253609361, valid_loss: 0.9768135459334761\n",
            "Validation loss decreased (0.9863372202272769 --> 0.9768135459334761).  Saving model ...\n",
            "test_ind: 3, Epoch: 1923, train_loss: 1.1204151341944566, valid_loss: 1.0014229174013491\n",
            "test_ind: 3, Epoch: 1924, train_loss: 1.1602372593349881, valid_loss: 1.1258420061182093\n",
            "test_ind: 3, Epoch: 1925, train_loss: 1.0841660617310322, valid_loss: 1.0372622807820637\n",
            "test_ind: 3, Epoch: 1926, train_loss: 1.1316073971030152, valid_loss: 1.0721156508834273\n",
            "test_ind: 3, Epoch: 1927, train_loss: 1.0688401563667957, valid_loss: 1.229634408597593\n",
            "test_ind: 3, Epoch: 1928, train_loss: 1.0728167251304346, valid_loss: 1.1811357074313693\n",
            "test_ind: 3, Epoch: 1929, train_loss: 1.2118954364164376, valid_loss: 1.0073389300593623\n",
            "test_ind: 3, Epoch: 1930, train_loss: 1.1009320270867995, valid_loss: 1.0472910315902144\n",
            "test_ind: 3, Epoch: 1931, train_loss: 1.1954907782283832, valid_loss: 1.1131720719514069\n",
            "test_ind: 3, Epoch: 1932, train_loss: 1.143366030704828, valid_loss: 1.1780058366281017\n",
            "test_ind: 3, Epoch: 1933, train_loss: 1.172827090746091, valid_loss: 1.1788237889607747\n",
            "test_ind: 3, Epoch: 1934, train_loss: 1.1129131317138672, valid_loss: 1.3003348950986509\n",
            "test_ind: 3, Epoch: 1935, train_loss: 1.1105870317529751, valid_loss: 1.116945955488417\n",
            "test_ind: 3, Epoch: 1936, train_loss: 1.0658001899719238, valid_loss: 1.020264819816307\n",
            "test_ind: 3, Epoch: 1937, train_loss: 1.1809890535142686, valid_loss: 1.601317705931487\n",
            "test_ind: 3, Epoch: 1938, train_loss: 1.0869942358982414, valid_loss: 1.0849614496584292\n",
            "test_ind: 3, Epoch: 1939, train_loss: 1.1451438150288147, valid_loss: 1.286715684113679\n",
            "test_ind: 3, Epoch: 1940, train_loss: 1.1091720439769603, valid_loss: 1.0390238761901855\n",
            "test_ind: 3, Epoch: 1941, train_loss: 1.1236424563843526, valid_loss: 1.08533094547413\n",
            "test_ind: 3, Epoch: 1942, train_loss: 1.1155019336276586, valid_loss: 1.0215851642467357\n",
            "test_ind: 3, Epoch: 1943, train_loss: 1.0540814929538302, valid_loss: 1.012440186959726\n",
            "test_ind: 3, Epoch: 1944, train_loss: 1.1336582737204468, valid_loss: 1.1333715827376754\n",
            "test_ind: 3, Epoch: 1945, train_loss: 1.039789582476204, valid_loss: 1.1395255724589028\n",
            "test_ind: 3, Epoch: 1946, train_loss: 1.1563563052518868, valid_loss: 1.1409089476973922\n",
            "test_ind: 3, Epoch: 1947, train_loss: 1.1103506794682254, valid_loss: 1.0908519250375253\n",
            "test_ind: 3, Epoch: 1948, train_loss: 1.09484170395651, valid_loss: 1.2358025091665763\n",
            "test_ind: 3, Epoch: 1949, train_loss: 1.1105124214549122, valid_loss: 1.0520385636223688\n",
            "test_ind: 3, Epoch: 1950, train_loss: 1.2040957756984383, valid_loss: 1.4049985673692493\n",
            "test_ind: 3, Epoch: 1951, train_loss: 1.1569842997892401, valid_loss: 1.0020774028919361\n",
            "test_ind: 3, Epoch: 1952, train_loss: 1.0672262686270253, valid_loss: 1.0523095660739474\n",
            "test_ind: 3, Epoch: 1953, train_loss: 1.0908400394298412, valid_loss: 1.0508458879258897\n",
            "test_ind: 3, Epoch: 1954, train_loss: 1.0898661142514077, valid_loss: 1.1242880468015317\n",
            "test_ind: 3, Epoch: 1955, train_loss: 1.12715427963822, valid_loss: 1.0702628559536405\n",
            "test_ind: 3, Epoch: 1956, train_loss: 1.1343125531702867, valid_loss: 0.9904273704246238\n",
            "test_ind: 3, Epoch: 1957, train_loss: 1.117933220333523, valid_loss: 0.9558838738335504\n",
            "Validation loss decreased (0.9768135459334761 --> 0.9558838738335504).  Saving model ...\n",
            "test_ind: 3, Epoch: 1958, train_loss: 1.112048519982232, valid_loss: 1.4200973157529477\n",
            "test_ind: 3, Epoch: 1959, train_loss: 1.1394047913727936, valid_loss: 1.1408341549060963\n",
            "test_ind: 3, Epoch: 1960, train_loss: 1.1959898913348161, valid_loss: 1.1803758056075484\n",
            "test_ind: 3, Epoch: 1961, train_loss: 1.151766035291884, valid_loss: 1.2187897187692147\n",
            "test_ind: 3, Epoch: 1962, train_loss: 1.0783730259648077, valid_loss: 1.0864975893938984\n",
            "test_ind: 3, Epoch: 1963, train_loss: 1.1383634908699696, valid_loss: 1.0516922915423357\n",
            "test_ind: 3, Epoch: 1964, train_loss: 1.015935797750214, valid_loss: 1.1801945191842538\n",
            "test_ind: 3, Epoch: 1965, train_loss: 1.1209647743790239, valid_loss: 1.0574096220510978\n",
            "test_ind: 3, Epoch: 1966, train_loss: 1.1113925863195349, valid_loss: 1.0979261221709073\n",
            "test_ind: 3, Epoch: 1967, train_loss: 1.1740434258072465, valid_loss: 1.0156340952272769\n",
            "test_ind: 3, Epoch: 1968, train_loss: 1.1252594818303616, valid_loss: 1.0102726441842538\n",
            "test_ind: 3, Epoch: 1969, train_loss: 1.066654005168397, valid_loss: 0.9918565220303006\n",
            "test_ind: 3, Epoch: 1970, train_loss: 1.1536208317603593, valid_loss: 1.1740046783729836\n",
            "test_ind: 3, Epoch: 1971, train_loss: 1.0678367732483665, valid_loss: 0.949583442122848\n",
            "Validation loss decreased (0.9558838738335504 --> 0.949583442122848).  Saving model ...\n",
            "test_ind: 3, Epoch: 1972, train_loss: 1.1258488172366294, valid_loss: 1.4585202711599843\n",
            "test_ind: 3, Epoch: 1973, train_loss: 1.082331333631351, valid_loss: 1.050537798139784\n",
            "test_ind: 3, Epoch: 1974, train_loss: 1.036384176324915, valid_loss: 1.0562656367266618\n",
            "test_ind: 3, Epoch: 1975, train_loss: 1.0746173740905007, valid_loss: 1.1935583220587835\n",
            "test_ind: 3, Epoch: 1976, train_loss: 1.0598942203286252, valid_loss: 0.9701717164781358\n",
            "test_ind: 3, Epoch: 1977, train_loss: 1.0671906235777302, valid_loss: 1.0428423528318052\n",
            "test_ind: 3, Epoch: 1978, train_loss: 1.1631859143575032, valid_loss: 1.1751574233726219\n",
            "test_ind: 3, Epoch: 1979, train_loss: 1.132085499940095, valid_loss: 1.148745819374367\n",
            "test_ind: 3, Epoch: 1980, train_loss: 1.0225976367055636, valid_loss: 1.033910415790699\n",
            "test_ind: 3, Epoch: 1981, train_loss: 1.1232191839335877, valid_loss: 1.040687296125624\n",
            "test_ind: 3, Epoch: 1982, train_loss: 1.1072216504885828, valid_loss: 0.9972660453231246\n",
            "test_ind: 3, Epoch: 1983, train_loss: 1.118028458253837, valid_loss: 1.3111901813083224\n",
            "test_ind: 3, Epoch: 1984, train_loss: 1.1381102609045712, valid_loss: 1.0853531978748463\n",
            "test_ind: 3, Epoch: 1985, train_loss: 1.1743829279770086, valid_loss: 1.1811809186582212\n",
            "test_ind: 3, Epoch: 1986, train_loss: 1.1821607012807587, valid_loss: 1.1086484767772533\n",
            "test_ind: 3, Epoch: 1987, train_loss: 1.1229399751733853, valid_loss: 1.4236325511225947\n",
            "test_ind: 3, Epoch: 1988, train_loss: 1.15504452622967, valid_loss: 1.2963209328828034\n",
            "test_ind: 3, Epoch: 1989, train_loss: 1.121035016613242, valid_loss: 1.0552038086785211\n",
            "test_ind: 3, Epoch: 1990, train_loss: 1.2937264560181418, valid_loss: 1.135622713300917\n",
            "test_ind: 3, Epoch: 1991, train_loss: 1.0596617887049546, valid_loss: 1.0628425280253093\n",
            "test_ind: 3, Epoch: 1992, train_loss: 1.071225384135305, valid_loss: 1.3075281602365\n",
            "test_ind: 3, Epoch: 1993, train_loss: 1.1555095484227311, valid_loss: 1.3450718809057167\n",
            "test_ind: 3, Epoch: 1994, train_loss: 1.1088953312532404, valid_loss: 1.014229862778275\n",
            "test_ind: 3, Epoch: 1995, train_loss: 1.0773107387401442, valid_loss: 1.0963347752888997\n",
            "test_ind: 3, Epoch: 1996, train_loss: 1.110368310669322, valid_loss: 1.1517491340637207\n",
            "test_ind: 3, Epoch: 1997, train_loss: 1.0954077979664745, valid_loss: 1.0184446087589971\n",
            "test_ind: 3, Epoch: 1998, train_loss: 1.0858415438805098, valid_loss: 1.0420205858018665\n",
            "test_ind: 3, Epoch: 1999, train_loss: 1.0528316203458807, valid_loss: 1.1128543041370533\n",
            "test_ind: 3, Epoch: 2000, train_loss: 1.1088273495803644, valid_loss: 1.0996675314726654\n",
            "test_ind: 3, Epoch: 2001, train_loss: 1.0648388038446872, valid_loss: 1.2545493620413322\n",
            "test_ind: 3, Epoch: 2002, train_loss: 1.1030260604104878, valid_loss: 1.2111615428218134\n",
            "test_ind: 3, Epoch: 2003, train_loss: 1.1951734872511877, valid_loss: 1.1621512483667444\n",
            "test_ind: 3, Epoch: 2004, train_loss: 1.111613397245054, valid_loss: 1.1649015921133536\n",
            "test_ind: 3, Epoch: 2005, train_loss: 1.0543708624663175, valid_loss: 1.0995265466195567\n",
            "test_ind: 3, Epoch: 2006, train_loss: 1.105959774535379, valid_loss: 1.1478338241577148\n",
            "test_ind: 3, Epoch: 2007, train_loss: 1.0442782978952665, valid_loss: 1.1979258855183919\n",
            "test_ind: 3, Epoch: 2008, train_loss: 1.1766720465671867, valid_loss: 1.1988680804217304\n",
            "test_ind: 3, Epoch: 2009, train_loss: 1.1093809928423093, valid_loss: 1.4291667585019714\n",
            "test_ind: 3, Epoch: 2010, train_loss: 1.1360295907950697, valid_loss: 1.0232100663361727\n",
            "test_ind: 3, Epoch: 2011, train_loss: 1.2507368900157785, valid_loss: 1.036118895919235\n",
            "test_ind: 3, Epoch: 2012, train_loss: 1.177716690816997, valid_loss: 1.2287620615076136\n",
            "test_ind: 3, Epoch: 2013, train_loss: 1.109624480023796, valid_loss: 1.4352966237951208\n",
            "test_ind: 3, Epoch: 2014, train_loss: 1.131566724659484, valid_loss: 0.9556780567875616\n",
            "test_ind: 3, Epoch: 2015, train_loss: 1.2010065832255798, valid_loss: 1.079499774509006\n",
            "test_ind: 3, Epoch: 2016, train_loss: 1.1537652780980239, valid_loss: 1.0679628230907299\n",
            "test_ind: 3, Epoch: 2017, train_loss: 1.0533745259414484, valid_loss: 1.2931053903367786\n",
            "test_ind: 3, Epoch: 2018, train_loss: 1.0501727704648618, valid_loss: 1.0115065044826932\n",
            "test_ind: 3, Epoch: 2019, train_loss: 1.0832216710220148, valid_loss: 1.1143439964011863\n",
            "test_ind: 3, Epoch: 2020, train_loss: 1.1901446330694503, valid_loss: 1.1519034173753526\n",
            "test_ind: 3, Epoch: 2021, train_loss: 1.0824364320731457, valid_loss: 1.0398334397210016\n",
            "test_ind: 3, Epoch: 2022, train_loss: 1.0506738968837408, valid_loss: 1.089226969966182\n",
            "test_ind: 3, Epoch: 2023, train_loss: 1.023158797511348, valid_loss: 0.960705527552852\n",
            "test_ind: 3, Epoch: 2024, train_loss: 1.1827243404623904, valid_loss: 1.4260339913544833\n",
            "test_ind: 3, Epoch: 2025, train_loss: 1.1189320941030243, valid_loss: 1.0446324701662417\n",
            "test_ind: 3, Epoch: 2026, train_loss: 1.0171683923697765, valid_loss: 1.1199455791049533\n",
            "test_ind: 3, Epoch: 2027, train_loss: 1.1582860828917703, valid_loss: 1.13099029329088\n",
            "test_ind: 3, Epoch: 2028, train_loss: 1.1232394406824937, valid_loss: 0.9816566749855324\n",
            "test_ind: 3, Epoch: 2029, train_loss: 1.08762060565713, valid_loss: 1.3634502976029008\n",
            "test_ind: 3, Epoch: 2030, train_loss: 1.1143538981308172, valid_loss: 1.0400794876946344\n",
            "test_ind: 3, Epoch: 2031, train_loss: 1.0582087481463394, valid_loss: 1.0248339970906577\n",
            "test_ind: 3, Epoch: 2032, train_loss: 1.2560111092932429, valid_loss: 1.2536769266481753\n",
            "test_ind: 3, Epoch: 2033, train_loss: 1.1246643243012606, valid_loss: 1.06967325563784\n",
            "test_ind: 3, Epoch: 2034, train_loss: 1.1212976067154496, valid_loss: 1.0382092263963487\n",
            "test_ind: 3, Epoch: 2035, train_loss: 1.0948694782492556, valid_loss: 1.0409937964545355\n",
            "test_ind: 3, Epoch: 2036, train_loss: 1.1508904150974604, valid_loss: 1.0792195532057018\n",
            "test_ind: 3, Epoch: 2037, train_loss: 1.1479997576018912, valid_loss: 0.939625104268392\n",
            "Validation loss decreased (0.949583442122848 --> 0.939625104268392).  Saving model ...\n",
            "test_ind: 3, Epoch: 2038, train_loss: 1.087829530974965, valid_loss: 1.287678294711643\n",
            "test_ind: 3, Epoch: 2039, train_loss: 1.1096833841300304, valid_loss: 1.1623014167503074\n",
            "test_ind: 3, Epoch: 2040, train_loss: 1.1452934477064345, valid_loss: 1.3503438631693523\n",
            "test_ind: 3, Epoch: 2041, train_loss: 1.0619776631578985, valid_loss: 1.1551407178243003\n",
            "test_ind: 3, Epoch: 2042, train_loss: 1.1385908185699842, valid_loss: 1.3997026019626193\n",
            "test_ind: 3, Epoch: 2043, train_loss: 1.1762228541904025, valid_loss: 1.149274402194553\n",
            "test_ind: 3, Epoch: 2044, train_loss: 1.0816420154807007, valid_loss: 1.2747343911064997\n",
            "test_ind: 3, Epoch: 2045, train_loss: 1.04421220002351, valid_loss: 0.955127027299669\n",
            "test_ind: 3, Epoch: 2046, train_loss: 1.0721604205943922, valid_loss: 1.1230776574876575\n",
            "test_ind: 3, Epoch: 2047, train_loss: 1.06149924242938, valid_loss: 1.0526552200317383\n",
            "test_ind: 3, Epoch: 2048, train_loss: 1.1166532657764574, valid_loss: 0.9835494535940665\n",
            "test_ind: 3, Epoch: 2049, train_loss: 1.1121081305138858, valid_loss: 1.0667189845332392\n",
            "test_ind: 3, Epoch: 2050, train_loss: 1.0865550335542657, valid_loss: 1.0571391317579482\n",
            "test_ind: 3, Epoch: 2051, train_loss: 1.1029870539535713, valid_loss: 1.0560061136881511\n",
            "test_ind: 3, Epoch: 2052, train_loss: 1.0682981573505166, valid_loss: 1.0269873230545608\n",
            "test_ind: 3, Epoch: 2053, train_loss: 1.0590071854767975, valid_loss: 0.9805251404091164\n",
            "test_ind: 3, Epoch: 2054, train_loss: 1.0658315493736739, valid_loss: 1.0456966824001737\n",
            "test_ind: 3, Epoch: 2055, train_loss: 1.1474052240819106, valid_loss: 1.026565710703532\n",
            "test_ind: 3, Epoch: 2056, train_loss: 1.0843574147165558, valid_loss: 1.0332095181500471\n",
            "test_ind: 3, Epoch: 2057, train_loss: 1.1705604423711329, valid_loss: 1.179536430924027\n",
            "test_ind: 3, Epoch: 2058, train_loss: 1.0670446937466846, valid_loss: 0.9260246841995804\n",
            "Validation loss decreased (0.939625104268392 --> 0.9260246841995804).  Saving model ...\n",
            "test_ind: 3, Epoch: 2059, train_loss: 1.0729221944455747, valid_loss: 1.110547348305031\n",
            "test_ind: 3, Epoch: 2060, train_loss: 1.0527717625653303, valid_loss: 1.0396344396803114\n",
            "test_ind: 3, Epoch: 2061, train_loss: 1.0510969750675154, valid_loss: 0.9731885238930033\n",
            "test_ind: 3, Epoch: 2062, train_loss: 1.0403163992328408, valid_loss: 0.9772050822222674\n",
            "test_ind: 3, Epoch: 2063, train_loss: 1.068837042208071, valid_loss: 1.0184731836672183\n",
            "test_ind: 3, Epoch: 2064, train_loss: 1.0602883232964408, valid_loss: 0.9912777300234195\n",
            "test_ind: 3, Epoch: 2065, train_loss: 1.1612886676081904, valid_loss: 1.2759835631759078\n",
            "test_ind: 3, Epoch: 2066, train_loss: 1.00599714561745, valid_loss: 0.9302429623074002\n",
            "test_ind: 3, Epoch: 2067, train_loss: 1.0621023590182077, valid_loss: 1.2635801456592701\n",
            "test_ind: 3, Epoch: 2068, train_loss: 1.0326714044735754, valid_loss: 1.0305572792335793\n",
            "test_ind: 3, Epoch: 2069, train_loss: 1.1010906666885187, valid_loss: 1.1595606450681333\n",
            "test_ind: 3, Epoch: 2070, train_loss: 1.014941362687099, valid_loss: 1.0199654897054036\n",
            "test_ind: 3, Epoch: 2071, train_loss: 1.1078301300237208, valid_loss: 1.242059213143808\n",
            "test_ind: 3, Epoch: 2072, train_loss: 1.1063802742663724, valid_loss: 1.2474981943766275\n",
            "test_ind: 3, Epoch: 2073, train_loss: 1.176312658521864, valid_loss: 1.1063253614637587\n",
            "test_ind: 3, Epoch: 2074, train_loss: 1.1490485108928914, valid_loss: 0.9932022624545627\n",
            "test_ind: 3, Epoch: 2075, train_loss: 1.1204575609277798, valid_loss: 1.1546380431563765\n",
            "test_ind: 3, Epoch: 2076, train_loss: 1.107857651180691, valid_loss: 1.2286669766461407\n",
            "test_ind: 3, Epoch: 2077, train_loss: 1.1516064243552124, valid_loss: 1.1893523534138997\n",
            "test_ind: 3, Epoch: 2078, train_loss: 1.0608621703253853, valid_loss: 0.9858654163501881\n",
            "test_ind: 3, Epoch: 2079, train_loss: 1.197228537665473, valid_loss: 1.0261614587571886\n",
            "test_ind: 3, Epoch: 2080, train_loss: 1.096853933216613, valid_loss: 0.9847874994631167\n",
            "test_ind: 3, Epoch: 2081, train_loss: 1.0134665877730757, valid_loss: 1.2114323156851308\n",
            "test_ind: 3, Epoch: 2082, train_loss: 1.0966336521101585, valid_loss: 1.0509743160671658\n",
            "test_ind: 3, Epoch: 2083, train_loss: 1.0062338275673948, valid_loss: 1.1777279641893175\n",
            "test_ind: 3, Epoch: 2084, train_loss: 1.1266405788468725, valid_loss: 1.061884774102105\n",
            "test_ind: 3, Epoch: 2085, train_loss: 1.0884976210417572, valid_loss: 1.26747700020119\n",
            "test_ind: 3, Epoch: 2086, train_loss: 0.9749527801702051, valid_loss: 1.0393037619414154\n",
            "test_ind: 3, Epoch: 2087, train_loss: 1.070861127641466, valid_loss: 1.028048691926179\n",
            "test_ind: 3, Epoch: 2088, train_loss: 1.1173303922017415, valid_loss: 1.348553110052038\n",
            "test_ind: 3, Epoch: 2089, train_loss: 1.0615955399878232, valid_loss: 0.9791189652902108\n",
            "test_ind: 3, Epoch: 2090, train_loss: 1.0910470103040153, valid_loss: 1.2971541793258101\n",
            "test_ind: 3, Epoch: 2091, train_loss: 1.1033566498462064, valid_loss: 1.0849474977563929\n",
            "test_ind: 3, Epoch: 2092, train_loss: 1.1601215527381423, valid_loss: 1.1751876583805791\n",
            "test_ind: 3, Epoch: 2093, train_loss: 1.1187850810863351, valid_loss: 1.0255761323151764\n",
            "test_ind: 3, Epoch: 2094, train_loss: 1.0691506009043, valid_loss: 1.178252308456986\n",
            "test_ind: 3, Epoch: 2095, train_loss: 1.0512473082836764, valid_loss: 1.1189796129862468\n",
            "test_ind: 3, Epoch: 2096, train_loss: 1.081012749377592, valid_loss: 0.9794998345551668\n",
            "test_ind: 3, Epoch: 2097, train_loss: 1.0327656004163956, valid_loss: 1.0527948626765498\n",
            "test_ind: 3, Epoch: 2098, train_loss: 0.9722279972500271, valid_loss: 0.9446584383646647\n",
            "test_ind: 3, Epoch: 2099, train_loss: 1.0178295771280923, valid_loss: 0.9035405406245479\n",
            "Validation loss decreased (0.9260246841995804 --> 0.9035405406245479).  Saving model ...\n",
            "test_ind: 3, Epoch: 2100, train_loss: 1.0963240376225223, valid_loss: 1.3871965408325195\n",
            "test_ind: 3, Epoch: 2101, train_loss: 1.0787670641769598, valid_loss: 1.0520041253831651\n",
            "test_ind: 3, Epoch: 2102, train_loss: 1.091409194616624, valid_loss: 1.3014625090139884\n",
            "test_ind: 3, Epoch: 2103, train_loss: 1.0199384100643205, valid_loss: 1.0647264409948278\n",
            "test_ind: 3, Epoch: 2104, train_loss: 1.011724419063992, valid_loss: 1.0244550351743344\n",
            "test_ind: 3, Epoch: 2105, train_loss: 1.079865332002993, valid_loss: 1.0345996220906575\n",
            "test_ind: 3, Epoch: 2106, train_loss: 1.0840266663351177, valid_loss: 1.1927487585279677\n",
            "test_ind: 3, Epoch: 2107, train_loss: 1.0903321725350839, valid_loss: 1.0700766951949507\n",
            "test_ind: 3, Epoch: 2108, train_loss: 1.070754369099935, valid_loss: 1.1010120356524433\n",
            "test_ind: 3, Epoch: 2109, train_loss: 1.0269876115116072, valid_loss: 0.940716337274622\n",
            "test_ind: 3, Epoch: 2110, train_loss: 1.1596947775946722, valid_loss: 1.0132756056608976\n",
            "test_ind: 3, Epoch: 2111, train_loss: 1.0560633753552848, valid_loss: 1.1293490197923448\n",
            "test_ind: 3, Epoch: 2112, train_loss: 1.0860954331762998, valid_loss: 1.0194961406566478\n",
            "test_ind: 3, Epoch: 2113, train_loss: 1.0714438991782105, valid_loss: 1.1134757465786405\n",
            "test_ind: 3, Epoch: 2114, train_loss: 1.077394762156922, valid_loss: 1.008635821165862\n",
            "test_ind: 3, Epoch: 2115, train_loss: 0.9790475456802933, valid_loss: 0.9775670016253436\n",
            "test_ind: 3, Epoch: 2116, train_loss: 1.0468483442141685, valid_loss: 1.059139922813133\n",
            "test_ind: 3, Epoch: 2117, train_loss: 1.1008603661148637, valid_loss: 1.228315830230713\n",
            "test_ind: 3, Epoch: 2118, train_loss: 1.0826673036740149, valid_loss: 1.0964388494138364\n",
            "test_ind: 3, Epoch: 2119, train_loss: 1.0846512464829432, valid_loss: 0.9545043309529623\n",
            "test_ind: 3, Epoch: 2120, train_loss: 1.070098959369424, valid_loss: 0.9665639841998064\n",
            "test_ind: 3, Epoch: 2121, train_loss: 1.0558076081452545, valid_loss: 1.0957748625013564\n",
            "test_ind: 3, Epoch: 2122, train_loss: 1.0830735689328042, valid_loss: 1.0575141906738281\n",
            "test_ind: 3, Epoch: 2123, train_loss: 0.9625749882356621, valid_loss: 0.9097790011653193\n",
            "test_ind: 3, Epoch: 2124, train_loss: 1.0808659424016505, valid_loss: 0.9867466820610895\n",
            "test_ind: 3, Epoch: 2125, train_loss: 1.014751151756004, valid_loss: 0.9578974865101001\n",
            "test_ind: 3, Epoch: 2126, train_loss: 1.0279999721197435, valid_loss: 1.0266259158099138\n",
            "test_ind: 3, Epoch: 2127, train_loss: 1.0693030769442335, valid_loss: 1.1127521197001138\n",
            "test_ind: 3, Epoch: 2128, train_loss: 1.0376898094459817, valid_loss: 1.1480601981834129\n",
            "test_ind: 3, Epoch: 2129, train_loss: 1.0163020969908914, valid_loss: 1.02198475378531\n",
            "test_ind: 3, Epoch: 2130, train_loss: 1.101625560242453, valid_loss: 1.0572188165452745\n",
            "test_ind: 3, Epoch: 2131, train_loss: 0.9600160798908752, valid_loss: 0.9148049177946868\n",
            "test_ind: 3, Epoch: 2132, train_loss: 1.0299173932016632, valid_loss: 1.1233744268064145\n",
            "test_ind: 3, Epoch: 2133, train_loss: 1.0768488189320506, valid_loss: 1.1102233816076208\n",
            "test_ind: 3, Epoch: 2134, train_loss: 1.012546710026117, valid_loss: 0.9116571567676686\n",
            "test_ind: 3, Epoch: 2135, train_loss: 1.0047579518070926, valid_loss: 1.0753032366434734\n",
            "test_ind: 3, Epoch: 2136, train_loss: 1.043245233135459, valid_loss: 0.9490552301760073\n",
            "test_ind: 3, Epoch: 2137, train_loss: 1.0060283284128448, valid_loss: 0.9190819175155076\n",
            "test_ind: 3, Epoch: 2138, train_loss: 1.043721717080952, valid_loss: 1.2072275126421894\n",
            "test_ind: 3, Epoch: 2139, train_loss: 1.0809092462798695, valid_loss: 1.1194555671126756\n",
            "test_ind: 3, Epoch: 2140, train_loss: 1.043675269609616, valid_loss: 0.9815043166831687\n",
            "test_ind: 3, Epoch: 2141, train_loss: 0.9584488456631883, valid_loss: 0.9751229816012913\n",
            "test_ind: 3, Epoch: 2142, train_loss: 1.018086721867691, valid_loss: 0.864285363091363\n",
            "Validation loss decreased (0.9035405406245479 --> 0.864285363091363).  Saving model ...\n",
            "test_ind: 3, Epoch: 2143, train_loss: 1.036893049875895, valid_loss: 1.0948755652816207\n",
            "test_ind: 3, Epoch: 2144, train_loss: 1.018352961834566, valid_loss: 1.0632479102523238\n",
            "test_ind: 3, Epoch: 2145, train_loss: 1.03140084537459, valid_loss: 0.9369126249242712\n",
            "test_ind: 3, Epoch: 2146, train_loss: 0.9981187891077111, valid_loss: 0.9618424133018211\n",
            "test_ind: 3, Epoch: 2147, train_loss: 1.0337601473302018, valid_loss: 0.9770827293395996\n",
            "test_ind: 3, Epoch: 2148, train_loss: 1.0327453377806108, valid_loss: 0.9649238586425781\n",
            "test_ind: 3, Epoch: 2149, train_loss: 1.053473284215103, valid_loss: 1.0971882784808125\n",
            "test_ind: 3, Epoch: 2150, train_loss: 1.014073430755992, valid_loss: 1.1201619748716\n",
            "test_ind: 3, Epoch: 2151, train_loss: 1.050626089543472, valid_loss: 0.9350792037116157\n",
            "test_ind: 3, Epoch: 2152, train_loss: 1.080270943818269, valid_loss: 0.8733101774145056\n",
            "test_ind: 3, Epoch: 2153, train_loss: 1.0524914117506994, valid_loss: 0.9713943799336751\n",
            "test_ind: 3, Epoch: 2154, train_loss: 1.0050591009634513, valid_loss: 0.9764594325312862\n",
            "test_ind: 3, Epoch: 2155, train_loss: 1.095985930642964, valid_loss: 1.0042950135690194\n",
            "test_ind: 3, Epoch: 2156, train_loss: 1.0737457157653054, valid_loss: 1.0809359903688784\n",
            "test_ind: 3, Epoch: 2157, train_loss: 1.1162521750838668, valid_loss: 0.998895627480966\n",
            "test_ind: 3, Epoch: 2158, train_loss: 0.9755637086467978, valid_loss: 1.0891184277004666\n",
            "test_ind: 3, Epoch: 2159, train_loss: 0.9631206253428518, valid_loss: 0.9120430769743744\n",
            "test_ind: 3, Epoch: 2160, train_loss: 1.0832876158349307, valid_loss: 1.3928867092838995\n",
            "test_ind: 3, Epoch: 2161, train_loss: 1.0834225842982164, valid_loss: 1.1319321349815086\n",
            "test_ind: 3, Epoch: 2162, train_loss: 1.0305523872375488, valid_loss: 0.9410657352871364\n",
            "test_ind: 3, Epoch: 2163, train_loss: 1.0594857769247925, valid_loss: 1.0179972648620605\n",
            "test_ind: 3, Epoch: 2164, train_loss: 1.0925761152196813, valid_loss: 1.0457992906923645\n",
            "test_ind: 3, Epoch: 2165, train_loss: 0.9916273517373168, valid_loss: 0.8468640998557763\n",
            "Validation loss decreased (0.864285363091363 --> 0.8468640998557763).  Saving model ...\n",
            "test_ind: 3, Epoch: 2166, train_loss: 1.0157457928598663, valid_loss: 0.8877410712065521\n",
            "test_ind: 3, Epoch: 2167, train_loss: 1.059955025896614, valid_loss: 0.9110198020935059\n",
            "test_ind: 3, Epoch: 2168, train_loss: 0.9970383997316715, valid_loss: 0.9676907857259114\n",
            "test_ind: 3, Epoch: 2169, train_loss: 1.0888229417212216, valid_loss: 1.2727470574555573\n",
            "test_ind: 3, Epoch: 2170, train_loss: 1.0335082007043155, valid_loss: 1.0780243520383483\n",
            "test_ind: 3, Epoch: 2171, train_loss: 1.0606858524275415, valid_loss: 0.9850492654023346\n",
            "test_ind: 3, Epoch: 2172, train_loss: 1.0458653767903647, valid_loss: 1.0707758444326896\n",
            "test_ind: 3, Epoch: 2173, train_loss: 1.0141437318589952, valid_loss: 1.0936400802047164\n",
            "test_ind: 3, Epoch: 2174, train_loss: 0.9992924089784975, valid_loss: 1.0062391316449202\n",
            "test_ind: 3, Epoch: 2175, train_loss: 1.0367099444071453, valid_loss: 0.9491339789496529\n",
            "test_ind: 3, Epoch: 2176, train_loss: 1.053923871782091, valid_loss: 0.9714968116195115\n",
            "test_ind: 3, Epoch: 2177, train_loss: 0.9645901079531068, valid_loss: 0.9745662830494067\n",
            "test_ind: 3, Epoch: 2178, train_loss: 1.0331765398567105, valid_loss: 1.1464654781200267\n",
            "test_ind: 3, Epoch: 2179, train_loss: 1.0267190638883614, valid_loss: 1.0756713725902416\n",
            "test_ind: 3, Epoch: 2180, train_loss: 0.9771907064649795, valid_loss: 0.9718027821293584\n",
            "test_ind: 3, Epoch: 2181, train_loss: 1.0065579885317957, valid_loss: 0.9913925241540978\n",
            "test_ind: 3, Epoch: 2182, train_loss: 1.0265746528719677, valid_loss: 1.233914675535979\n",
            "test_ind: 3, Epoch: 2183, train_loss: 1.0413132655767747, valid_loss: 0.8799836900499132\n",
            "test_ind: 3, Epoch: 2184, train_loss: 1.0450725143338426, valid_loss: 1.015355039525915\n",
            "test_ind: 3, Epoch: 2185, train_loss: 0.9302335433018061, valid_loss: 0.9561488893296983\n",
            "test_ind: 3, Epoch: 2186, train_loss: 0.9982683275952751, valid_loss: 0.956080154136375\n",
            "test_ind: 3, Epoch: 2187, train_loss: 1.0490767337657787, valid_loss: 1.057296629305239\n",
            "test_ind: 3, Epoch: 2188, train_loss: 1.040785136046233, valid_loss: 1.3168002587777596\n",
            "test_ind: 3, Epoch: 2189, train_loss: 1.054067258481626, valid_loss: 1.1450766104239003\n",
            "test_ind: 3, Epoch: 2190, train_loss: 1.0723986390196247, valid_loss: 1.0100726198267052\n",
            "test_ind: 3, Epoch: 2191, train_loss: 1.0435519100707253, valid_loss: 0.9606584972805448\n",
            "test_ind: 3, Epoch: 2192, train_loss: 1.0872529642081554, valid_loss: 1.1337047153049045\n",
            "test_ind: 3, Epoch: 2193, train_loss: 1.0429487640475048, valid_loss: 1.1230465571085613\n",
            "test_ind: 3, Epoch: 2194, train_loss: 1.0105385485990548, valid_loss: 0.8728017807006836\n",
            "test_ind: 3, Epoch: 2195, train_loss: 0.924103318909068, valid_loss: 0.9061569637722439\n",
            "test_ind: 3, Epoch: 2196, train_loss: 1.046107886750021, valid_loss: 0.9996498779014305\n",
            "test_ind: 3, Epoch: 2197, train_loss: 1.0386643409729004, valid_loss: 1.10682921939426\n",
            "test_ind: 3, Epoch: 2198, train_loss: 1.0088476133935245, valid_loss: 1.1806598416081182\n",
            "test_ind: 3, Epoch: 2199, train_loss: 1.0192057115060311, valid_loss: 1.2020027549178511\n",
            "test_ind: 3, Epoch: 2200, train_loss: 1.0672275048715096, valid_loss: 1.0739447629010237\n",
            "test_ind: 3, Epoch: 2201, train_loss: 1.051379810144872, valid_loss: 0.948034922281901\n",
            "test_ind: 3, Epoch: 2202, train_loss: 1.0862228664351097, valid_loss: 1.1987924752412018\n",
            "test_ind: 3, Epoch: 2203, train_loss: 1.0485991607477636, valid_loss: 0.9817692438761394\n",
            "test_ind: 3, Epoch: 2204, train_loss: 1.0210072847060216, valid_loss: 0.884123307687265\n",
            "test_ind: 3, Epoch: 2205, train_loss: 1.0146001356619374, valid_loss: 1.0239538616604276\n",
            "test_ind: 3, Epoch: 2206, train_loss: 1.0902539123723534, valid_loss: 1.0087733621950503\n",
            "test_ind: 3, Epoch: 2207, train_loss: 0.9927356449174293, valid_loss: 0.9321787622239854\n",
            "test_ind: 3, Epoch: 2208, train_loss: 1.012448752367938, valid_loss: 1.013099299536811\n",
            "test_ind: 3, Epoch: 2209, train_loss: 1.0076066358589832, valid_loss: 0.9741825880827727\n",
            "test_ind: 3, Epoch: 2210, train_loss: 1.0576399932672949, valid_loss: 1.1351932419670954\n",
            "test_ind: 3, Epoch: 2211, train_loss: 1.0154032589476787, valid_loss: 1.0353344634727195\n",
            "test_ind: 3, Epoch: 2212, train_loss: 0.9813740871570729, valid_loss: 0.9948410634641294\n",
            "test_ind: 3, Epoch: 2213, train_loss: 1.0071654260894398, valid_loss: 0.9915595761051884\n",
            "test_ind: 3, Epoch: 2214, train_loss: 1.0694183420251917, valid_loss: 1.2014964774802879\n",
            "test_ind: 3, Epoch: 2215, train_loss: 1.0051943049018766, valid_loss: 1.1112507360952872\n",
            "test_ind: 3, Epoch: 2216, train_loss: 1.0416657306529857, valid_loss: 1.1113935399938513\n",
            "test_ind: 3, Epoch: 2217, train_loss: 1.054272946016288, valid_loss: 1.1348550937793873\n",
            "test_ind: 3, Epoch: 2218, train_loss: 0.9724131572393725, valid_loss: 0.8866466592859339\n",
            "test_ind: 3, Epoch: 2219, train_loss: 1.0091999725059226, valid_loss: 0.9254464043511285\n",
            "test_ind: 3, Epoch: 2220, train_loss: 0.9889296013631939, valid_loss: 1.055579044200756\n",
            "test_ind: 3, Epoch: 2221, train_loss: 1.0575178346516174, valid_loss: 1.0757518344455295\n",
            "test_ind: 3, Epoch: 2222, train_loss: 0.9588877418894827, valid_loss: 1.0447689338966653\n",
            "test_ind: 3, Epoch: 2223, train_loss: 1.004292493985023, valid_loss: 1.0310074311715585\n",
            "test_ind: 3, Epoch: 2224, train_loss: 0.985163712207182, valid_loss: 1.0880905080724645\n",
            "test_ind: 3, Epoch: 2225, train_loss: 0.9712264390639317, valid_loss: 0.9890700799447519\n",
            "test_ind: 3, Epoch: 2226, train_loss: 0.9783562613122256, valid_loss: 1.1262479888068304\n",
            "test_ind: 3, Epoch: 2227, train_loss: 0.9902953571743436, valid_loss: 0.8139967388576932\n",
            "Validation loss decreased (0.8468640998557763 --> 0.8139967388576932).  Saving model ...\n",
            "test_ind: 3, Epoch: 2228, train_loss: 1.0251663820243175, valid_loss: 1.0486774797792788\n",
            "test_ind: 3, Epoch: 2229, train_loss: 1.0326486458013087, valid_loss: 0.9968540756790727\n",
            "test_ind: 3, Epoch: 2230, train_loss: 0.9432549417754749, valid_loss: 1.0361378457811146\n",
            "test_ind: 3, Epoch: 2231, train_loss: 0.9642710626861195, valid_loss: 0.9500071737501357\n",
            "test_ind: 3, Epoch: 2232, train_loss: 1.0797634654574924, valid_loss: 1.6123204054655853\n",
            "test_ind: 3, Epoch: 2233, train_loss: 1.0583397017584906, valid_loss: 1.0450650497719094\n",
            "test_ind: 3, Epoch: 2234, train_loss: 1.0314086454885976, valid_loss: 0.9400917865611889\n",
            "test_ind: 3, Epoch: 2235, train_loss: 0.9809147928967888, valid_loss: 0.924813606120922\n",
            "test_ind: 3, Epoch: 2236, train_loss: 1.0282658471001518, valid_loss: 1.045474229035554\n",
            "test_ind: 3, Epoch: 2237, train_loss: 1.1336967680189345, valid_loss: 0.9916002662093553\n",
            "test_ind: 3, Epoch: 2238, train_loss: 1.0884903742943282, valid_loss: 1.0164726222002949\n",
            "test_ind: 3, Epoch: 2239, train_loss: 0.9655341277887792, valid_loss: 0.9657286714624475\n",
            "test_ind: 3, Epoch: 2240, train_loss: 1.034652727621573, valid_loss: 0.9907583307336878\n",
            "test_ind: 3, Epoch: 2241, train_loss: 0.9587686797718943, valid_loss: 0.9632557586387351\n",
            "test_ind: 3, Epoch: 2242, train_loss: 1.054209561995518, valid_loss: 1.4236078438935458\n",
            "test_ind: 3, Epoch: 2243, train_loss: 0.9989623846831145, valid_loss: 1.0238254865010579\n",
            "test_ind: 3, Epoch: 2244, train_loss: 0.9663354496897003, valid_loss: 0.8832160631815593\n",
            "test_ind: 3, Epoch: 2245, train_loss: 0.9664164472509313, valid_loss: 0.9236674485383211\n",
            "test_ind: 3, Epoch: 2246, train_loss: 1.0350749345473302, valid_loss: 1.006503740946452\n",
            "test_ind: 3, Epoch: 2247, train_loss: 1.0182610617743597, valid_loss: 1.0288062272248446\n",
            "test_ind: 3, Epoch: 2248, train_loss: 0.981051003491437, valid_loss: 0.9258067696182816\n",
            "test_ind: 3, Epoch: 2249, train_loss: 1.0000951260696223, valid_loss: 0.9726556671990289\n",
            "test_ind: 3, Epoch: 2250, train_loss: 0.927466786937949, valid_loss: 0.9631039124948007\n",
            "test_ind: 3, Epoch: 2251, train_loss: 0.9815908832314574, valid_loss: 0.9353699860749422\n",
            "test_ind: 3, Epoch: 2252, train_loss: 0.9743996726142035, valid_loss: 1.1458727695323803\n",
            "test_ind: 3, Epoch: 2253, train_loss: 0.9466522240344389, valid_loss: 0.9699458016289604\n",
            "test_ind: 3, Epoch: 2254, train_loss: 1.0108621679706338, valid_loss: 1.0130747159322102\n",
            "test_ind: 3, Epoch: 2255, train_loss: 1.0285302562478147, valid_loss: 1.0093794575443975\n",
            "test_ind: 3, Epoch: 2256, train_loss: 1.1008347935146754, valid_loss: 0.9329651726616754\n",
            "test_ind: 3, Epoch: 2257, train_loss: 0.9174537364347483, valid_loss: 0.9981530684011954\n",
            "test_ind: 3, Epoch: 2258, train_loss: 0.979832661004714, valid_loss: 1.2091513033266421\n",
            "test_ind: 3, Epoch: 2259, train_loss: 1.0449735500194408, valid_loss: 1.0351048045688205\n",
            "test_ind: 3, Epoch: 2260, train_loss: 0.9926503205005034, valid_loss: 0.9404061458728931\n",
            "test_ind: 3, Epoch: 2261, train_loss: 0.9862096633440183, valid_loss: 0.895482822700783\n",
            "test_ind: 3, Epoch: 2262, train_loss: 1.1518991376146859, valid_loss: 1.162643273671468\n",
            "test_ind: 3, Epoch: 2263, train_loss: 1.029182392873882, valid_loss: 0.9025385821307147\n",
            "test_ind: 3, Epoch: 2264, train_loss: 0.9639957922476308, valid_loss: 1.008020595267967\n",
            "test_ind: 3, Epoch: 2265, train_loss: 0.9880604626219949, valid_loss: 0.9470526730572736\n",
            "test_ind: 3, Epoch: 2266, train_loss: 0.9755794678205325, valid_loss: 1.0839537691186976\n",
            "test_ind: 3, Epoch: 2267, train_loss: 0.9327896377186717, valid_loss: 0.831467999352349\n",
            "test_ind: 3, Epoch: 2268, train_loss: 0.9692577373834302, valid_loss: 1.060774326324463\n",
            "test_ind: 3, Epoch: 2269, train_loss: 1.0340541615898227, valid_loss: 1.0540822523611564\n",
            "test_ind: 3, Epoch: 2270, train_loss: 1.0004045050821189, valid_loss: 1.0209955462702998\n",
            "test_ind: 3, Epoch: 2271, train_loss: 1.0599603476347748, valid_loss: 0.9876136603178802\n",
            "test_ind: 3, Epoch: 2272, train_loss: 1.0018740877693082, valid_loss: 0.9872749469898365\n",
            "test_ind: 3, Epoch: 2273, train_loss: 0.9855641965512876, valid_loss: 1.1338136284439653\n",
            "test_ind: 3, Epoch: 2274, train_loss: 1.0553962978315943, valid_loss: 0.8551134533352321\n",
            "test_ind: 3, Epoch: 2275, train_loss: 0.9727750001130281, valid_loss: 0.9467031160990397\n",
            "test_ind: 3, Epoch: 2276, train_loss: 0.9240096468984346, valid_loss: 0.8595777440954139\n",
            "test_ind: 3, Epoch: 2277, train_loss: 0.9508512814839681, valid_loss: 1.0144018773679382\n",
            "test_ind: 3, Epoch: 2278, train_loss: 1.0438927426750275, valid_loss: 0.9511131710476346\n",
            "test_ind: 3, Epoch: 2279, train_loss: 0.9930978939857013, valid_loss: 1.0395306834468134\n",
            "test_ind: 3, Epoch: 2280, train_loss: 1.0036321451634538, valid_loss: 0.971029899738453\n",
            "test_ind: 3, Epoch: 2281, train_loss: 0.9216660629084079, valid_loss: 1.03742484693174\n",
            "test_ind: 3, Epoch: 2282, train_loss: 1.0050945046507282, valid_loss: 0.8905478053622775\n",
            "test_ind: 3, Epoch: 2283, train_loss: 0.9760819482214657, valid_loss: 1.0002079363222474\n",
            "test_ind: 3, Epoch: 2284, train_loss: 1.0096696158986032, valid_loss: 0.972747838055646\n",
            "test_ind: 3, Epoch: 2285, train_loss: 1.0767632708137418, valid_loss: 0.9864344243650083\n",
            "test_ind: 3, Epoch: 2286, train_loss: 0.9816904951024937, valid_loss: 1.0575770801968045\n",
            "test_ind: 3, Epoch: 2287, train_loss: 0.9610032328852899, valid_loss: 0.9041949731332285\n",
            "test_ind: 3, Epoch: 2288, train_loss: 0.9928820162643619, valid_loss: 0.9572284133345993\n",
            "test_ind: 3, Epoch: 2289, train_loss: 1.0746492515375585, valid_loss: 1.1084910675331399\n",
            "test_ind: 3, Epoch: 2290, train_loss: 0.9812364519378285, valid_loss: 0.9336999963831019\n",
            "test_ind: 3, Epoch: 2291, train_loss: 1.0348410900728202, valid_loss: 1.067160959596987\n",
            "test_ind: 3, Epoch: 2292, train_loss: 0.9752949726434401, valid_loss: 0.9656200055722838\n",
            "test_ind: 3, Epoch: 2293, train_loss: 0.9567102797237443, valid_loss: 0.9581323906227392\n",
            "test_ind: 3, Epoch: 2294, train_loss: 0.9599316267319669, valid_loss: 0.8832947942945693\n",
            "test_ind: 3, Epoch: 2295, train_loss: 1.003947134371157, valid_loss: 0.9466205349674932\n",
            "test_ind: 3, Epoch: 2296, train_loss: 0.9632145269417469, valid_loss: 0.8813980773643211\n",
            "test_ind: 3, Epoch: 2297, train_loss: 1.0096673906585316, valid_loss: 0.9243000878228081\n",
            "test_ind: 3, Epoch: 2298, train_loss: 1.0508482721116805, valid_loss: 1.2354811915644892\n",
            "test_ind: 3, Epoch: 2299, train_loss: 1.002001432724941, valid_loss: 1.0648801944873951\n",
            "test_ind: 3, Epoch: 2300, train_loss: 0.978526904259199, valid_loss: 0.8789381274470577\n",
            "test_ind: 3, Epoch: 2301, train_loss: 0.937721152364472, valid_loss: 0.9602513666506167\n",
            "test_ind: 3, Epoch: 2302, train_loss: 0.9632449385560587, valid_loss: 0.8741541262026187\n",
            "test_ind: 3, Epoch: 2303, train_loss: 0.9886660929079409, valid_loss: 1.0282802051968045\n",
            "test_ind: 3, Epoch: 2304, train_loss: 1.0483323203192816, valid_loss: 1.0019716686672635\n",
            "test_ind: 3, Epoch: 2305, train_loss: 1.0676422943303614, valid_loss: 0.9227150281270345\n",
            "test_ind: 3, Epoch: 2306, train_loss: 0.9539693903040002, valid_loss: 1.160513030158149\n",
            "test_ind: 3, Epoch: 2307, train_loss: 1.000370826250241, valid_loss: 1.234079254998101\n",
            "test_ind: 3, Epoch: 2308, train_loss: 1.0175298525963299, valid_loss: 1.0947331145957664\n",
            "test_ind: 3, Epoch: 2309, train_loss: 1.016212475152663, valid_loss: 1.086877363699454\n",
            "test_ind: 3, Epoch: 2310, train_loss: 0.975100099304576, valid_loss: 0.9236713515387641\n",
            "test_ind: 3, Epoch: 2311, train_loss: 0.9679903042169264, valid_loss: 0.9631646120989763\n",
            "test_ind: 3, Epoch: 2312, train_loss: 0.9496910130536116, valid_loss: 0.9837730372393574\n",
            "test_ind: 3, Epoch: 2313, train_loss: 0.982701648900538, valid_loss: 1.074852978741681\n",
            "test_ind: 3, Epoch: 2314, train_loss: 0.9677680039111478, valid_loss: 1.012492956938567\n",
            "test_ind: 3, Epoch: 2315, train_loss: 0.9321949158185793, valid_loss: 0.8828552563985188\n",
            "test_ind: 3, Epoch: 2316, train_loss: 1.0016992592517242, valid_loss: 1.0584475375987865\n",
            "test_ind: 3, Epoch: 2317, train_loss: 0.9784352867691605, valid_loss: 0.9718407171743888\n",
            "test_ind: 3, Epoch: 2318, train_loss: 1.0183210137449668, valid_loss: 0.851214709105315\n",
            "test_ind: 3, Epoch: 2319, train_loss: 0.9390779542334285, valid_loss: 0.8637203287195275\n",
            "test_ind: 3, Epoch: 2320, train_loss: 1.0466552604863673, valid_loss: 1.0352296829223633\n",
            "test_ind: 3, Epoch: 2321, train_loss: 0.9949922031826443, valid_loss: 0.9741157425774468\n",
            "test_ind: 3, Epoch: 2322, train_loss: 0.9354733949826088, valid_loss: 0.9685609428970904\n",
            "test_ind: 3, Epoch: 2323, train_loss: 0.9616867524606211, valid_loss: 1.021735844788728\n",
            "test_ind: 3, Epoch: 2324, train_loss: 0.9642618968163007, valid_loss: 1.0651972735369646\n",
            "test_ind: 3, Epoch: 2325, train_loss: 0.9539125524921183, valid_loss: 0.9305629553618254\n",
            "test_ind: 3, Epoch: 2326, train_loss: 0.9474361148881322, valid_loss: 0.9980858343618888\n",
            "test_ind: 3, Epoch: 2327, train_loss: 1.0031104382173512, valid_loss: 0.9934223139727557\n",
            "test_ind: 3, Epoch: 2328, train_loss: 0.9987866613599989, valid_loss: 1.1122435640405726\n",
            "test_ind: 3, Epoch: 2329, train_loss: 1.0027193081231764, valid_loss: 0.9411567582024468\n",
            "test_ind: 3, Epoch: 2330, train_loss: 0.9668441584080825, valid_loss: 0.9387796190049914\n",
            "test_ind: 3, Epoch: 2331, train_loss: 1.0006289776460624, valid_loss: 1.0617543856302896\n",
            "test_ind: 3, Epoch: 2332, train_loss: 0.9208022046972203, valid_loss: 0.8059629510950159\n",
            "Validation loss decreased (0.8139967388576932 --> 0.8059629510950159).  Saving model ...\n",
            "test_ind: 3, Epoch: 2333, train_loss: 0.9891985434072988, valid_loss: 1.0405844935664426\n",
            "test_ind: 3, Epoch: 2334, train_loss: 1.0065070964671947, valid_loss: 0.9566607122068053\n",
            "test_ind: 3, Epoch: 2335, train_loss: 0.9880187776353624, valid_loss: 0.9568382722360117\n",
            "test_ind: 3, Epoch: 2336, train_loss: 0.9762415944794077, valid_loss: 0.86507221504494\n",
            "test_ind: 3, Epoch: 2337, train_loss: 0.9551516050173912, valid_loss: 1.2364917154665347\n",
            "test_ind: 3, Epoch: 2338, train_loss: 1.0055499371187184, valid_loss: 0.9698965284559462\n",
            "test_ind: 3, Epoch: 2339, train_loss: 1.0008969424683372, valid_loss: 1.2148989924678095\n",
            "test_ind: 3, Epoch: 2340, train_loss: 1.1231572304242925, valid_loss: 0.986290348900689\n",
            "test_ind: 3, Epoch: 2341, train_loss: 1.0119228245299539, valid_loss: 1.1209172319482874\n",
            "test_ind: 3, Epoch: 2342, train_loss: 1.0241345476221155, valid_loss: 0.8707559020430954\n",
            "test_ind: 3, Epoch: 2343, train_loss: 1.0624695707250522, valid_loss: 1.122571309407552\n",
            "test_ind: 3, Epoch: 2344, train_loss: 1.0073456234402127, valid_loss: 0.8751537711532029\n",
            "test_ind: 3, Epoch: 2345, train_loss: 1.0112945179880402, valid_loss: 0.9083689230459708\n",
            "test_ind: 3, Epoch: 2346, train_loss: 0.9111718130700383, valid_loss: 1.0582602642200611\n",
            "test_ind: 3, Epoch: 2347, train_loss: 0.9574531920162247, valid_loss: 0.9413577185736761\n",
            "test_ind: 3, Epoch: 2348, train_loss: 0.9643178045013805, valid_loss: 0.8957137531704373\n",
            "test_ind: 3, Epoch: 2349, train_loss: 0.9571299376311125, valid_loss: 1.3245917249608925\n",
            "test_ind: 3, Epoch: 2350, train_loss: 1.002139703727063, valid_loss: 1.115051922974763\n",
            "test_ind: 3, Epoch: 2351, train_loss: 0.9416469938961077, valid_loss: 0.8852913291366011\n",
            "test_ind: 3, Epoch: 2352, train_loss: 0.9853086177213694, valid_loss: 1.0196900367736816\n",
            "test_ind: 3, Epoch: 2353, train_loss: 0.9349945503988384, valid_loss: 1.0087047506261755\n",
            "test_ind: 3, Epoch: 2354, train_loss: 0.9920102696359894, valid_loss: 0.9774724465829354\n",
            "test_ind: 3, Epoch: 2355, train_loss: 0.9919454374431091, valid_loss: 1.086210745352286\n",
            "test_ind: 3, Epoch: 2356, train_loss: 0.9664451987655072, valid_loss: 1.0494512805232294\n",
            "test_ind: 3, Epoch: 2357, train_loss: 1.0046239370181236, valid_loss: 0.9510075251261393\n",
            "test_ind: 3, Epoch: 2358, train_loss: 0.9876604080200195, valid_loss: 0.9761063434459545\n",
            "test_ind: 3, Epoch: 2359, train_loss: 0.9637875380339445, valid_loss: 1.022427205686216\n",
            "test_ind: 3, Epoch: 2360, train_loss: 0.9878596141014572, valid_loss: 0.974978694209346\n",
            "test_ind: 3, Epoch: 2361, train_loss: 0.9576859650788484, valid_loss: 1.1036392317877877\n",
            "test_ind: 3, Epoch: 2362, train_loss: 0.9771139768906582, valid_loss: 1.0106381663569697\n",
            "test_ind: 3, Epoch: 2363, train_loss: 1.0199070153412995, valid_loss: 1.0463440153333876\n",
            "test_ind: 3, Epoch: 2364, train_loss: 1.0032341450820734, valid_loss: 0.9250286420186361\n",
            "test_ind: 3, Epoch: 2365, train_loss: 0.9423901887587559, valid_loss: 1.062402054115578\n",
            "test_ind: 3, Epoch: 2366, train_loss: 1.0040067331290539, valid_loss: 0.9931738818133318\n",
            "test_ind: 3, Epoch: 2367, train_loss: 0.9724888212886855, valid_loss: 0.8164285024007162\n",
            "test_ind: 3, Epoch: 2368, train_loss: 0.9552193335544917, valid_loss: 0.8814601544980649\n",
            "test_ind: 3, Epoch: 2369, train_loss: 0.9602988560994468, valid_loss: 1.0290780244050204\n",
            "test_ind: 3, Epoch: 2370, train_loss: 0.9837254182792005, valid_loss: 0.9369180467393663\n",
            "test_ind: 3, Epoch: 2371, train_loss: 0.9598602424433202, valid_loss: 1.0121498107910156\n",
            "test_ind: 3, Epoch: 2372, train_loss: 0.9650006411988058, valid_loss: 0.9004395096390336\n",
            "test_ind: 3, Epoch: 2373, train_loss: 0.9371689690483941, valid_loss: 0.909792564533375\n",
            "test_ind: 3, Epoch: 2374, train_loss: 0.9543144908952123, valid_loss: 0.9938661257425944\n",
            "test_ind: 3, Epoch: 2375, train_loss: 0.9739992942339109, valid_loss: 0.8705393296700936\n",
            "test_ind: 3, Epoch: 2376, train_loss: 0.929349551966161, valid_loss: 1.1227394386574074\n",
            "test_ind: 3, Epoch: 2377, train_loss: 0.9578915525365761, valid_loss: 0.8001420939410175\n",
            "Validation loss decreased (0.8059629510950159 --> 0.8001420939410175).  Saving model ...\n",
            "test_ind: 3, Epoch: 2378, train_loss: 0.9873811109566395, valid_loss: 1.0193534074006256\n",
            "test_ind: 3, Epoch: 2379, train_loss: 0.9294526841905382, valid_loss: 0.9010058685585304\n",
            "test_ind: 3, Epoch: 2380, train_loss: 0.9372455455638744, valid_loss: 1.0926301391036422\n",
            "test_ind: 3, Epoch: 2381, train_loss: 0.9899541183754248, valid_loss: 0.90836383678295\n",
            "test_ind: 3, Epoch: 2382, train_loss: 0.9080301567360205, valid_loss: 0.8301884863111708\n",
            "test_ind: 3, Epoch: 2383, train_loss: 0.8749557954293709, valid_loss: 1.0123612262584545\n",
            "test_ind: 3, Epoch: 2384, train_loss: 1.0479066990039967, valid_loss: 1.2318449373598452\n",
            "test_ind: 3, Epoch: 2385, train_loss: 1.0995281419636291, valid_loss: 0.982182608710395\n",
            "test_ind: 3, Epoch: 2386, train_loss: 0.9897041026456858, valid_loss: 1.027408281962077\n",
            "test_ind: 3, Epoch: 2387, train_loss: 0.9263500990691008, valid_loss: 0.9341790587813765\n",
            "test_ind: 3, Epoch: 2388, train_loss: 0.9656794924794891, valid_loss: 1.1554802965234827\n",
            "test_ind: 3, Epoch: 2389, train_loss: 0.9706540814152469, valid_loss: 0.9612931145562066\n",
            "test_ind: 3, Epoch: 2390, train_loss: 0.9940199910858529, valid_loss: 0.9552456537882487\n",
            "test_ind: 3, Epoch: 2391, train_loss: 0.9963667422165106, valid_loss: 0.862262902436433\n",
            "test_ind: 3, Epoch: 2392, train_loss: 0.9298862233574007, valid_loss: 0.9157660625599049\n",
            "test_ind: 3, Epoch: 2393, train_loss: 0.9423195756511924, valid_loss: 0.8606891102261013\n",
            "test_ind: 3, Epoch: 2394, train_loss: 0.9749592910578222, valid_loss: 0.9516581605981898\n",
            "test_ind: 3, Epoch: 2395, train_loss: 1.0264344862949704, valid_loss: 1.0930921589886702\n",
            "test_ind: 3, Epoch: 2396, train_loss: 0.9640823646827982, valid_loss: 0.9125158875076859\n",
            "test_ind: 3, Epoch: 2397, train_loss: 0.9402624589425547, valid_loss: 1.0667264726426866\n",
            "test_ind: 3, Epoch: 2398, train_loss: 0.9821313928674769, valid_loss: 0.9057223355328596\n",
            "test_ind: 3, Epoch: 2399, train_loss: 0.9391131989749861, valid_loss: 0.9126655084115488\n",
            "test_ind: 3, Epoch: 2400, train_loss: 0.9207676781548395, valid_loss: 0.9359334133289477\n",
            "test_ind: 3, Epoch: 2401, train_loss: 1.0051224084548007, valid_loss: 0.9386947949727377\n",
            "test_ind: 3, Epoch: 2402, train_loss: 0.9734363143826708, valid_loss: 0.9655682069283944\n",
            "test_ind: 3, Epoch: 2403, train_loss: 0.9568790153220845, valid_loss: 0.8804073157133879\n",
            "test_ind: 3, Epoch: 2404, train_loss: 1.016790337032742, valid_loss: 1.0459966836152255\n",
            "test_ind: 3, Epoch: 2405, train_loss: 0.9544888484625169, valid_loss: 0.9903521890993472\n",
            "test_ind: 3, Epoch: 2406, train_loss: 0.9447496496600867, valid_loss: 0.9301029311286078\n",
            "test_ind: 3, Epoch: 2407, train_loss: 0.9379869684760952, valid_loss: 0.8557808310897261\n",
            "test_ind: 3, Epoch: 2408, train_loss: 0.9059350814348387, valid_loss: 0.9813475785432039\n",
            "test_ind: 3, Epoch: 2409, train_loss: 1.032580734770975, valid_loss: 0.9531683745207611\n",
            "test_ind: 3, Epoch: 2410, train_loss: 1.0408190797876429, valid_loss: 0.9393094910515679\n",
            "test_ind: 3, Epoch: 2411, train_loss: 0.9828927252027723, valid_loss: 0.9000719741538719\n",
            "test_ind: 3, Epoch: 2412, train_loss: 0.9224799532949188, valid_loss: 0.8720057981985586\n",
            "test_ind: 3, Epoch: 2413, train_loss: 0.9843312251715012, valid_loss: 1.0334314770168729\n",
            "test_ind: 3, Epoch: 2414, train_loss: 0.9789177517832061, valid_loss: 0.9457645063047057\n",
            "test_ind: 3, Epoch: 2415, train_loss: 1.0636423664328492, valid_loss: 0.9145873740867332\n",
            "test_ind: 3, Epoch: 2416, train_loss: 0.9596673706431447, valid_loss: 0.9170017065825286\n",
            "test_ind: 3, Epoch: 2417, train_loss: 0.961409662976677, valid_loss: 0.9981445559748896\n",
            "test_ind: 3, Epoch: 2418, train_loss: 0.9285494368753315, valid_loss: 0.8470744556850858\n",
            "test_ind: 3, Epoch: 2419, train_loss: 0.945171038309733, valid_loss: 1.0603885120815701\n",
            "test_ind: 3, Epoch: 2420, train_loss: 0.9409709035614391, valid_loss: 0.7877051565382216\n",
            "Validation loss decreased (0.8001420939410175 --> 0.7877051565382216).  Saving model ...\n",
            "test_ind: 3, Epoch: 2421, train_loss: 0.9753832051783432, valid_loss: 1.019146619019685\n",
            "test_ind: 3, Epoch: 2422, train_loss: 0.8738714912791311, valid_loss: 0.8824128751401548\n",
            "test_ind: 3, Epoch: 2423, train_loss: 0.943457214920609, valid_loss: 1.0059485082273132\n",
            "test_ind: 3, Epoch: 2424, train_loss: 0.9206275704466265, valid_loss: 0.8223562947026006\n",
            "test_ind: 3, Epoch: 2425, train_loss: 0.9652331846731681, valid_loss: 0.8779208925035265\n",
            "test_ind: 3, Epoch: 2426, train_loss: 0.9329774232558263, valid_loss: 0.819898004885073\n",
            "test_ind: 3, Epoch: 2427, train_loss: 0.8927996188034245, valid_loss: 0.9003116113168221\n",
            "test_ind: 3, Epoch: 2428, train_loss: 0.9533629064206725, valid_loss: 0.8552478154500326\n",
            "test_ind: 3, Epoch: 2429, train_loss: 0.9838227578151375, valid_loss: 0.9211294739334672\n",
            "test_ind: 3, Epoch: 2430, train_loss: 0.97018739912245, valid_loss: 1.0894214135629159\n",
            "test_ind: 3, Epoch: 2431, train_loss: 0.9888278525552631, valid_loss: 1.1019524998135037\n",
            "test_ind: 3, Epoch: 2432, train_loss: 0.9263310491302867, valid_loss: 0.8744058255796079\n",
            "test_ind: 3, Epoch: 2433, train_loss: 0.9269698284290456, valid_loss: 1.178570005628798\n",
            "test_ind: 3, Epoch: 2434, train_loss: 1.0584518114725747, valid_loss: 1.0578770284299497\n",
            "test_ind: 3, Epoch: 2435, train_loss: 1.0177296179312245, valid_loss: 0.9739366460729528\n",
            "test_ind: 3, Epoch: 2436, train_loss: 0.937803162468804, valid_loss: 0.8808933540626809\n",
            "test_ind: 3, Epoch: 2437, train_loss: 0.9169692110132288, valid_loss: 0.9369063200774016\n",
            "test_ind: 3, Epoch: 2438, train_loss: 0.9995691864578813, valid_loss: 1.309149901072184\n",
            "test_ind: 3, Epoch: 2439, train_loss: 0.9850327409343953, valid_loss: 0.9014047516716852\n",
            "test_ind: 3, Epoch: 2440, train_loss: 0.9169034369197891, valid_loss: 0.8975522076642071\n",
            "test_ind: 3, Epoch: 2441, train_loss: 1.1081557273864746, valid_loss: 1.220695795836272\n",
            "test_ind: 3, Epoch: 2442, train_loss: 0.9406939318150648, valid_loss: 1.099010944366455\n",
            "test_ind: 3, Epoch: 2443, train_loss: 0.8852792374881696, valid_loss: 0.8768264099403664\n",
            "test_ind: 3, Epoch: 2444, train_loss: 0.8949742964756342, valid_loss: 0.8354740849247685\n",
            "test_ind: 3, Epoch: 2445, train_loss: 0.9747465510427216, valid_loss: 1.032339996761746\n",
            "test_ind: 3, Epoch: 2446, train_loss: 0.9781838110935541, valid_loss: 0.9360630247328016\n",
            "test_ind: 3, Epoch: 2447, train_loss: 0.9378994364797334, valid_loss: 1.0563777817620172\n",
            "test_ind: 3, Epoch: 2448, train_loss: 0.9721367741808479, valid_loss: 0.9169899622599285\n",
            "test_ind: 3, Epoch: 2449, train_loss: 0.9523745525030441, valid_loss: 0.8650033738878038\n",
            "test_ind: 3, Epoch: 2450, train_loss: 0.88056841014344, valid_loss: 0.8296923107571073\n",
            "test_ind: 3, Epoch: 2451, train_loss: 1.0136062892866724, valid_loss: 0.9465697959617333\n",
            "test_ind: 3, Epoch: 2452, train_loss: 0.9355277603055223, valid_loss: 0.8992811662179452\n",
            "test_ind: 3, Epoch: 2453, train_loss: 0.9576540994055477, valid_loss: 1.0251753595140247\n",
            "test_ind: 3, Epoch: 2454, train_loss: 1.010215370743363, valid_loss: 1.0986608929104276\n",
            "test_ind: 3, Epoch: 2455, train_loss: 0.9090336222707489, valid_loss: 0.9358781178792317\n",
            "test_ind: 3, Epoch: 2456, train_loss: 0.9562150225227263, valid_loss: 0.852322260538737\n",
            "test_ind: 3, Epoch: 2457, train_loss: 0.931645405145339, valid_loss: 0.9156570081357605\n",
            "test_ind: 3, Epoch: 2458, train_loss: 0.9042027496997221, valid_loss: 0.9884247779846191\n",
            "test_ind: 3, Epoch: 2459, train_loss: 0.9733217616140107, valid_loss: 0.9029343216507523\n",
            "test_ind: 3, Epoch: 2460, train_loss: 0.9212331183162735, valid_loss: 1.009430691047951\n",
            "test_ind: 3, Epoch: 2461, train_loss: 0.9212862532815816, valid_loss: 1.1641498848243996\n",
            "test_ind: 3, Epoch: 2462, train_loss: 0.9549317183317961, valid_loss: 0.8801341410036441\n",
            "test_ind: 3, Epoch: 2463, train_loss: 0.9119310614503459, valid_loss: 0.8963645829094782\n",
            "test_ind: 3, Epoch: 2464, train_loss: 0.9234188456594209, valid_loss: 0.9240189128451878\n",
            "test_ind: 3, Epoch: 2465, train_loss: 1.0804874455487288, valid_loss: 1.0204546010052717\n",
            "test_ind: 3, Epoch: 2466, train_loss: 0.9473943298245653, valid_loss: 0.9629720052083333\n",
            "test_ind: 3, Epoch: 2467, train_loss: 0.9798564616544748, valid_loss: 0.9135439130995008\n",
            "test_ind: 3, Epoch: 2468, train_loss: 0.933134408644688, valid_loss: 0.9377321667141385\n",
            "test_ind: 3, Epoch: 2469, train_loss: 0.9355040773933316, valid_loss: 1.0869580198217321\n",
            "test_ind: 3, Epoch: 2470, train_loss: 0.9244397422413766, valid_loss: 0.9412696096632216\n",
            "test_ind: 3, Epoch: 2471, train_loss: 0.868931058012409, valid_loss: 0.8427441208450883\n",
            "test_ind: 3, Epoch: 2472, train_loss: 0.8951259365788211, valid_loss: 0.9352496641653555\n",
            "test_ind: 3, Epoch: 2473, train_loss: 0.955543694672761, valid_loss: 0.9334823467113353\n",
            "test_ind: 3, Epoch: 2474, train_loss: 1.007968113746172, valid_loss: 1.1779592831929524\n",
            "test_ind: 3, Epoch: 2475, train_loss: 0.9031129766393592, valid_loss: 0.9672061778880932\n",
            "test_ind: 3, Epoch: 2476, train_loss: 0.935010297798816, valid_loss: 0.9501596026950412\n",
            "test_ind: 3, Epoch: 2477, train_loss: 0.9536482964032962, valid_loss: 0.8822917585019712\n",
            "test_ind: 3, Epoch: 2478, train_loss: 0.9008165759804808, valid_loss: 0.9512640988385237\n",
            "test_ind: 3, Epoch: 2479, train_loss: 0.9409011499381359, valid_loss: 0.9350362177248354\n",
            "test_ind: 3, Epoch: 2480, train_loss: 0.9354769977522484, valid_loss: 1.0532177642539695\n",
            "test_ind: 3, Epoch: 2481, train_loss: 0.9781816270616319, valid_loss: 0.9349363998130515\n",
            "test_ind: 3, Epoch: 2482, train_loss: 0.9098668510531202, valid_loss: 0.8263922090883608\n",
            "test_ind: 3, Epoch: 2483, train_loss: 0.9270440560800058, valid_loss: 0.8656444196347837\n",
            "test_ind: 3, Epoch: 2484, train_loss: 0.8748553064134386, valid_loss: 0.7896524711891457\n",
            "test_ind: 3, Epoch: 2485, train_loss: 0.9143356923703794, valid_loss: 0.8683273703963669\n",
            "test_ind: 3, Epoch: 2486, train_loss: 0.9426550158747921, valid_loss: 0.8279973136054146\n",
            "test_ind: 3, Epoch: 2487, train_loss: 0.9472640061084135, valid_loss: 0.8976588779025608\n",
            "test_ind: 3, Epoch: 2488, train_loss: 0.926319110540696, valid_loss: 0.7845501369900174\n",
            "Validation loss decreased (0.7877051565382216 --> 0.7845501369900174).  Saving model ...\n",
            "test_ind: 3, Epoch: 2489, train_loss: 1.0222426873666268, valid_loss: 1.1428410565411604\n",
            "test_ind: 3, Epoch: 2490, train_loss: 0.9965923803823965, valid_loss: 1.0977931729069463\n",
            "test_ind: 3, Epoch: 2491, train_loss: 0.9569817707862383, valid_loss: 1.026105492203324\n",
            "test_ind: 3, Epoch: 2492, train_loss: 0.8915191638616868, valid_loss: 1.32973141140408\n",
            "test_ind: 3, Epoch: 2493, train_loss: 0.9575435143929941, valid_loss: 0.987115400808829\n",
            "test_ind: 3, Epoch: 2494, train_loss: 0.9524617666079676, valid_loss: 0.851981763486509\n",
            "test_ind: 3, Epoch: 2495, train_loss: 0.9192376313386141, valid_loss: 0.9662168290879991\n",
            "test_ind: 3, Epoch: 2496, train_loss: 0.9535794964543096, valid_loss: 0.9221303374679001\n",
            "test_ind: 3, Epoch: 2497, train_loss: 0.928458826041516, valid_loss: 1.0098395700807923\n",
            "test_ind: 3, Epoch: 2498, train_loss: 0.9224362432220837, valid_loss: 1.0364118682013617\n",
            "test_ind: 3, Epoch: 2499, train_loss: 0.9003050886554482, valid_loss: 0.8384135034349229\n",
            "test_ind: 3, Epoch: 2500, train_loss: 0.9089299307929146, valid_loss: 0.8130678777341489\n",
            "test_ind: 3, Epoch: 2501, train_loss: 0.8908550356641227, valid_loss: 0.9652506510416666\n",
            "test_ind: 3, Epoch: 2502, train_loss: 0.8872363655655472, valid_loss: 0.801206041265417\n",
            "test_ind: 3, Epoch: 2503, train_loss: 0.9560436142815484, valid_loss: 0.8814409044053819\n",
            "test_ind: 3, Epoch: 2504, train_loss: 0.9419290872267734, valid_loss: 0.7415154951590079\n",
            "Validation loss decreased (0.7845501369900174 --> 0.7415154951590079).  Saving model ...\n",
            "test_ind: 3, Epoch: 2505, train_loss: 0.9577714131202227, valid_loss: 1.073084001187925\n",
            "test_ind: 3, Epoch: 2506, train_loss: 0.9595212995270154, valid_loss: 0.9609180379796911\n",
            "test_ind: 3, Epoch: 2507, train_loss: 0.8911915060914593, valid_loss: 0.8540413467972366\n",
            "test_ind: 3, Epoch: 2508, train_loss: 1.0230957078345027, valid_loss: 1.007006256668656\n",
            "test_ind: 3, Epoch: 2509, train_loss: 0.9429670027744623, valid_loss: 0.9326069443314164\n",
            "test_ind: 3, Epoch: 2510, train_loss: 0.9340754261723272, valid_loss: 1.221598819450096\n",
            "test_ind: 3, Epoch: 2511, train_loss: 0.9868000760490512, valid_loss: 1.020360310872396\n",
            "test_ind: 3, Epoch: 2512, train_loss: 0.9504746272240157, valid_loss: 0.8932425887496385\n",
            "test_ind: 3, Epoch: 2513, train_loss: 0.9243641017395774, valid_loss: 0.8892961608039007\n",
            "test_ind: 3, Epoch: 2514, train_loss: 0.9101411560435355, valid_loss: 0.8551861445109049\n",
            "test_ind: 3, Epoch: 2515, train_loss: 0.8719343491542487, valid_loss: 0.9240581017953378\n",
            "test_ind: 3, Epoch: 2516, train_loss: 0.9214328130086263, valid_loss: 0.9825530758610479\n",
            "test_ind: 3, Epoch: 2517, train_loss: 0.9417760460465042, valid_loss: 1.1146913281193487\n",
            "test_ind: 3, Epoch: 2518, train_loss: 0.9573950826385876, valid_loss: 1.0449635364391185\n",
            "test_ind: 3, Epoch: 2519, train_loss: 0.9917131824257932, valid_loss: 1.1181780850445784\n",
            "test_ind: 3, Epoch: 2520, train_loss: 0.8952498141630195, valid_loss: 0.9196394461172599\n",
            "test_ind: 3, Epoch: 2521, train_loss: 0.913165639947962, valid_loss: 0.858726872338189\n",
            "test_ind: 3, Epoch: 2522, train_loss: 0.9317907050803855, valid_loss: 1.0891468436629683\n",
            "test_ind: 3, Epoch: 2523, train_loss: 0.8992772749912592, valid_loss: 0.8120672261273418\n",
            "test_ind: 3, Epoch: 2524, train_loss: 0.9383247399035795, valid_loss: 0.9164755609300402\n",
            "test_ind: 3, Epoch: 2525, train_loss: 0.9132722630912875, valid_loss: 0.9179841147528754\n",
            "test_ind: 3, Epoch: 2526, train_loss: 0.9330099364857615, valid_loss: 0.981345777158384\n",
            "test_ind: 3, Epoch: 2527, train_loss: 1.0451122448768146, valid_loss: 1.1634359006528503\n",
            "test_ind: 3, Epoch: 2528, train_loss: 1.0328998389067474, valid_loss: 0.9496693787751376\n",
            "test_ind: 3, Epoch: 2529, train_loss: 0.920591860641668, valid_loss: 0.8027216770030834\n",
            "test_ind: 3, Epoch: 2530, train_loss: 0.9240101590568637, valid_loss: 0.8907314936319988\n",
            "test_ind: 3, Epoch: 2531, train_loss: 0.9209353894363215, valid_loss: 0.8748864421138056\n",
            "test_ind: 3, Epoch: 2532, train_loss: 0.9659612267105666, valid_loss: 0.8876288378680194\n",
            "test_ind: 3, Epoch: 2533, train_loss: 0.8971483265912091, valid_loss: 0.8928814817357946\n",
            "test_ind: 3, Epoch: 2534, train_loss: 0.9121207366754981, valid_loss: 0.8627938871030455\n",
            "test_ind: 3, Epoch: 2535, train_loss: 0.9726597114845559, valid_loss: 1.0785173663386591\n",
            "test_ind: 3, Epoch: 2536, train_loss: 0.9388546708189413, valid_loss: 1.0695372157626681\n",
            "test_ind: 3, Epoch: 2537, train_loss: 0.9722138510810003, valid_loss: 0.9377367938006365\n",
            "test_ind: 3, Epoch: 2538, train_loss: 0.9238375969875006, valid_loss: 0.9529421241195115\n",
            "test_ind: 3, Epoch: 2539, train_loss: 0.9742108274389197, valid_loss: 0.9450623900802047\n",
            "test_ind: 3, Epoch: 2540, train_loss: 0.9942911348225156, valid_loss: 0.9403257546601473\n",
            "test_ind: 3, Epoch: 2541, train_loss: 0.9379824414665316, valid_loss: 0.8120012636537904\n",
            "test_ind: 3, Epoch: 2542, train_loss: 0.9010007940692667, valid_loss: 0.8109835871943721\n",
            "test_ind: 3, Epoch: 2543, train_loss: 0.8928385192965284, valid_loss: 0.7981098316333912\n",
            "test_ind: 3, Epoch: 2544, train_loss: 0.9377647976816437, valid_loss: 1.0916422561362937\n",
            "test_ind: 3, Epoch: 2545, train_loss: 0.9339797643967616, valid_loss: 0.8905415181760435\n",
            "test_ind: 3, Epoch: 2546, train_loss: 0.9423551382841886, valid_loss: 0.8200123928211354\n",
            "test_ind: 3, Epoch: 2547, train_loss: 0.8778166417722348, valid_loss: 0.7915920504817255\n",
            "test_ind: 3, Epoch: 2548, train_loss: 0.9137766861621244, valid_loss: 0.8524569405449761\n",
            "test_ind: 3, Epoch: 2549, train_loss: 1.0040437910291884, valid_loss: 0.8653604012948495\n",
            "test_ind: 3, Epoch: 2550, train_loss: 0.9156450459986556, valid_loss: 0.7716484599643284\n",
            "test_ind: 3, Epoch: 2551, train_loss: 0.8837876319885253, valid_loss: 0.7376584830107512\n",
            "Validation loss decreased (0.7415154951590079 --> 0.7376584830107512).  Saving model ...\n",
            "test_ind: 3, Epoch: 2552, train_loss: 0.9133942509874884, valid_loss: 0.9655633502536349\n",
            "test_ind: 3, Epoch: 2553, train_loss: 0.9426520606617869, valid_loss: 0.8470996574119285\n",
            "test_ind: 3, Epoch: 2554, train_loss: 0.9158758822782541, valid_loss: 0.9767482545640733\n",
            "test_ind: 3, Epoch: 2555, train_loss: 0.9328127083954987, valid_loss: 1.1089040085121438\n",
            "test_ind: 3, Epoch: 2556, train_loss: 0.858106153982657, valid_loss: 0.8670503475047924\n",
            "test_ind: 3, Epoch: 2557, train_loss: 0.9173672876240294, valid_loss: 0.8643201015613697\n",
            "test_ind: 3, Epoch: 2558, train_loss: 0.8901297663464958, valid_loss: 1.08295977557147\n",
            "test_ind: 3, Epoch: 2559, train_loss: 1.0304103898413386, valid_loss: 0.9406734749122901\n",
            "test_ind: 3, Epoch: 2560, train_loss: 0.8798054177084088, valid_loss: 0.9510630854853876\n",
            "test_ind: 3, Epoch: 2561, train_loss: 0.9666994059527362, valid_loss: 0.8871652285257975\n",
            "test_ind: 3, Epoch: 2562, train_loss: 0.8805799543121713, valid_loss: 0.8491302066379123\n",
            "test_ind: 3, Epoch: 2563, train_loss: 0.8457570075988768, valid_loss: 0.8629680209689671\n",
            "test_ind: 3, Epoch: 2564, train_loss: 0.9185265376244063, valid_loss: 1.0584517584906683\n",
            "test_ind: 3, Epoch: 2565, train_loss: 0.9476518984194153, valid_loss: 0.9368362603364168\n",
            "test_ind: 3, Epoch: 2566, train_loss: 0.8935768221631462, valid_loss: 0.8848095470004612\n",
            "test_ind: 3, Epoch: 2567, train_loss: 0.8845530498174974, valid_loss: 0.791051882284659\n",
            "test_ind: 3, Epoch: 2568, train_loss: 0.8512327582747847, valid_loss: 0.9156600987469707\n",
            "test_ind: 3, Epoch: 2569, train_loss: 0.9171838642638406, valid_loss: 0.8988835016886393\n",
            "test_ind: 3, Epoch: 2570, train_loss: 0.8644191188576781, valid_loss: 0.8732087523848922\n",
            "test_ind: 3, Epoch: 2571, train_loss: 0.8871686546890825, valid_loss: 0.8674388814855505\n",
            "test_ind: 3, Epoch: 2572, train_loss: 0.9282426657500091, valid_loss: 0.7928919262356228\n",
            "test_ind: 3, Epoch: 2573, train_loss: 0.9177702915521314, valid_loss: 1.0730725217748571\n",
            "test_ind: 3, Epoch: 2574, train_loss: 0.9340351716971693, valid_loss: 0.9193813535902235\n",
            "test_ind: 3, Epoch: 2575, train_loss: 0.8729749137972608, valid_loss: 0.9074754538359464\n",
            "test_ind: 3, Epoch: 2576, train_loss: 0.9430322235013233, valid_loss: 0.8667636624088993\n",
            "test_ind: 3, Epoch: 2577, train_loss: 0.925518818843512, valid_loss: 0.9496486628497088\n",
            "test_ind: 3, Epoch: 2578, train_loss: 0.8778716134436337, valid_loss: 0.8028925436514396\n",
            "test_ind: 3, Epoch: 2579, train_loss: 0.8560118851838288, valid_loss: 1.060309957574915\n",
            "test_ind: 3, Epoch: 2580, train_loss: 0.9502407121069637, valid_loss: 0.899029431519685\n",
            "test_ind: 3, Epoch: 2581, train_loss: 0.9172460885695469, valid_loss: 0.9737866366351091\n",
            "test_ind: 3, Epoch: 2582, train_loss: 0.8782164432384351, valid_loss: 0.9163991433602792\n",
            "test_ind: 3, Epoch: 2583, train_loss: 0.9412615328659246, valid_loss: 0.8041594999807853\n",
            "test_ind: 3, Epoch: 2584, train_loss: 0.8706965269865812, valid_loss: 0.7707565802114982\n",
            "test_ind: 3, Epoch: 2585, train_loss: 0.9538333151075575, valid_loss: 0.7589260737101237\n",
            "test_ind: 3, Epoch: 2586, train_loss: 0.8754035160865313, valid_loss: 0.8736291461520724\n",
            "test_ind: 3, Epoch: 2587, train_loss: 0.9293463318436234, valid_loss: 0.7897664352699563\n",
            "test_ind: 3, Epoch: 2588, train_loss: 0.8895368576049805, valid_loss: 0.9039543469746908\n",
            "test_ind: 3, Epoch: 2589, train_loss: 0.8972505581231764, valid_loss: 0.883331298828125\n",
            "test_ind: 3, Epoch: 2590, train_loss: 0.9700779914855957, valid_loss: 0.9432857478106463\n",
            "test_ind: 3, Epoch: 2591, train_loss: 0.8629099351388437, valid_loss: 0.8458432974638763\n",
            "test_ind: 3, Epoch: 2592, train_loss: 0.9468904306859146, valid_loss: 0.8800435772648564\n",
            "test_ind: 3, Epoch: 2593, train_loss: 0.861980073245955, valid_loss: 0.7997370296054416\n",
            "test_ind: 3, Epoch: 2594, train_loss: 0.8764523105856811, valid_loss: 0.9704594965334292\n",
            "test_ind: 3, Epoch: 2595, train_loss: 0.9239549460234464, valid_loss: 0.8514675211023401\n",
            "test_ind: 3, Epoch: 2596, train_loss: 0.9548964264952107, valid_loss: 0.8798657876473885\n",
            "test_ind: 3, Epoch: 2597, train_loss: 0.930947368527636, valid_loss: 1.1440291228117765\n",
            "test_ind: 3, Epoch: 2598, train_loss: 0.8618417963569547, valid_loss: 0.8997672222278736\n",
            "test_ind: 3, Epoch: 2599, train_loss: 0.8971305541050286, valid_loss: 0.9509969817267523\n",
            "test_ind: 3, Epoch: 2600, train_loss: 0.8712913842848791, valid_loss: 0.7500476660551848\n",
            "test_ind: 3, Epoch: 2601, train_loss: 0.8615172291979378, valid_loss: 0.8572734196980795\n",
            "test_ind: 3, Epoch: 2602, train_loss: 0.8935250529536495, valid_loss: 0.8622521647700556\n",
            "test_ind: 3, Epoch: 2603, train_loss: 0.9724078531618472, valid_loss: 0.8381351718196162\n",
            "test_ind: 3, Epoch: 2604, train_loss: 0.8855734754491734, valid_loss: 0.8091775929486309\n",
            "test_ind: 3, Epoch: 2605, train_loss: 0.8914837307400172, valid_loss: 0.8923289864151566\n",
            "test_ind: 3, Epoch: 2606, train_loss: 0.9114954501022527, valid_loss: 0.882437511726662\n",
            "test_ind: 3, Epoch: 2607, train_loss: 0.8888332520002201, valid_loss: 0.9238424654360171\n",
            "test_ind: 3, Epoch: 2608, train_loss: 0.8922483773879064, valid_loss: 0.8362895117865667\n",
            "test_ind: 3, Epoch: 2609, train_loss: 0.8875293143001604, valid_loss: 1.009159105795401\n",
            "test_ind: 3, Epoch: 2610, train_loss: 0.9112393355663913, valid_loss: 0.8893336013511375\n",
            "test_ind: 3, Epoch: 2611, train_loss: 0.9090613200340742, valid_loss: 0.9276654632003218\n",
            "test_ind: 3, Epoch: 2612, train_loss: 0.8763142161899143, valid_loss: 0.8508101039462619\n",
            "test_ind: 3, Epoch: 2613, train_loss: 0.9029845661587184, valid_loss: 0.8975814006946704\n",
            "test_ind: 3, Epoch: 2614, train_loss: 0.9126383345804097, valid_loss: 0.7916148503621419\n",
            "test_ind: 3, Epoch: 2615, train_loss: 0.9385857935304995, valid_loss: 0.9062552981906467\n",
            "test_ind: 3, Epoch: 2616, train_loss: 0.9504548826335388, valid_loss: 0.9468749894036187\n",
            "test_ind: 3, Epoch: 2617, train_loss: 0.9019652472601998, valid_loss: 0.953591364401358\n",
            "test_ind: 3, Epoch: 2618, train_loss: 0.9267282839174623, valid_loss: 0.9754515224032931\n",
            "test_ind: 3, Epoch: 2619, train_loss: 0.9213160467736515, valid_loss: 0.8330445996037237\n",
            "test_ind: 3, Epoch: 2620, train_loss: 0.9193089861928682, valid_loss: 0.7910719447665744\n",
            "test_ind: 3, Epoch: 2621, train_loss: 0.8898322552810479, valid_loss: 0.9721437913400156\n",
            "test_ind: 3, Epoch: 2622, train_loss: 0.9025368808228293, valid_loss: 0.8714174871091489\n",
            "test_ind: 3, Epoch: 2623, train_loss: 0.8756903071462372, valid_loss: 0.8100847491511592\n",
            "test_ind: 3, Epoch: 2624, train_loss: 0.8882568500660083, valid_loss: 0.919300291273329\n",
            "test_ind: 3, Epoch: 2625, train_loss: 0.9302643552238559, valid_loss: 0.8411481821978535\n",
            "test_ind: 3, Epoch: 2626, train_loss: 0.9105565165296011, valid_loss: 0.9015051170631692\n",
            "test_ind: 3, Epoch: 2627, train_loss: 0.8772015041775174, valid_loss: 0.8207987326162832\n",
            "test_ind: 3, Epoch: 2628, train_loss: 0.8257637906957556, valid_loss: 0.895919058057997\n",
            "test_ind: 3, Epoch: 2629, train_loss: 0.958194026240596, valid_loss: 0.9115393956502278\n",
            "test_ind: 3, Epoch: 2630, train_loss: 0.8954897103486238, valid_loss: 1.1096981543081776\n",
            "test_ind: 3, Epoch: 2631, train_loss: 0.8598715935224368, valid_loss: 0.9275068177117243\n",
            "test_ind: 3, Epoch: 2632, train_loss: 0.8849344194671254, valid_loss: 0.8671613622594763\n",
            "test_ind: 3, Epoch: 2633, train_loss: 0.9581389368316274, valid_loss: 0.9884841000592268\n",
            "test_ind: 3, Epoch: 2634, train_loss: 0.8662682521490404, valid_loss: 0.7679773083439579\n",
            "test_ind: 3, Epoch: 2635, train_loss: 0.9331294224586015, valid_loss: 0.8510624214454934\n",
            "test_ind: 3, Epoch: 2636, train_loss: 0.9027940432230631, valid_loss: 0.8658026412681297\n",
            "test_ind: 3, Epoch: 2637, train_loss: 0.8587770167692208, valid_loss: 1.028926213582357\n",
            "test_ind: 3, Epoch: 2638, train_loss: 0.8487503440291791, valid_loss: 1.0254584595009133\n",
            "test_ind: 3, Epoch: 2639, train_loss: 0.8691374460856119, valid_loss: 0.916090347148754\n",
            "test_ind: 3, Epoch: 2640, train_loss: 0.9386757744683161, valid_loss: 0.8899443591082538\n",
            "test_ind: 3, Epoch: 2641, train_loss: 0.9305601826420536, valid_loss: 0.8238056853965476\n",
            "test_ind: 3, Epoch: 2642, train_loss: 0.8587344840720847, valid_loss: 1.034600399158619\n",
            "test_ind: 3, Epoch: 2643, train_loss: 0.9607549184634361, valid_loss: 0.9678637716505262\n",
            "test_ind: 3, Epoch: 2644, train_loss: 0.8933283664562084, valid_loss: 1.0198372381704826\n",
            "test_ind: 3, Epoch: 2645, train_loss: 0.9040859304828409, valid_loss: 0.931916766696506\n",
            "test_ind: 3, Epoch: 2646, train_loss: 0.910607514558015, valid_loss: 0.940148494861744\n",
            "test_ind: 3, Epoch: 2647, train_loss: 0.9068799489810144, valid_loss: 0.948643066264965\n",
            "test_ind: 3, Epoch: 2648, train_loss: 0.9199718075034059, valid_loss: 0.922401852077908\n",
            "test_ind: 3, Epoch: 2649, train_loss: 0.9069882498847113, valid_loss: 0.9516184241683394\n",
            "test_ind: 3, Epoch: 2650, train_loss: 0.9538853845478575, valid_loss: 1.023295420187491\n",
            "test_ind: 3, Epoch: 2651, train_loss: 0.900599409032751, valid_loss: 0.8207282136987757\n",
            "test_ind: 3, Epoch: 2652, train_loss: 0.8566011440606764, valid_loss: 0.8390265394140173\n",
            "test_ind: 3, Epoch: 2653, train_loss: 0.8814002260749723, valid_loss: 0.9686612729673032\n",
            "test_ind: 3, Epoch: 2654, train_loss: 0.8858537673950195, valid_loss: 1.060268207832619\n",
            "test_ind: 3, Epoch: 2655, train_loss: 0.9078317983650868, valid_loss: 0.8763886734291358\n",
            "test_ind: 3, Epoch: 2656, train_loss: 0.8956793561393832, valid_loss: 0.8830807120711716\n",
            "test_ind: 3, Epoch: 2657, train_loss: 0.8735453346629202, valid_loss: 0.8871182512353968\n",
            "test_ind: 3, Epoch: 2658, train_loss: 1.0026835335625541, valid_loss: 0.9017601013183594\n",
            "test_ind: 3, Epoch: 2659, train_loss: 0.9398898489681291, valid_loss: 0.9623093781647859\n",
            "test_ind: 3, Epoch: 2660, train_loss: 0.9390227882950395, valid_loss: 0.8193067268089012\n",
            "test_ind: 3, Epoch: 2661, train_loss: 0.8880115261784306, valid_loss: 0.8497703516924823\n",
            "test_ind: 3, Epoch: 2662, train_loss: 0.8221478521088024, valid_loss: 0.8230233898869267\n",
            "test_ind: 3, Epoch: 2663, train_loss: 0.8645582905522098, valid_loss: 0.8967730027657967\n",
            "test_ind: 3, Epoch: 2664, train_loss: 0.9181406998339995, valid_loss: 1.0443882059167933\n",
            "test_ind: 3, Epoch: 2665, train_loss: 0.8668385317296158, valid_loss: 0.7993327599984628\n",
            "test_ind: 3, Epoch: 2666, train_loss: 0.8735624654793445, valid_loss: 0.8814643224080404\n",
            "test_ind: 3, Epoch: 2667, train_loss: 0.8692743866531938, valid_loss: 0.7757504957693595\n",
            "test_ind: 3, Epoch: 2668, train_loss: 0.848506815639543, valid_loss: 0.8531089712072303\n",
            "test_ind: 3, Epoch: 2669, train_loss: 0.8509264934210131, valid_loss: 0.848978183887623\n",
            "test_ind: 3, Epoch: 2670, train_loss: 0.8868578451651115, valid_loss: 0.9758066248010706\n",
            "test_ind: 3, Epoch: 2671, train_loss: 0.9039941010651765, valid_loss: 0.9284530392399539\n",
            "test_ind: 3, Epoch: 2672, train_loss: 0.8909931418336469, valid_loss: 0.9670112751148365\n",
            "test_ind: 3, Epoch: 2673, train_loss: 0.8995060332027484, valid_loss: 0.8835822917796947\n",
            "test_ind: 3, Epoch: 2674, train_loss: 0.8901934329374338, valid_loss: 1.1443587055912725\n",
            "test_ind: 3, Epoch: 2675, train_loss: 0.9289547955548322, valid_loss: 0.898537900712755\n",
            "test_ind: 3, Epoch: 2676, train_loss: 0.8494165444079741, valid_loss: 0.8850494843942147\n",
            "test_ind: 3, Epoch: 2677, train_loss: 0.8616749327859761, valid_loss: 0.9673582359596535\n",
            "test_ind: 3, Epoch: 2678, train_loss: 0.9678980921521599, valid_loss: 1.0832479794820151\n",
            "test_ind: 3, Epoch: 2679, train_loss: 0.8684839260430983, valid_loss: 0.8927410267017506\n",
            "test_ind: 3, Epoch: 2680, train_loss: 0.8661630241959183, valid_loss: 0.7392998977943702\n",
            "test_ind: 3, Epoch: 2681, train_loss: 0.8705776767966188, valid_loss: 0.8309866587320964\n",
            "test_ind: 3, Epoch: 2682, train_loss: 0.8556096818712021, valid_loss: 0.9807092878553603\n",
            "test_ind: 3, Epoch: 2683, train_loss: 0.9897653850508324, valid_loss: 1.0747360123528373\n",
            "test_ind: 3, Epoch: 2684, train_loss: 0.9416218981330778, valid_loss: 1.1040310153254758\n",
            "test_ind: 3, Epoch: 2685, train_loss: 0.8854748702343599, valid_loss: 1.0359176176565665\n",
            "test_ind: 3, Epoch: 2686, train_loss: 0.9341234689877358, valid_loss: 1.0352789914166487\n",
            "test_ind: 3, Epoch: 2687, train_loss: 0.8270657916127899, valid_loss: 0.7876425495854129\n",
            "test_ind: 3, Epoch: 2688, train_loss: 0.8370842403835721, valid_loss: 0.8492548907244648\n",
            "test_ind: 3, Epoch: 2689, train_loss: 0.9380418224099242, valid_loss: 0.8537254510102448\n",
            "test_ind: 3, Epoch: 2690, train_loss: 0.8870756125744478, valid_loss: 0.8546767588014956\n",
            "test_ind: 3, Epoch: 2691, train_loss: 0.8371190848173917, valid_loss: 0.9282253936485008\n",
            "test_ind: 3, Epoch: 2692, train_loss: 0.962248855166965, valid_loss: 0.9345071404068559\n",
            "test_ind: 3, Epoch: 2693, train_loss: 0.8599401579962836, valid_loss: 0.7690835175690827\n",
            "test_ind: 3, Epoch: 2694, train_loss: 0.874062826604019, valid_loss: 0.8860370847913954\n",
            "test_ind: 3, Epoch: 2695, train_loss: 0.9046032987994912, valid_loss: 1.041622832969383\n",
            "test_ind: 3, Epoch: 2696, train_loss: 0.8898703316111622, valid_loss: 0.938523945985017\n",
            "test_ind: 3, Epoch: 2697, train_loss: 0.906468962445671, valid_loss: 0.8247349880359791\n",
            "test_ind: 3, Epoch: 2698, train_loss: 0.8329457235924991, valid_loss: 0.8103929095798069\n",
            "test_ind: 3, Epoch: 2699, train_loss: 0.8509139720304513, valid_loss: 1.0202220280965169\n",
            "test_ind: 3, Epoch: 2700, train_loss: 0.8488330958802022, valid_loss: 0.7912294776351364\n",
            "test_ind: 3, Epoch: 2701, train_loss: 0.8991008864508734, valid_loss: 1.0328182291101526\n",
            "test_ind: 3, Epoch: 2702, train_loss: 0.8757819952788175, valid_loss: 0.9410690025047019\n",
            "test_ind: 3, Epoch: 2703, train_loss: 0.943978374387011, valid_loss: 0.9519815798159\n",
            "test_ind: 3, Epoch: 2704, train_loss: 0.8717479234860266, valid_loss: 0.7218538213659216\n",
            "Validation loss decreased (0.7376584830107512 --> 0.7218538213659216).  Saving model ...\n",
            "test_ind: 3, Epoch: 2705, train_loss: 0.851187723654288, valid_loss: 1.1010729295236092\n",
            "test_ind: 3, Epoch: 2706, train_loss: 0.8561013068681881, valid_loss: 0.871142793584753\n",
            "test_ind: 3, Epoch: 2707, train_loss: 0.9086728508089795, valid_loss: 1.0753543818438493\n",
            "test_ind: 3, Epoch: 2708, train_loss: 0.9097668447612245, valid_loss: 0.9691235401012278\n",
            "test_ind: 3, Epoch: 2709, train_loss: 0.8522662763242368, valid_loss: 0.7676046865957755\n",
            "test_ind: 3, Epoch: 2710, train_loss: 0.9122774453810704, valid_loss: 0.8797337567364728\n",
            "test_ind: 3, Epoch: 2711, train_loss: 0.8643091048723385, valid_loss: 0.9625856788070113\n",
            "test_ind: 3, Epoch: 2712, train_loss: 0.9135489404937365, valid_loss: 0.8776073455810547\n",
            "test_ind: 3, Epoch: 2713, train_loss: 0.8720782421253346, valid_loss: 0.8507615018773962\n",
            "test_ind: 3, Epoch: 2714, train_loss: 0.8175888002654653, valid_loss: 0.742414704075566\n",
            "test_ind: 3, Epoch: 2715, train_loss: 0.845072811032519, valid_loss: 0.9971758171364113\n",
            "test_ind: 3, Epoch: 2716, train_loss: 0.8297580200948832, valid_loss: 1.011068202831127\n",
            "test_ind: 3, Epoch: 2717, train_loss: 0.8948916447015456, valid_loss: 0.9128210986102068\n",
            "test_ind: 3, Epoch: 2718, train_loss: 0.8671364843109508, valid_loss: 0.8714234387433089\n",
            "test_ind: 3, Epoch: 2719, train_loss: 0.8434639860082556, valid_loss: 0.8207961188422309\n",
            "test_ind: 3, Epoch: 2720, train_loss: 0.9922131726771226, valid_loss: 0.9752371399490922\n",
            "test_ind: 3, Epoch: 2721, train_loss: 0.8630052554754563, valid_loss: 0.8599606796547219\n",
            "test_ind: 3, Epoch: 2722, train_loss: 0.9268095522751042, valid_loss: 0.9513819835804127\n",
            "test_ind: 3, Epoch: 2723, train_loss: 0.9476516629442757, valid_loss: 1.1759400014524108\n",
            "test_ind: 3, Epoch: 2724, train_loss: 0.8598243748700177, valid_loss: 0.9513632103248878\n",
            "test_ind: 3, Epoch: 2725, train_loss: 0.8235249990298426, valid_loss: 0.8623619786015262\n",
            "test_ind: 3, Epoch: 2726, train_loss: 0.8336207307415243, valid_loss: 0.8523108164469402\n",
            "test_ind: 3, Epoch: 2727, train_loss: 0.8929496400150251, valid_loss: 0.8646320237053766\n",
            "test_ind: 3, Epoch: 2728, train_loss: 0.9279970828397774, valid_loss: 0.8097346976951316\n",
            "test_ind: 3, Epoch: 2729, train_loss: 1.0012790185433844, valid_loss: 1.2509095403883193\n",
            "test_ind: 3, Epoch: 2730, train_loss: 0.8745754971916293, valid_loss: 0.789723449283176\n",
            "test_ind: 3, Epoch: 2731, train_loss: 0.8581792454660673, valid_loss: 0.7970208591885037\n",
            "test_ind: 3, Epoch: 2732, train_loss: 0.916462085865162, valid_loss: 0.8074370136967411\n",
            "test_ind: 3, Epoch: 2733, train_loss: 0.8717910036628629, valid_loss: 0.7392684971844709\n",
            "test_ind: 3, Epoch: 2734, train_loss: 0.9065276899455506, valid_loss: 1.081004637259024\n",
            "test_ind: 3, Epoch: 2735, train_loss: 0.8749037083284353, valid_loss: 0.9621370456836842\n",
            "test_ind: 3, Epoch: 2736, train_loss: 0.8497701986336412, valid_loss: 0.8380345591792353\n",
            "test_ind: 3, Epoch: 2737, train_loss: 0.8821675335919417, valid_loss: 0.9373622823644567\n",
            "test_ind: 3, Epoch: 2738, train_loss: 0.9556589244324484, valid_loss: 0.937724060482449\n",
            "test_ind: 3, Epoch: 2739, train_loss: 0.9436495156935705, valid_loss: 0.7955501344468858\n",
            "test_ind: 3, Epoch: 2740, train_loss: 0.9261072535573701, valid_loss: 0.9370360021237973\n",
            "test_ind: 3, Epoch: 2741, train_loss: 0.8802476871160814, valid_loss: 0.8055776666711878\n",
            "test_ind: 3, Epoch: 2742, train_loss: 0.8448063473642609, valid_loss: 0.9969306521945529\n",
            "test_ind: 3, Epoch: 2743, train_loss: 0.8589864895667558, valid_loss: 0.9171181431523076\n",
            "test_ind: 3, Epoch: 2744, train_loss: 0.8921628174958406, valid_loss: 0.9127867486741807\n",
            "test_ind: 3, Epoch: 2745, train_loss: 0.8932989320637268, valid_loss: 0.8292969067891439\n",
            "test_ind: 3, Epoch: 2746, train_loss: 0.8362675478428969, valid_loss: 0.9293793219107169\n",
            "test_ind: 3, Epoch: 2747, train_loss: 0.8421554624298473, valid_loss: 0.7083527776930068\n",
            "Validation loss decreased (0.7218538213659216 --> 0.7083527776930068).  Saving model ...\n",
            "test_ind: 3, Epoch: 2748, train_loss: 0.888569278481566, valid_loss: 0.857176109596535\n",
            "test_ind: 3, Epoch: 2749, train_loss: 0.8491368176024636, valid_loss: 0.8942005369398329\n",
            "test_ind: 3, Epoch: 2750, train_loss: 0.9817950460645888, valid_loss: 1.0311227904425726\n",
            "test_ind: 3, Epoch: 2751, train_loss: 0.8503389770602002, valid_loss: 0.7360930442810059\n",
            "test_ind: 3, Epoch: 2752, train_loss: 0.8301232008286464, valid_loss: 0.9059880397937916\n",
            "test_ind: 3, Epoch: 2753, train_loss: 0.8613967012476039, valid_loss: 0.9261914889017742\n",
            "test_ind: 3, Epoch: 2754, train_loss: 0.8514814847781333, valid_loss: 0.8378285831875273\n",
            "test_ind: 3, Epoch: 2755, train_loss: 0.8813138008117676, valid_loss: 0.8173652225070529\n",
            "test_ind: 3, Epoch: 2756, train_loss: 0.8592156539728613, valid_loss: 0.7996723033763744\n",
            "test_ind: 3, Epoch: 2757, train_loss: 0.8441819732571827, valid_loss: 0.9695093366834853\n",
            "test_ind: 3, Epoch: 2758, train_loss: 0.8630584787439417, valid_loss: 0.8064824210272895\n",
            "test_ind: 3, Epoch: 2759, train_loss: 0.8358162597373681, valid_loss: 0.8069679118968822\n",
            "test_ind: 3, Epoch: 2760, train_loss: 0.8613663190676841, valid_loss: 0.9371957072505245\n",
            "test_ind: 3, Epoch: 2761, train_loss: 0.8428174124823676, valid_loss: 1.025689584237558\n",
            "test_ind: 3, Epoch: 2762, train_loss: 0.8589433269736207, valid_loss: 0.9404518162762677\n",
            "test_ind: 3, Epoch: 2763, train_loss: 0.8352482348312567, valid_loss: 0.7673031312448007\n",
            "test_ind: 3, Epoch: 2764, train_loss: 0.9088620904051228, valid_loss: 0.9746227087797942\n",
            "test_ind: 3, Epoch: 2765, train_loss: 0.8612458146648644, valid_loss: 0.898041530891701\n",
            "test_ind: 3, Epoch: 2766, train_loss: 0.8975119826234417, valid_loss: 0.9024471883420591\n",
            "test_ind: 3, Epoch: 2767, train_loss: 0.8106186125013563, valid_loss: 0.807772918983742\n",
            "test_ind: 3, Epoch: 2768, train_loss: 0.811228722701838, valid_loss: 0.7562653047067147\n",
            "test_ind: 3, Epoch: 2769, train_loss: 0.8358110321892631, valid_loss: 0.7522268648500796\n",
            "test_ind: 3, Epoch: 2770, train_loss: 0.882097703439218, valid_loss: 0.8659958839416504\n",
            "test_ind: 3, Epoch: 2771, train_loss: 0.8252786530388727, valid_loss: 0.7087419474566424\n",
            "test_ind: 3, Epoch: 2772, train_loss: 0.8287122632250374, valid_loss: 0.9069502265365035\n",
            "test_ind: 3, Epoch: 2773, train_loss: 0.8357508211960027, valid_loss: 0.936223136054145\n",
            "test_ind: 3, Epoch: 2774, train_loss: 0.9118629797005359, valid_loss: 1.0949807167053223\n",
            "test_ind: 3, Epoch: 2775, train_loss: 0.8388911353217231, valid_loss: 0.8728075557284884\n",
            "test_ind: 3, Epoch: 2776, train_loss: 0.848588619703128, valid_loss: 0.9098224110073514\n",
            "test_ind: 3, Epoch: 2777, train_loss: 0.9175692723121173, valid_loss: 0.8142936671221698\n",
            "test_ind: 3, Epoch: 2778, train_loss: 0.8775963547789021, valid_loss: 0.7977906156469274\n",
            "test_ind: 3, Epoch: 2779, train_loss: 0.8540803768016674, valid_loss: 0.7712739838494195\n",
            "test_ind: 3, Epoch: 2780, train_loss: 0.8287828822194795, valid_loss: 1.0115697825396504\n",
            "test_ind: 3, Epoch: 2781, train_loss: 0.9042260323041751, valid_loss: 0.8538901540968152\n",
            "test_ind: 3, Epoch: 2782, train_loss: 0.8655418466638636, valid_loss: 0.8549286347848397\n",
            "test_ind: 3, Epoch: 2783, train_loss: 0.8614729480978883, valid_loss: 0.9633315580862539\n",
            "test_ind: 3, Epoch: 2784, train_loss: 0.8754529658659004, valid_loss: 0.9058330323961046\n",
            "test_ind: 3, Epoch: 2785, train_loss: 0.9528070320317775, valid_loss: 1.1079657342698839\n",
            "test_ind: 3, Epoch: 2786, train_loss: 0.8548319840136869, valid_loss: 0.9131757065101906\n",
            "test_ind: 3, Epoch: 2787, train_loss: 0.8087606312316141, valid_loss: 0.9244937896728516\n",
            "test_ind: 3, Epoch: 2788, train_loss: 0.8411663373311361, valid_loss: 0.8681211824770327\n",
            "test_ind: 3, Epoch: 2789, train_loss: 0.8263484872417686, valid_loss: 0.7308026419745551\n",
            "test_ind: 3, Epoch: 2790, train_loss: 0.8117534672772444, valid_loss: 1.0394878564057528\n",
            "test_ind: 3, Epoch: 2791, train_loss: 0.8575926710058143, valid_loss: 0.8628844331811976\n",
            "test_ind: 3, Epoch: 2792, train_loss: 0.9307214419047039, valid_loss: 0.8268788655598959\n",
            "test_ind: 3, Epoch: 2793, train_loss: 0.8248122179949725, valid_loss: 0.9397711224026151\n",
            "test_ind: 3, Epoch: 2794, train_loss: 0.8508533018606681, valid_loss: 0.8020642775076408\n",
            "test_ind: 3, Epoch: 2795, train_loss: 0.7800232157295132, valid_loss: 0.7273718516031901\n",
            "test_ind: 3, Epoch: 2796, train_loss: 0.883490968633581, valid_loss: 0.7810173387880679\n",
            "test_ind: 3, Epoch: 2797, train_loss: 0.8811715090716328, valid_loss: 0.7781613844412344\n",
            "test_ind: 3, Epoch: 2798, train_loss: 0.84966829676687, valid_loss: 0.8741584883795844\n",
            "test_ind: 3, Epoch: 2799, train_loss: 0.8352327052457834, valid_loss: 0.6882221610457809\n",
            "Validation loss decreased (0.7083527776930068 --> 0.6882221610457809).  Saving model ...\n",
            "test_ind: 3, Epoch: 2800, train_loss: 0.8212299111448688, valid_loss: 0.7090354318971988\n",
            "test_ind: 3, Epoch: 2801, train_loss: 0.8368369149573056, valid_loss: 0.7192948482654712\n",
            "test_ind: 3, Epoch: 2802, train_loss: 0.839313554175106, valid_loss: 0.8283841874864366\n",
            "test_ind: 3, Epoch: 2803, train_loss: 0.9040167949817798, valid_loss: 0.8512433193348072\n",
            "test_ind: 3, Epoch: 2804, train_loss: 0.8321342232786579, valid_loss: 0.8407975126195838\n",
            "test_ind: 3, Epoch: 2805, train_loss: 0.8232483275142717, valid_loss: 0.8430708426016349\n",
            "test_ind: 3, Epoch: 2806, train_loss: 0.8460893748719015, valid_loss: 0.824656504171866\n",
            "test_ind: 3, Epoch: 2807, train_loss: 0.884304929662634, valid_loss: 0.8952115730003074\n",
            "test_ind: 3, Epoch: 2808, train_loss: 0.8929409745298788, valid_loss: 0.8541916034839772\n",
            "test_ind: 3, Epoch: 2809, train_loss: 0.8390222478795937, valid_loss: 0.8515657672175654\n",
            "test_ind: 3, Epoch: 2810, train_loss: 0.8609227663204992, valid_loss: 0.993033868295175\n",
            "test_ind: 3, Epoch: 2811, train_loss: 0.8313480542029863, valid_loss: 0.7907852243494105\n",
            "test_ind: 3, Epoch: 2812, train_loss: 0.9663361266807274, valid_loss: 0.8700745547259294\n",
            "test_ind: 3, Epoch: 2813, train_loss: 0.831705570220947, valid_loss: 0.8576302528381348\n",
            "test_ind: 3, Epoch: 2814, train_loss: 0.8137784004211426, valid_loss: 0.7681484752231175\n",
            "test_ind: 3, Epoch: 2815, train_loss: 0.8836031372164501, valid_loss: 0.890853104767976\n",
            "test_ind: 3, Epoch: 2816, train_loss: 0.8340420369748716, valid_loss: 0.8368721891332556\n",
            "test_ind: 3, Epoch: 2817, train_loss: 0.883030809002158, valid_loss: 0.9876465620817962\n",
            "test_ind: 3, Epoch: 2818, train_loss: 0.8252313343095189, valid_loss: 0.8699890419288919\n",
            "test_ind: 3, Epoch: 2819, train_loss: 0.8584826080887406, valid_loss: 0.8985992890817148\n",
            "test_ind: 3, Epoch: 2820, train_loss: 0.8061559641802752, valid_loss: 0.7936027314927843\n",
            "test_ind: 3, Epoch: 2821, train_loss: 0.8729936811659071, valid_loss: 1.0146406314991139\n",
            "test_ind: 3, Epoch: 2822, train_loss: 0.8541052312026789, valid_loss: 0.8166685810795536\n",
            "test_ind: 3, Epoch: 2823, train_loss: 0.8157939439938392, valid_loss: 0.8227496853581181\n",
            "test_ind: 3, Epoch: 2824, train_loss: 0.811643694653923, valid_loss: 0.7440782829567238\n",
            "test_ind: 3, Epoch: 2825, train_loss: 0.9171172718942902, valid_loss: 0.920000570791739\n",
            "test_ind: 3, Epoch: 2826, train_loss: 0.9189392431282702, valid_loss: 0.7969831537317347\n",
            "test_ind: 3, Epoch: 2827, train_loss: 0.8030971892086077, valid_loss: 0.7739135247689707\n",
            "test_ind: 3, Epoch: 2828, train_loss: 0.841245627697603, valid_loss: 0.7275969364024975\n",
            "test_ind: 3, Epoch: 2829, train_loss: 0.8205997502362286, valid_loss: 0.8192901434721769\n",
            "test_ind: 3, Epoch: 2830, train_loss: 0.8980781472759484, valid_loss: 1.0685047926726163\n",
            "test_ind: 3, Epoch: 2831, train_loss: 0.8119984438389907, valid_loss: 0.8004071094371654\n",
            "test_ind: 3, Epoch: 2832, train_loss: 0.8922711183995377, valid_loss: 0.9623788551047996\n",
            "test_ind: 3, Epoch: 2833, train_loss: 0.913671334584554, valid_loss: 0.9025815504568595\n",
            "test_ind: 3, Epoch: 2834, train_loss: 0.8620076179504393, valid_loss: 0.9087032918576841\n",
            "test_ind: 3, Epoch: 2835, train_loss: 0.8272862257780851, valid_loss: 0.8363245681480125\n",
            "test_ind: 3, Epoch: 2836, train_loss: 0.7823868622014551, valid_loss: 0.7855764318395544\n",
            "test_ind: 3, Epoch: 2837, train_loss: 0.8842554033538441, valid_loss: 0.8982261904963741\n",
            "test_ind: 3, Epoch: 2838, train_loss: 0.8365781101179712, valid_loss: 0.7627112423932111\n",
            "test_ind: 3, Epoch: 2839, train_loss: 0.8107232223322363, valid_loss: 0.8237808368824147\n",
            "test_ind: 3, Epoch: 2840, train_loss: 0.8334956581209912, valid_loss: 0.8914981064973053\n",
            "test_ind: 3, Epoch: 2841, train_loss: 0.8255846353224766, valid_loss: 0.724666295228181\n",
            "test_ind: 3, Epoch: 2842, train_loss: 0.7988447377711165, valid_loss: 0.7260320274918167\n",
            "test_ind: 3, Epoch: 2843, train_loss: 0.7801329648053205, valid_loss: 0.7681502589473017\n",
            "test_ind: 3, Epoch: 2844, train_loss: 0.8246381724322284, valid_loss: 1.0167528788248699\n",
            "test_ind: 3, Epoch: 2845, train_loss: 0.8472937065878033, valid_loss: 0.7785546867935746\n",
            "test_ind: 3, Epoch: 2846, train_loss: 0.8036030663384331, valid_loss: 0.8072801166110568\n",
            "test_ind: 3, Epoch: 2847, train_loss: 0.7915903256263263, valid_loss: 0.7361915906270345\n",
            "test_ind: 3, Epoch: 2848, train_loss: 0.8064232343508875, valid_loss: 0.7452650600009494\n",
            "test_ind: 3, Epoch: 2849, train_loss: 0.8141563321337288, valid_loss: 0.8959641809816713\n",
            "test_ind: 3, Epoch: 2850, train_loss: 0.9035354249271345, valid_loss: 0.8587915455853498\n",
            "test_ind: 3, Epoch: 2851, train_loss: 0.8220228560176897, valid_loss: 0.8114618901853209\n",
            "test_ind: 3, Epoch: 2852, train_loss: 0.8698424998624825, valid_loss: 0.8239367626331471\n",
            "test_ind: 3, Epoch: 2853, train_loss: 0.8483363787333171, valid_loss: 0.9680468947799117\n",
            "test_ind: 3, Epoch: 2854, train_loss: 0.8048893610636393, valid_loss: 0.7691825584129051\n",
            "test_ind: 3, Epoch: 2855, train_loss: 0.8307505713568794, valid_loss: 0.9221209596704554\n",
            "test_ind: 3, Epoch: 2856, train_loss: 0.9001766134191442, valid_loss: 0.8562691300003618\n",
            "test_ind: 3, Epoch: 2857, train_loss: 0.8765491026419182, valid_loss: 0.7977932117603443\n",
            "test_ind: 3, Epoch: 2858, train_loss: 0.8435415220849307, valid_loss: 0.8748518449288827\n",
            "test_ind: 3, Epoch: 2859, train_loss: 0.8754996546992548, valid_loss: 0.7886139375192147\n",
            "test_ind: 3, Epoch: 2860, train_loss: 0.8085678418477376, valid_loss: 0.8204852386757179\n",
            "test_ind: 3, Epoch: 2861, train_loss: 0.8747103773517374, valid_loss: 1.2660701716387712\n",
            "test_ind: 3, Epoch: 2862, train_loss: 0.8316830646844557, valid_loss: 0.8151805135938857\n",
            "test_ind: 3, Epoch: 2863, train_loss: 0.8106764687432183, valid_loss: 0.8901923144305194\n",
            "test_ind: 3, Epoch: 2864, train_loss: 0.8449491100546754, valid_loss: 1.0021082207008645\n",
            "test_ind: 3, Epoch: 2865, train_loss: 0.9328344604115427, valid_loss: 0.8656306090178314\n",
            "test_ind: 3, Epoch: 2866, train_loss: 0.9295831197573815, valid_loss: 0.8887065781487359\n",
            "test_ind: 3, Epoch: 2867, train_loss: 0.8723499686629684, valid_loss: 0.9242797074494539\n",
            "test_ind: 3, Epoch: 2868, train_loss: 0.8443100893938985, valid_loss: 0.9208740481623896\n",
            "test_ind: 3, Epoch: 2869, train_loss: 0.8186533774858639, valid_loss: 0.8354814847310383\n",
            "test_ind: 3, Epoch: 2870, train_loss: 0.8627213018911856, valid_loss: 0.8113380538092719\n",
            "test_ind: 3, Epoch: 2871, train_loss: 0.7792166780542445, valid_loss: 0.772696918911404\n",
            "test_ind: 3, Epoch: 2872, train_loss: 0.7929780865892951, valid_loss: 0.8944448718318232\n",
            "test_ind: 3, Epoch: 2873, train_loss: 0.8157581517725816, valid_loss: 0.8325273725721571\n",
            "test_ind: 3, Epoch: 2874, train_loss: 0.8729606440037857, valid_loss: 0.8073113759358723\n",
            "test_ind: 3, Epoch: 2875, train_loss: 0.8070320317774644, valid_loss: 0.8942396375868056\n",
            "test_ind: 3, Epoch: 2876, train_loss: 0.8787152149059153, valid_loss: 0.9348499156810619\n",
            "test_ind: 3, Epoch: 2877, train_loss: 0.841240464905162, valid_loss: 0.7407849453113697\n",
            "test_ind: 3, Epoch: 2878, train_loss: 0.8421608960186993, valid_loss: 0.7139805864404749\n",
            "test_ind: 3, Epoch: 2879, train_loss: 0.8623233724523474, valid_loss: 0.9883462058173285\n",
            "test_ind: 3, Epoch: 2880, train_loss: 0.8391159198902272, valid_loss: 0.8063672913445367\n",
            "test_ind: 3, Epoch: 2881, train_loss: 0.8583558164996865, valid_loss: 0.9622115559048123\n",
            "test_ind: 3, Epoch: 2882, train_loss: 0.9250642694072959, valid_loss: 0.937638141490795\n",
            "test_ind: 3, Epoch: 2883, train_loss: 0.8279386861824695, valid_loss: 0.8850051915204084\n",
            "test_ind: 3, Epoch: 2884, train_loss: 0.7912857620804399, valid_loss: 0.76617341571384\n",
            "test_ind: 3, Epoch: 2885, train_loss: 0.8657658129562568, valid_loss: 0.7322236520272714\n",
            "test_ind: 3, Epoch: 2886, train_loss: 0.8845044948436597, valid_loss: 0.8405703262046531\n",
            "test_ind: 3, Epoch: 2887, train_loss: 0.8179462868490336, valid_loss: 0.7655232040970414\n",
            "test_ind: 3, Epoch: 2888, train_loss: 0.8163657011809173, valid_loss: 0.8303690309877747\n",
            "test_ind: 3, Epoch: 2889, train_loss: 0.8555601202411416, valid_loss: 0.9451794447722258\n",
            "test_ind: 3, Epoch: 2890, train_loss: 0.813023614294735, valid_loss: 0.8433453948409468\n",
            "test_ind: 3, Epoch: 2891, train_loss: 0.8635372821195625, valid_loss: 0.6863157484266493\n",
            "Validation loss decreased (0.6882221610457809 --> 0.6863157484266493).  Saving model ...\n",
            "test_ind: 3, Epoch: 2892, train_loss: 0.8245568334320444, valid_loss: 0.819483757019043\n",
            "test_ind: 3, Epoch: 2893, train_loss: 0.8965434439388321, valid_loss: 1.203537110929136\n",
            "test_ind: 3, Epoch: 2894, train_loss: 0.8633922353202914, valid_loss: 0.7573089069790311\n",
            "test_ind: 3, Epoch: 2895, train_loss: 0.847653748076639, valid_loss: 0.8249007331000433\n",
            "test_ind: 3, Epoch: 2896, train_loss: 0.928081241654761, valid_loss: 1.0084902798687971\n",
            "test_ind: 3, Epoch: 2897, train_loss: 0.7934086646562741, valid_loss: 0.833099400555646\n",
            "test_ind: 3, Epoch: 2898, train_loss: 0.8727835608117374, valid_loss: 0.7624687088860405\n",
            "test_ind: 3, Epoch: 2899, train_loss: 0.851675816524176, valid_loss: 0.8088703685336643\n",
            "test_ind: 3, Epoch: 2900, train_loss: 0.7883912781138479, valid_loss: 0.8203553800229673\n",
            "test_ind: 3, Epoch: 2901, train_loss: 0.8715013221458152, valid_loss: 1.021012959656892\n",
            "test_ind: 3, Epoch: 2902, train_loss: 0.8174072430457597, valid_loss: 0.9111397178084764\n",
            "test_ind: 3, Epoch: 2903, train_loss: 0.8379305910181117, valid_loss: 1.0368491808573403\n",
            "test_ind: 3, Epoch: 2904, train_loss: 0.8611016862186385, valid_loss: 0.8595344048959237\n",
            "test_ind: 3, Epoch: 2905, train_loss: 0.8074009153578017, valid_loss: 0.7722521004853424\n",
            "test_ind: 3, Epoch: 2906, train_loss: 0.854280201005347, valid_loss: 0.764629046122233\n",
            "test_ind: 3, Epoch: 2907, train_loss: 0.8464870394012075, valid_loss: 0.7065324783325195\n",
            "test_ind: 3, Epoch: 2908, train_loss: 0.8174863097108442, valid_loss: 0.8343017189591019\n",
            "test_ind: 3, Epoch: 2909, train_loss: 0.870325806700153, valid_loss: 0.9219681775128399\n",
            "test_ind: 3, Epoch: 2910, train_loss: 0.8512803713480632, valid_loss: 0.892588456471761\n",
            "test_ind: 3, Epoch: 2911, train_loss: 0.8470119723567255, valid_loss: 0.7403742472330729\n",
            "test_ind: 3, Epoch: 2912, train_loss: 0.8435294539840132, valid_loss: 0.800362163119846\n",
            "test_ind: 3, Epoch: 2913, train_loss: 0.8531842408356842, valid_loss: 0.7564481805872034\n",
            "test_ind: 3, Epoch: 2914, train_loss: 0.820933336093102, valid_loss: 0.7506290541754829\n",
            "test_ind: 3, Epoch: 2915, train_loss: 0.8057452955363711, valid_loss: 0.6833658394990143\n",
            "Validation loss decreased (0.6863157484266493 --> 0.6833658394990143).  Saving model ...\n",
            "test_ind: 3, Epoch: 2916, train_loss: 0.8001799289091134, valid_loss: 0.7444065941704644\n",
            "test_ind: 3, Epoch: 2917, train_loss: 0.8803866939780152, valid_loss: 0.9448727325156884\n",
            "test_ind: 3, Epoch: 2918, train_loss: 0.856662185103805, valid_loss: 0.85926123018618\n",
            "test_ind: 3, Epoch: 2919, train_loss: 0.8987685250647275, valid_loss: 0.9539510232430918\n",
            "test_ind: 3, Epoch: 2920, train_loss: 0.8240931534472806, valid_loss: 0.8490040567186143\n",
            "test_ind: 3, Epoch: 2921, train_loss: 0.7872945408762236, valid_loss: 0.8541969193352593\n",
            "test_ind: 3, Epoch: 2922, train_loss: 0.8080114906216844, valid_loss: 0.7771085633171929\n",
            "test_ind: 3, Epoch: 2923, train_loss: 0.8270698594458309, valid_loss: 0.7434297844215676\n",
            "test_ind: 3, Epoch: 2924, train_loss: 0.8352060435730735, valid_loss: 0.8157896289119014\n",
            "test_ind: 3, Epoch: 2925, train_loss: 0.7997029799002189, valid_loss: 0.767196496327718\n",
            "test_ind: 3, Epoch: 2926, train_loss: 0.8139807559825755, valid_loss: 0.785608273965341\n",
            "test_ind: 3, Epoch: 2927, train_loss: 0.8026455537772472, valid_loss: 0.8328374756707086\n",
            "test_ind: 3, Epoch: 2928, train_loss: 0.832434571819541, valid_loss: 0.7535671304773401\n",
            "test_ind: 3, Epoch: 2929, train_loss: 0.780490086402422, valid_loss: 0.8403312895033095\n",
            "test_ind: 3, Epoch: 2930, train_loss: 0.8072170033866977, valid_loss: 0.8933995034959581\n",
            "test_ind: 3, Epoch: 2931, train_loss: 0.8599721001990049, valid_loss: 0.7455808674847638\n",
            "test_ind: 3, Epoch: 2932, train_loss: 0.8845572177274726, valid_loss: 0.8995952959413882\n",
            "test_ind: 3, Epoch: 2933, train_loss: 0.792458681412685, valid_loss: 0.7749275631374783\n",
            "test_ind: 3, Epoch: 2934, train_loss: 0.8299247129463855, valid_loss: 0.7205551465352377\n",
            "test_ind: 3, Epoch: 2935, train_loss: 0.8655766675501695, valid_loss: 0.7588998653270581\n",
            "test_ind: 3, Epoch: 2936, train_loss: 0.8160271703461072, valid_loss: 0.7562335862053765\n",
            "test_ind: 3, Epoch: 2937, train_loss: 0.8905896846159005, valid_loss: 0.8739673826429579\n",
            "test_ind: 3, Epoch: 2938, train_loss: 0.8801128481641229, valid_loss: 0.852975191893401\n",
            "test_ind: 3, Epoch: 2939, train_loss: 0.793069138938998, valid_loss: 0.8403179910447863\n",
            "test_ind: 3, Epoch: 2940, train_loss: 0.7965277742456508, valid_loss: 0.7351790180912725\n",
            "test_ind: 3, Epoch: 2941, train_loss: 0.8394458794299468, valid_loss: 0.8106510374281143\n",
            "test_ind: 3, Epoch: 2942, train_loss: 0.7994134278944982, valid_loss: 0.9367121590508356\n",
            "test_ind: 3, Epoch: 2943, train_loss: 0.8133582597897377, valid_loss: 0.8469434137697573\n",
            "test_ind: 3, Epoch: 2944, train_loss: 0.8210182837498041, valid_loss: 0.9103628441139504\n",
            "test_ind: 3, Epoch: 2945, train_loss: 0.8908366391688217, valid_loss: 0.898235109117296\n",
            "test_ind: 3, Epoch: 2946, train_loss: 0.8220467390837493, valid_loss: 0.8488062399405021\n",
            "test_ind: 3, Epoch: 2947, train_loss: 0.8473894861009386, valid_loss: 0.9570894771152073\n",
            "test_ind: 3, Epoch: 2948, train_loss: 0.7755748784100569, valid_loss: 0.7347222080937139\n",
            "test_ind: 3, Epoch: 2949, train_loss: 0.7899749838275675, valid_loss: 0.8017027996204518\n",
            "test_ind: 3, Epoch: 2950, train_loss: 0.841900654781012, valid_loss: 0.7518611307497377\n",
            "test_ind: 3, Epoch: 2951, train_loss: 0.7928191290961374, valid_loss: 0.7403385727493851\n",
            "test_ind: 3, Epoch: 2952, train_loss: 0.8049507906407486, valid_loss: 0.7993201326440882\n",
            "test_ind: 3, Epoch: 2953, train_loss: 0.8118972307370034, valid_loss: 0.7570072809855143\n",
            "test_ind: 3, Epoch: 2954, train_loss: 0.8061904495145068, valid_loss: 0.711425198449029\n",
            "test_ind: 3, Epoch: 2955, train_loss: 0.8493681071717062, valid_loss: 0.7262985264813459\n",
            "test_ind: 3, Epoch: 2956, train_loss: 0.8142081484382535, valid_loss: 0.8808794374819156\n",
            "test_ind: 3, Epoch: 2957, train_loss: 0.8538544796131275, valid_loss: 0.9546788003709581\n",
            "test_ind: 3, Epoch: 2958, train_loss: 0.8546452345671476, valid_loss: 0.7892048623826768\n",
            "test_ind: 3, Epoch: 2959, train_loss: 0.7968648627952293, valid_loss: 0.7246964595935963\n",
            "test_ind: 3, Epoch: 2960, train_loss: 0.8623708972224482, valid_loss: 0.8382233160513418\n",
            "test_ind: 3, Epoch: 2961, train_loss: 0.8144310315450033, valid_loss: 0.7805097544634785\n",
            "test_ind: 3, Epoch: 2962, train_loss: 0.8709277988951882, valid_loss: 0.84494090963293\n",
            "test_ind: 3, Epoch: 2963, train_loss: 0.8198522991604276, valid_loss: 0.8092549112108018\n",
            "test_ind: 3, Epoch: 2964, train_loss: 0.8027437822318373, valid_loss: 0.8160844025788483\n",
            "test_ind: 3, Epoch: 2965, train_loss: 0.7812096748823002, valid_loss: 0.793177639996564\n",
            "test_ind: 3, Epoch: 2966, train_loss: 0.8091417536323453, valid_loss: 1.032798378555863\n",
            "test_ind: 3, Epoch: 2967, train_loss: 0.7918466991848416, valid_loss: 0.872777779897054\n",
            "test_ind: 3, Epoch: 2968, train_loss: 0.8293631989278912, valid_loss: 0.8323058375605831\n",
            "test_ind: 3, Epoch: 2969, train_loss: 0.8319155846113041, valid_loss: 0.8448380364312066\n",
            "test_ind: 3, Epoch: 2970, train_loss: 0.8503450052237805, valid_loss: 0.869736565483941\n",
            "test_ind: 3, Epoch: 2971, train_loss: 0.8130026040253815, valid_loss: 0.8680514229668512\n",
            "test_ind: 3, Epoch: 2972, train_loss: 0.8803852045977556, valid_loss: 0.8799371012934932\n",
            "test_ind: 3, Epoch: 2973, train_loss: 0.8787566997386791, valid_loss: 0.8911853013215241\n",
            "test_ind: 3, Epoch: 2974, train_loss: 0.8564775490466459, valid_loss: 0.7687901214317039\n",
            "test_ind: 3, Epoch: 2975, train_loss: 0.7975013050032251, valid_loss: 0.7005684464066116\n",
            "test_ind: 3, Epoch: 2976, train_loss: 0.849016160140803, valid_loss: 0.9421312544080946\n",
            "test_ind: 3, Epoch: 2977, train_loss: 0.8287230597601998, valid_loss: 0.8604371636002152\n",
            "test_ind: 3, Epoch: 2978, train_loss: 0.7975148801450377, valid_loss: 0.7378729890894007\n",
            "test_ind: 3, Epoch: 2979, train_loss: 0.8690429440251103, valid_loss: 0.8989301964088723\n",
            "test_ind: 3, Epoch: 2980, train_loss: 0.9005291079297478, valid_loss: 0.833639180218732\n",
            "test_ind: 3, Epoch: 2981, train_loss: 0.7675057929239155, valid_loss: 0.7469620528044524\n",
            "test_ind: 3, Epoch: 2982, train_loss: 0.8396968370602456, valid_loss: 0.8651642799377441\n",
            "test_ind: 3, Epoch: 2983, train_loss: 0.8076594964957532, valid_loss: 0.8276631037394206\n",
            "test_ind: 3, Epoch: 2984, train_loss: 0.8227602935131685, valid_loss: 0.8307867226777254\n",
            "test_ind: 3, Epoch: 2985, train_loss: 0.9069352620913659, valid_loss: 1.1051214889243797\n",
            "test_ind: 3, Epoch: 2986, train_loss: 0.8060847800454976, valid_loss: 0.8264590722543221\n",
            "test_ind: 3, Epoch: 2987, train_loss: 0.8288459365750537, valid_loss: 0.7586704360114203\n",
            "test_ind: 3, Epoch: 2988, train_loss: 0.7756491884773159, valid_loss: 0.6820432874891492\n",
            "Validation loss decreased (0.6833658394990143 --> 0.6820432874891492).  Saving model ...\n",
            "test_ind: 3, Epoch: 2989, train_loss: 0.7751417925328383, valid_loss: 0.8096384648923521\n",
            "test_ind: 3, Epoch: 2990, train_loss: 0.8299510214063855, valid_loss: 0.7442466417948406\n",
            "test_ind: 3, Epoch: 2991, train_loss: 0.8186721507413888, valid_loss: 0.7310753398471408\n",
            "test_ind: 3, Epoch: 2992, train_loss: 0.8209905388914507, valid_loss: 1.0369416696053966\n",
            "test_ind: 3, Epoch: 2993, train_loss: 0.8149564060164087, valid_loss: 0.7417500637195729\n",
            "test_ind: 3, Epoch: 2994, train_loss: 0.8299753813096035, valid_loss: 0.8085761600070529\n",
            "test_ind: 3, Epoch: 2995, train_loss: 0.8659368974191172, valid_loss: 0.8679762769628454\n",
            "test_ind: 3, Epoch: 2996, train_loss: 0.7896381484137641, valid_loss: 0.9090922673543295\n",
            "test_ind: 3, Epoch: 2997, train_loss: 0.8760287320172344, valid_loss: 0.8470894142433449\n",
            "test_ind: 3, Epoch: 2998, train_loss: 0.8092414773540733, valid_loss: 0.8296601684005173\n",
            "test_ind: 3, Epoch: 2999, train_loss: 0.8179824558305152, valid_loss: 0.7647671876130281\n",
            "test_ind: 3, Epoch: 3000, train_loss: 0.7733607351044077, valid_loss: 0.8500256185178403\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvReRWkidFah",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dccf053a-60f6-4295-bae3-a43ece6bdf25"
      },
      "source": [
        "print(result_output)\n",
        "print(result_target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[tensor([33.7156], grad_fn=<AddBackward0>), tensor([24.3219], grad_fn=<AddBackward0>), tensor([22.8295], grad_fn=<AddBackward0>), tensor([34.3811], grad_fn=<AddBackward0>), tensor([31.7448], grad_fn=<AddBackward0>), tensor([30.8835], grad_fn=<AddBackward0>), tensor([24.9252], grad_fn=<AddBackward0>), tensor([33.8593], grad_fn=<AddBackward0>), tensor([30.7616], grad_fn=<AddBackward0>), tensor([29.9242], grad_fn=<AddBackward0>), tensor([27.4534], grad_fn=<AddBackward0>), tensor([29.6831], grad_fn=<AddBackward0>), tensor([30.3794], grad_fn=<AddBackward0>), tensor([28.8411], grad_fn=<AddBackward0>), tensor([31.3692], grad_fn=<AddBackward0>), tensor([21.5869], grad_fn=<AddBackward0>), tensor([31.3066], grad_fn=<AddBackward0>), tensor([30.0153], grad_fn=<AddBackward0>), tensor([25.2552], grad_fn=<AddBackward0>), tensor([31.0555], grad_fn=<AddBackward0>), tensor([35.6068], grad_fn=<AddBackward0>), tensor([32.9595], grad_fn=<AddBackward0>), tensor([30.3727], grad_fn=<AddBackward0>), tensor([24.9541], grad_fn=<AddBackward0>), tensor([32.3023], grad_fn=<AddBackward0>), tensor([29.2066], grad_fn=<AddBackward0>), tensor([23.6402], grad_fn=<AddBackward0>), tensor([27.6103], grad_fn=<AddBackward0>), tensor([32.1135], grad_fn=<AddBackward0>), tensor([32.0886], grad_fn=<AddBackward0>), tensor([28.7263], grad_fn=<AddBackward0>), tensor([28.8825], grad_fn=<AddBackward0>), tensor([29.3642], grad_fn=<AddBackward0>), tensor([29.7009], grad_fn=<AddBackward0>), tensor([28.4174], grad_fn=<AddBackward0>), tensor([28.2343], grad_fn=<AddBackward0>), tensor([31.9576], grad_fn=<AddBackward0>), tensor([29.5621], grad_fn=<AddBackward0>), tensor([42.7393], grad_fn=<AddBackward0>), tensor([37.4428], grad_fn=<AddBackward0>), tensor([41.3093], grad_fn=<AddBackward0>), tensor([33.8206], grad_fn=<AddBackward0>), tensor([28.4746], grad_fn=<AddBackward0>), tensor([35.0795], grad_fn=<AddBackward0>), tensor([30.9527], grad_fn=<AddBackward0>), tensor([32.5625], grad_fn=<AddBackward0>), tensor([32.2520], grad_fn=<AddBackward0>), tensor([28.3778], grad_fn=<AddBackward0>), tensor([27.1752], grad_fn=<AddBackward0>), tensor([28.4427], grad_fn=<AddBackward0>), tensor([30.2384], grad_fn=<AddBackward0>), tensor([27.8251], grad_fn=<AddBackward0>), tensor([29.3229], grad_fn=<AddBackward0>), tensor([30.2645], grad_fn=<AddBackward0>), tensor([32.5386], grad_fn=<AddBackward0>), tensor([30.5669], grad_fn=<AddBackward0>), tensor([31.4371], grad_fn=<AddBackward0>), tensor([33.8028], grad_fn=<AddBackward0>), tensor([28.1385], grad_fn=<AddBackward0>), tensor([28.0548], grad_fn=<AddBackward0>), tensor([30.1754], grad_fn=<AddBackward0>), tensor([32.2032], grad_fn=<AddBackward0>), tensor([23.0282], grad_fn=<AddBackward0>), tensor([33.3878], grad_fn=<AddBackward0>), tensor([27.9482], grad_fn=<AddBackward0>), tensor([31.5771], grad_fn=<AddBackward0>), tensor([27.5693], grad_fn=<AddBackward0>), tensor([30.1733], grad_fn=<AddBackward0>), tensor([27.3318], grad_fn=<AddBackward0>), tensor([29.1772], grad_fn=<AddBackward0>), tensor([28.6464], grad_fn=<AddBackward0>), tensor([27.1005], grad_fn=<AddBackward0>), tensor([37.9865], grad_fn=<AddBackward0>), tensor([31.8208], grad_fn=<AddBackward0>), tensor([31.4743], grad_fn=<AddBackward0>), tensor([29.6941], grad_fn=<AddBackward0>), tensor([31.1175], grad_fn=<AddBackward0>), tensor([34.5815], grad_fn=<AddBackward0>), tensor([27.3616], grad_fn=<AddBackward0>), tensor([26.9324], grad_fn=<AddBackward0>), tensor([37.4185], grad_fn=<AddBackward0>), tensor([31.1598], grad_fn=<AddBackward0>), tensor([28.6885], grad_fn=<AddBackward0>), tensor([36.5602], grad_fn=<AddBackward0>), tensor([34.8583], grad_fn=<AddBackward0>), tensor([32.0044], grad_fn=<AddBackward0>), tensor([38.8620], grad_fn=<AddBackward0>), tensor([31.2117], grad_fn=<AddBackward0>), tensor([28.5877], grad_fn=<AddBackward0>), tensor([30.2873], grad_fn=<AddBackward0>), tensor([30.9077], grad_fn=<AddBackward0>), tensor([31.7808], grad_fn=<AddBackward0>), tensor([37.8810], grad_fn=<AddBackward0>), tensor([36.8578], grad_fn=<AddBackward0>), tensor([30.8609], grad_fn=<AddBackward0>), tensor([31.3789], grad_fn=<AddBackward0>), tensor([33.5292], grad_fn=<AddBackward0>), tensor([34.8284], grad_fn=<AddBackward0>), tensor([32.0616], grad_fn=<AddBackward0>), tensor([31.1877], grad_fn=<AddBackward0>), tensor([31.7457], grad_fn=<AddBackward0>), tensor([32.2058], grad_fn=<AddBackward0>), tensor([28.5150], grad_fn=<AddBackward0>), tensor([28.7571], grad_fn=<AddBackward0>), tensor([28.4100], grad_fn=<AddBackward0>), tensor([29.2606], grad_fn=<AddBackward0>), tensor([27.5796], grad_fn=<AddBackward0>), tensor([30.4503], grad_fn=<AddBackward0>), tensor([33.6233], grad_fn=<AddBackward0>), tensor([34.4621], grad_fn=<AddBackward0>), tensor([36.9167], grad_fn=<AddBackward0>), tensor([40.9624], grad_fn=<AddBackward0>), tensor([32.7317], grad_fn=<AddBackward0>), tensor([32.5347], grad_fn=<AddBackward0>), tensor([39.3102], grad_fn=<AddBackward0>), tensor([36.7513], grad_fn=<AddBackward0>), tensor([31.4106], grad_fn=<AddBackward0>), tensor([43.5345], grad_fn=<AddBackward0>), tensor([32.6353], grad_fn=<AddBackward0>), tensor([32.6160], grad_fn=<AddBackward0>), tensor([31.7468], grad_fn=<AddBackward0>), tensor([33.2494], grad_fn=<AddBackward0>), tensor([33.4586], grad_fn=<AddBackward0>), tensor([36.7617], grad_fn=<AddBackward0>), tensor([25.8006], grad_fn=<AddBackward0>), tensor([31.0253], grad_fn=<AddBackward0>), tensor([34.3665], grad_fn=<AddBackward0>), tensor([27.7623], grad_fn=<AddBackward0>), tensor([33.1203], grad_fn=<AddBackward0>), tensor([29.4061], grad_fn=<AddBackward0>), tensor([30.5221], grad_fn=<AddBackward0>), tensor([29.3155], grad_fn=<AddBackward0>), tensor([32.2942], grad_fn=<AddBackward0>), tensor([32.1285], grad_fn=<AddBackward0>), tensor([30.3699], grad_fn=<AddBackward0>), tensor([28.8808], grad_fn=<AddBackward0>), tensor([29.6109], grad_fn=<AddBackward0>), tensor([29.0720], grad_fn=<AddBackward0>), tensor([30.1697], grad_fn=<AddBackward0>), tensor([28.9322], grad_fn=<AddBackward0>), tensor([31.1071], grad_fn=<AddBackward0>), tensor([27.5750], grad_fn=<AddBackward0>), tensor([28.7131], grad_fn=<AddBackward0>)]\n",
            "[tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([27.9474]), tensor([27.9474]), tensor([27.9474]), tensor([27.9474]), tensor([27.9474]), tensor([27.9474]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([27.9474]), tensor([27.9474]), tensor([27.9474]), tensor([27.9474]), tensor([27.9474]), tensor([27.9474]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([27.9474]), tensor([27.9474]), tensor([27.9474]), tensor([27.9474]), tensor([27.9474]), tensor([27.9474]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([33.5814]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([31.0944]), tensor([27.9474]), tensor([27.9474]), tensor([27.9474]), tensor([27.9474]), tensor([27.9474]), tensor([27.9474])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJltbexmiwmi"
      },
      "source": [
        "output_arr = []\n",
        "target_arr = []\n",
        "\n",
        "output3_arr = []\n",
        "output4_arr = []\n",
        "output5_arr = []\n",
        "\n",
        "for i in range(len(result_output)):\n",
        "  target = result_target[i].to('cpu').tolist()[0]\n",
        "  output = result_output[i].to('cpu').tolist()[0]\n",
        "\n",
        "  target_arr.append(target)\n",
        "  output_arr.append(output)\n",
        "\n",
        "  if target == 33.5814208984375:\n",
        "    output3_arr.append(output)\n",
        "  elif target == 31.094409942626953:\n",
        "    output4_arr.append(output)\n",
        "  elif target == 27.947439193725586:\n",
        "    output5_arr.append(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR0GlKIhBx-Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "67b74eb2-353d-4876-defc-aa5125e9c1ca"
      },
      "source": [
        "_dict = {'target' : target_arr, 'output': output_arr}\n",
        "\n",
        "pd_result = pd.DataFrame(_dict)\n",
        "\n",
        "display(pd_result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>33.581421</td>\n",
              "      <td>33.715576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>33.581421</td>\n",
              "      <td>24.321924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>33.581421</td>\n",
              "      <td>22.829470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33.581421</td>\n",
              "      <td>34.381062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33.581421</td>\n",
              "      <td>31.744780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>27.947439</td>\n",
              "      <td>30.169718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>27.947439</td>\n",
              "      <td>28.932240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>27.947439</td>\n",
              "      <td>31.107109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>27.947439</td>\n",
              "      <td>27.575043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>27.947439</td>\n",
              "      <td>28.713118</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>143 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        target     output\n",
              "0    33.581421  33.715576\n",
              "1    33.581421  24.321924\n",
              "2    33.581421  22.829470\n",
              "3    33.581421  34.381062\n",
              "4    33.581421  31.744780\n",
              "..         ...        ...\n",
              "138  27.947439  30.169718\n",
              "139  27.947439  28.932240\n",
              "140  27.947439  31.107109\n",
              "141  27.947439  27.575043\n",
              "142  27.947439  28.713118\n",
              "\n",
              "[143 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwwuLrGCIkqO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 725
        },
        "outputId": "e46c8372-2cb8-4e89-ad53-cd9e9f4c7f36"
      },
      "source": [
        "pd_result_gt = pd_result.groupby('target')\n",
        "sns.boxplot(x='output', \n",
        "                y='target', \n",
        "                hue='target', # different colors by group\n",
        "                s=100, # marker size\n",
        "                data=pd_result)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-1f815f2e4b12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0mhue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# different colors by group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# marker size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                 data=pd_result)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m             )\n\u001b[1;32m     45\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36mboxplot\u001b[0;34m(x, y, hue, data, order, hue_order, orient, color, palette, saturation, width, dodge, fliersize, linewidth, whis, ax, **kwargs)\u001b[0m\n\u001b[1;32m   2236\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2238\u001b[0;31m     \u001b[0mplotter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2239\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, ax, boxplot_kws)\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxplot_kws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;34m\"\"\"Make the plot.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_boxplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxplot_kws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"h\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36mdraw_boxplot\u001b[0;34m(self, ax, kws)\u001b[0m\n\u001b[1;32m    473\u001b[0m                                              \u001b[0mpositions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m                                              \u001b[0mwidths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnested_width\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m                                              **kws)\n\u001b[0m\u001b[1;32m    476\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestyle_boxplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                     \u001b[0;31m# Add legend data, but just for one set of boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m                 f\"for the old name will be dropped %(removal)s.\")\n\u001b[1;32m    295\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;31m# wrapper() must keep the same documented signature as func(): if we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1563\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: boxplot() got an unexpected keyword argument 's'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q58ZF1uMNcKl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}